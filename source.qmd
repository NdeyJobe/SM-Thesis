---
title: "source"
format: html
editor: visual
---

# Data

```{r}
library(tidyverse)
library(bigrquery)

# This query represents dataset "followup_cohort_data" for domain "person" and was generated for All of Us Controlled Tier Dataset v7
dataset_13140188_person_sql <- paste("
    SELECT
        person.person_id,
        p_gender_concept.concept_name as gender,
        person.birth_datetime as date_of_birth,
        p_race_concept.concept_name as race,
        p_ethnicity_concept.concept_name as ethnicity,
        p_sex_at_birth_concept.concept_name as sex_at_birth 
    FROM
        `person` person 
    LEFT JOIN
        `concept` p_gender_concept 
            ON person.gender_concept_id = p_gender_concept.concept_id 
    LEFT JOIN
        `concept` p_race_concept 
            ON person.race_concept_id = p_race_concept.concept_id 
    LEFT JOIN
        `concept` p_ethnicity_concept 
            ON person.ethnicity_concept_id = p_ethnicity_concept.concept_id 
    LEFT JOIN
        `concept` p_sex_at_birth_concept 
            ON person.sex_at_birth_concept_id = p_sex_at_birth_concept.concept_id  
    WHERE
        person.PERSON_ID IN (SELECT
            distinct person_id  
        FROM
            `cb_search_person` cb_search_person  
        WHERE
            cb_search_person.person_id IN (SELECT
                person_id 
            FROM
                `person` p 
            WHERE
                race_concept_id IN (8527, 8516, 2100000001, 8515, 903096, 2000000008, 45882607, 1177221, 38003615, 8557) ) 
            AND cb_search_person.person_id NOT IN (SELECT
                criteria.person_id 
            FROM
                (SELECT
                    DISTINCT person_id, entry_date, concept_id 
                FROM
                    `cb_search_all_events` 
                WHERE
                    (concept_id IN(SELECT
                        DISTINCT c.concept_id 
                    FROM
                        `cb_criteria` c 
                    JOIN
                        (SELECT
                            CAST(cr.id as string) AS id       
                        FROM
                            `cb_criteria` cr       
                        WHERE
                            concept_id IN (440508)       
                            AND full_text LIKE '%_rank1]%'      ) a 
                            ON (c.path LIKE CONCAT('%.', a.id, '.%') 
                            OR c.path LIKE CONCAT('%.', a.id) 
                            OR c.path LIKE CONCAT(a.id, '.%') 
                            OR c.path = a.id) 
                    WHERE
                        is_standard = 1 
                        AND is_selectable = 1) 
                    AND is_standard = 1 )) criteria ) 
            AND cb_search_person.person_id NOT IN (SELECT
                person_id 
            FROM
                `person` p 
            WHERE
                race_concept_id IN (2100000001, 903096, 2000000008, 45882607, 1177221, 38003615, 8557) ) )", sep="")

# Formulate a Cloud Storage destination path for the data exported from BigQuery.
# NOTE: By default data exported multiple times on the same day will overwrite older copies.
#       But data exported on a different days will write to a new location so that historical
#       copies can be kept as the dataset definition is changed.
person_13140188_path <- file.path(
  Sys.getenv("WORKSPACE_BUCKET"),
  "bq_exports",
  Sys.getenv("OWNER_EMAIL"),
  strftime(lubridate::now(), "%Y%m%d"),  # Comment out this line if you want the export to always overwrite.
  "person_13140188",
  "person_13140188_*.csv")
message(str_glue('The data will be written to {person_13140188_path}. Use this path when reading ',
                 'the data into your notebooks in the future.'))

# Perform the query and export the dataset to Cloud Storage as CSV files.
# NOTE: You only need to run `bq_table_save` once. After that, you can
#       just read data from the CSVs in Cloud Storage.
bq_table_save(
  bq_dataset_query(Sys.getenv("WORKSPACE_CDR"), dataset_13140188_person_sql, billing = Sys.getenv("GOOGLE_PROJECT")),
  person_13140188_path,
  destination_format = "CSV")


# Read the data directly from Cloud Storage into memory.
# NOTE: Alternatively you can `gsutil -m cp {person_13140188_path}` to copy these files
#       to the Jupyter disk.
read_bq_export_from_workspace_bucket <- function(export_path) {
  col_types <- cols(gender = col_character(), race = col_character(), ethnicity = col_character(), sex_at_birth = col_character())
  bind_rows(
    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),
        function(csv) {
          message(str_glue('Loading {csv}.'))
          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)
          if (is.null(col_types)) {
            col_types <- spec(chunk)
          }
          chunk
        }))
}
dataset_13140188_person_df <- read_bq_export_from_workspace_bucket(person_13140188_path)

dim(dataset_13140188_person_df)

head(dataset_13140188_person_df, 5)
library(tidyverse)
library(bigrquery)

# This query represents dataset "followup_cohort_data" for domain "condition" and was generated for All of Us Controlled Tier Dataset v7
dataset_13140188_condition_sql <- paste("
    SELECT
        c_occurrence.person_id,
        c_standard_concept.concept_name as standard_concept_name,
        c_standard_concept.concept_code as standard_concept_code,
        c_occurrence.condition_start_datetime,
        c_occurrence.condition_end_datetime,
        c_occurrence.visit_occurrence_id,
        c_occurrence.condition_source_value,
        c_source_concept.concept_code as source_concept_code 
    FROM
        ( SELECT
            * 
        FROM
            `condition_occurrence` c_occurrence 
        WHERE
            (
                condition_concept_id IN (SELECT
                    DISTINCT c.concept_id 
                FROM
                    `cb_criteria` c 
                JOIN
                    (SELECT
                        CAST(cr.id as string) AS id       
                    FROM
                        `cb_criteria` cr       
                    WHERE
                        concept_id IN (132736, 132797, 133729, 133810, 133956, 134057, 134442, 137275, 138384, 138713, 140362, 140673, 140842, 141050, 141934, 192359, 192673, 192686, 193519, 193688, 193782, 195314, 195363, 195556, 195771, 195834, 195861, 196158, 196236, 197320, 197593, 197921, 198185, 199064, 200618, 200687, 201254, 201820, 201826, 201965, 24609, 252365, 255891, 257628, 312358, 313223, 313800, 313928, 315558, 317002, 318712, 319835, 320128, 321052, 321596, 321887, 36717004, 36717279, 37119138, 372892, 372903, 37311303, 37312531, 376112, 4029602, 4030520, 40480859, 40483287, 40493038, 4090272, 4096347, 4111261, 4132314, 4140207, 4174977, 4177206, 4234480, 42538169, 42539502, 4256228, 4263367, 42709887, 4280354, 4295287, 4298809, 43021246, 43021252, 4306451, 432595, 432863, 4335872, 433811, 434004, 434311, 434314, 434610, 434622, 434908, 435308, 435510, 435511, 435517, 435788, 436222, 436659, 437247, 437992, 438297, 438624, 438721, 439770, 439777, 439928, 440302, 440674,
 440704, 440977, 440979, 441553, 443439, 443447, 443597, 443601, 443612, 443614, 443729, 443731, 443733, 443734, 44784621, 45768812, 45770886, 45773180, 46270384, 46271022, 75004, 75650, 764123)       
                        AND full_text LIKE '%_rank1]%'      ) a 
                        ON (c.path LIKE CONCAT('%.', a.id, '.%') 
                        OR c.path LIKE CONCAT('%.', a.id) 
                        OR c.path LIKE CONCAT(a.id, '.%') 
                        OR c.path = a.id) 
                WHERE
                    is_standard = 1 
                    AND is_selectable = 1) 
                OR  condition_source_concept_id IN (SELECT
                    DISTINCT c.concept_id 
                FROM
                    `cb_criteria` c 
                JOIN
                    (SELECT
                        CAST(cr.id as string) AS id       
                    FROM
                        `cb_criteria` cr       
                    WHERE
                        concept_id IN (1326482, 1326483, 1326492, 1326493, 1326494, 1326495, 1326496, 1326511, 1326512, 1326513, 1326514, 1326516, 1326517, 1326522, 1326523, 1326526, 1326527, 1326528, 1326529, 1326531, 1326532, 1326533, 1326534, 1326620, 1326621, 1326622, 1326623, 1326624, 1326625, 1326626, 1326627, 1326628, 1326632, 1326635, 1326637, 1326640, 1326643, 1326644, 1326646, 1326647, 1326648, 1326649, 1326652, 1326653, 1326654, 1326655, 1326657, 1326658, 1326660, 1326661, 1326662, 1326663, 1326664, 1326665, 1326666, 1326667, 1326668, 1326669, 1326670, 1326671, 1326672, 1326673, 1326674, 1326675, 1326676, 1326679, 1326680, 1326681, 1326682, 1326683, 1326684, 1326685, 1326686, 1326688, 1326689, 1326690, 1326691, 1326692, 1326693, 1326694, 1326695, 1326697, 1326700, 1326701, 1326702, 1326703, 1567286, 1567838, 1567852, 1567858, 1567859, 1567861, 1567862, 1567893, 1567922, 1567940, 1567943, 1567944, 1567949, 1567956, 1567958, 1567959, 1567960, 1567964, 1567965, 1567966, 1567969,
 1567971, 1567975, 1567988, 1567990, 1567991, 1568066, 1568073, 1568076, 1568079, 1568420, 1568421, 1568422, 1568430, 1569271, 1569324, 1569325, 1569336, 1569419, 1569420, 1569515, 1569637, 1569638, 1569811, 1569849, 1569878, 1569966, 1569967, 1569997, 1570377, 1570393, 1570611, 1570612, 1570619, 1571469, 1571473, 1571479, 1571485, 1571486, 1571533, 1572108, 1572216, 1572227, 1572241, 1572267, 1572283, 1572285, 1575304, 1575308, 1595536, 1595600, 1595601, 1595691, 1595692, 1595693, 1595694, 1595695, 19400, 19409, 19431, 19454, 19456, 19470, 19472, 19475, 19481, 19483, 19485, 19486, 19487, 19488, 19489, 19694, 19699, 19716, 19726, 19736, 35205420, 35205549, 35205550, 35205551, 35205552, 35205553, 35206692, 35206693, 35206694, 35206695, 35206702, 35206703, 35206704, 35206705, 35206706, 35206707, 35206708, 35206709, 35206710, 35206731, 35206732, 35206733, 35206734, 35206735, 35206736, 35206737, 35206738, 35206739, 35206750, 35206751, 35206752, 35206753, 35206754, 35206755, 35206757,
 35206761, 35206762, 35206763, 35206764, 35206765, 35206766, 35206838, 35206839, 35206840, 35206842, 35206852, 35206853, 35206854, 35206855, 35206856, 35206857, 35206858, 35206859, 35206878, 35206879, 35206881, 35206882, 35206884, 35206885, 35206887, 35206888, 35206889, 35206891, 35206892, 35206893, 35206894, 35206895, 35206896, 35206897, 35206898, 35206899, 35206900, 35206901, 35206902, 35206903, 35207072, 35207073, 35207074, 35207075, 35207076, 35207077, 35207078, 35207079, 35207081, 35207085, 35207086, 35207087, 35207088, 35207089, 35207090, 35207091, 35207092, 35207094, 35207097, 35207099, 35207101, 35207102, 35207107, 35207351, 35207560, 35207664, 35207665, 35207666, 35207667, 35207668, 35207671, 35207672, 35207673, 35207793, 35207794, 35207795, 35207796, 35207797, 35207798, 35207799, 35207800, 35207801, 35207836, 35207841, 35207842, 35207843, 35207859, 35207860, 35207868, 35207869, 35207870, 35207871, 35207872, 35207873, 35207874, 35207876, 35207904, 35207915, 35207916, 35207917,
 35207918, 35208099, 35208100, 35208101, 35208125, 35208126, 35208127, 35208128, 35208130, 35208131, 35208132, 35208133, 35208275, 35208276, 35208277, 35208278, 35208279, 35208280, 35208281, 35208282, 35208284, 35208285, 35208656, 35208657, 35208658, 35208659, 35208660, 35208661, 35208662, 35208663, 35208664, 35208665, 35208715, 35208716, 35208717, 35208718, 35208719, 35208720, 35208721, 35208722, 35208750, 35208756, 35208817, 35208825, 35208826, 35208827, 35208828, 35208829, 35208830, 35208831, 35209165, 35209166, 35209167, 35209170, 35209172, 35209173, 35209174, 35209196, 35209197, 35209198, 35209199, 35209200, 35209201, 35209202, 35209203, 35209204, 35209205, 35209206, 35209207, 35209208, 35209210, 35209211, 35209212, 35209213, 35209214, 35209215, 35209216, 35209217, 35209218, 35209219, 35209220, 35209221, 35209222, 35209223, 35209224, 35209235, 35209236, 35209237, 35209243, 35209244, 35209245, 35209246, 35209264, 35209265, 35209266, 35209267, 35209269, 35209270, 35209271, 35209272,
 35209273, 35209274, 35209275, 35209276, 35209277, 35209278, 35209279, 35209280, 35209291, 35209297, 35209298, 35209299, 35209340, 35209341, 35209342, 35209343, 35209344, 35211012, 35211013, 35211014, 35211015, 35211016, 35211017, 35211018, 35211019, 35211020, 35211021, 35211022, 35211023, 35211024, 35211134, 35211314, 35211315, 35211316, 35211317, 35211318, 35211319, 35211340, 35211392, 35211393, 35211394, 35211395, 35211404, 35211523, 35225373, 35225404, 37200021, 37200022, 37200023, 37200024, 37200025, 37200026, 37200027, 37200028, 37200029, 37200030, 37200031, 37200032, 37200033, 37200034, 37200035, 37200036, 37200037, 37200038, 37200039, 37200040, 37200041, 37200042, 37200043, 37200044, 37200045, 37200046, 37200047, 37200049, 37200050, 37200051, 37200052, 37200053, 37200054, 37200056, 37200057, 37200058, 37200059, 37200063, 37200067, 37200071, 37200072, 37200073, 37200074, 37200075, 37200076, 37200077, 37200078, 37200081, 37200082, 37200083, 37200141, 37200142, 37200143, 37200144,
 37200145, 37200146, 37200147, 37200148, 37200149, 37200150, 37200151, 37200152, 37200153, 37200154, 37200155, 37200156, 37200157, 37200158, 37200159, 37200160, 37200161, 37200162, 37200163, 37200164, 37200165, 37200166, 37200167, 37200168, 37200170, 37200171, 37200172, 37200175, 37200176, 37200177, 37200180, 37200181, 37200182, 37200185, 37200186, 37200187, 37200188, 37200189, 37200190, 37200191, 37200192, 37200194, 37200196, 37200198, 37200199, 37200200, 37200201, 37200202, 37200203, 37200204, 37200205, 37200206, 37200207, 37200208, 37200209, 37200210, 37200211, 37200212, 37200213, 37200214, 37200215, 37200216, 37200217, 37200218, 37200219, 37200220, 37200221, 37200222, 37200223, 37200224, 37200225, 37200227, 37200228, 37200229, 37200230, 37200232, 37200233, 37200234, 37200235, 37200237, 37200238, 37200239, 37200240, 37200242, 37200243, 37200244, 37200245, 37200246, 37200247, 37200248, 37200249, 37200251, 37200252, 37200253, 37200254, 37200255, 37200257, 37200258, 37200260, 37200261,
 37200262, 37200263, 37200265, 37200266, 37200269, 37200270, 37200273, 37200274, 37200275, 37200278, 37200279, 37200280, 37200281, 37200284, 37200291, 37200301, 37200302, 37200303, 37200304, 37200305, 37200306, 37200310, 37200311, 37200492, 37200493, 37200494, 37200495, 37200564, 37200565, 37200566, 37200567, 37200618, 37200619, 37200620, 37200622, 37200623, 37200624, 37200626, 37200627, 37200628, 37200630, 37200632, 37200634, 37200636, 37200640, 37200642, 37200645, 37200686, 37200693, 37200694, 37200695, 37200697, 37200698, 37200699, 37201087, 37201088, 37201937, 37201938, 37201939, 37201941, 37201942, 37202093, 37202097, 37202101, 37202102, 37202105, 37202106, 37202109, 37202113, 37202114, 37202117, 37202118, 37202119, 37202122, 37202123, 37202126, 37202130, 37202131, 37202132, 37202134, 37202135, 37202136, 37202138, 37202139, 37202140, 37202142, 37202146, 37202147, 37202148, 37202154, 37202155, 37202156, 37202162, 37202164, 37202166, 37202170, 37202171, 37202174, 37202175, 37202178,
 37202179, 37202180, 37202183, 37202186, 37202187, 37202188, 37202190, 37202194, 37202195, 37202196, 37202198, 37202202, 37202203, 37202204, 44819357, 44819497, 44819498, 44819499, 44819500, 44819501, 44819502, 44819503, 44819504, 44819520, 44819521, 44819522, 44819523, 44819526, 44819527, 44819530, 44819580, 44819605, 44819606, 44819711, 44819718, 44819719, 44819724, 44819729, 44819738, 44819751, 44819752, 44819753, 44819754, 44819755, 44819756, 44819757, 44819758, 44819801, 44819814, 44819823, 44819824, 44819940, 44819941, 44819989, 44819990, 44820017, 44820018, 44820027, 44820028, 44820029, 44820056, 44820057, 44820255, 44820680, 44820681, 44820682, 44820683, 44820684, 44820685, 44820692, 44820693, 44820694, 44820695, 44820696, 44820700, 44820701, 44820707, 44820780, 44820781, 44820879, 44820899, 44820900, 44820902, 44820903, 44820947, 44820951, 44820952, 44820953, 44820954, 44820965, 44820969, 44820970, 44820971, 44820972, 44820973, 44820974, 44820985, 44821099, 44821101, 44821168,
 44821169, 44821170, 44821171, 44821172, 44821174, 44821175, 44821176, 44821177, 44821178, 44821203, 44821204, 44821205, 44821206, 44821212, 44821238, 44821239, 44821247, 44821564, 44821667, 44821784, 44821785, 44821787, 44821794, 44821799, 44821802, 44821803, 44821807, 44821841, 44821863, 44821864, 44821865, 44821866, 44821870, 44821873, 44821949, 44821958, 44821962, 44821971, 44821972, 44821973, 44821998, 44821999, 44822000, 44822001, 44822002, 44822003, 44822036, 44822037, 44822038, 44822039, 44822046, 44822175, 44822176, 44822238, 44822239, 44822258, 44822265, 44822266, 44822292, 44822293, 44822297, 44822523, 44822524, 44822525, 44822705, 44822813, 44822932, 44822933, 44822934, 44822935, 44822936, 44822937, 44822947, 44822951, 44822954, 44822955, 44822958, 44822959, 44822960, 44823017, 44823034, 44823035, 44823036, 44823040, 44823108, 44823109, 44823120, 44823124, 44823125, 44823135, 44823136, 44823150, 44823151, 44823152, 44823184, 44823191, 44823192, 44823199, 44823329, 44823330,
 44823331, 44823332, 44823333, 44823384, 44823405, 44823412, 44823446, 44823651, 44823652, 44823653, 44823927, 44823969, 44824071, 44824072, 44824073, 44824074, 44824080, 44824088, 44824089, 44824090, 44824091, 44824094, 44824095, 44824098, 44824099, 44824154, 44824172, 44824234, 44824257, 44824266, 44824275, 44824295, 44824296, 44824297, 44824324, 44824332, 44824333, 44824339, 44824340, 44824341, 44824342, 44824344, 44824345, 44824352, 44824486, 44824487, 44824543, 44824544, 44824574, 44824575, 44824582, 44824583, 44824619, 44824620, 44824621, 44824622, 44824630, 44824845, 44824846, 44824852, 44825015, 44825094, 44825095, 44825096, 44825143, 44825144, 44825259, 44825263, 44825264, 44825265, 44825273, 44825274, 44825275, 44825280, 44825281, 44825282, 44825286, 44825349, 44825425, 44825426, 44825427, 44825443, 44825446, 44825447, 44825448, 44825451, 44825456, 44825457, 44825485, 44825486, 44825523, 44825524, 44825539, 44825540, 44825544, 44825551, 44825553, 44825554, 44825658, 44825659,
 44825660, 44825661, 44825736, 44825737, 44825764, 44825765, 44825766, 44825767, 44825805, 44825821, 44826028, 44826029, 44826030, 44826037, 44826288, 44826329, 44826330, 44826456, 44826459, 44826460, 44826461, 44826462, 44826468, 44826472, 44826474, 44826475, 44826476, 44826477, 44826478, 44826479, 44826480, 44826546, 44826547, 44826568, 44826570, 44826572, 44826573, 44826642, 44826643, 44826646, 44826647, 44826651, 44826654, 44826655, 44826657, 44826658, 44826659, 44826688, 44826689, 44826690, 44826725, 44826730, 44826731, 44826733, 44826744, 44826854, 44826916, 44826917, 44826939, 44826940, 44827183, 44827184, 44827368, 44827369, 44827370, 44827371, 44827487, 44827613, 44827615, 44827616, 44827617, 44827631, 44827632, 44827634, 44827636, 44827692, 44827693, 44827711, 44827712, 44827717, 44827797, 44827802, 44827804, 44827805, 44827806, 44827807, 44827808, 44827814, 44827815, 44827835, 44827836, 44827837, 44827874, 44827887, 44827888, 44827889, 44827890, 44827894, 44827899, 44828017,
 44828019, 44828080, 44828081, 44828082, 44828106, 44828107, 44828108, 44828114, 44828115, 44828116, 44828117, 44828118, 44828147, 44828148, 44828150, 44828164, 44828357, 44828358, 44828554, 44828669, 44828670, 44828671, 44828672, 44828786, 44828790, 44828792, 44828793, 44828794, 44828795, 44828796, 44828803, 44828805, 44828806, 44828807, 44828808, 44828810, 44828811, 44828812, 44828814, 44828815, 44828816, 44828820, 44828891, 44828892, 44828893, 44828894, 44828971, 44828984, 44828985, 44828991, 44828992, 44828994, 44828996, 44829021, 44829022, 44829024, 44829025, 44829026, 44829027, 44829028, 44829048, 44829051, 44829052, 44829053, 44829054, 44829058, 44829060, 44829062, 44829063, 44829064, 44829073, 44829187, 44829188, 44829235, 44829294, 44829295, 44829308, 44829479, 44829480, 44829481, 44829482, 44829639, 44829733, 44829874, 44829876, 44829877, 44829878, 44829879, 44829880, 44829881, 44829882, 44829884, 44829895, 44829897, 44829898, 44829899, 44829900, 44829902, 44829904, 44829905,
 44829906, 44829907, 44829990, 44829991, 44829992, 44829994, 44829995, 44830077, 44830087, 44830091, 44830094, 44830123, 44830159, 44830161, 44830162, 44830166, 44830171, 44830172, 44830173, 44830176, 44830307, 44830308, 44830310, 44830369, 44830370, 44830402, 44830410, 44830411, 44830412, 44830442, 44830443, 44830444, 44830633, 44830634, 44830794, 44830803, 44830804, 44830883, 44830917, 44831045, 44831046, 44831047, 44831048, 44831049, 44831055, 44831057, 44831058, 44831060, 44831061, 44831062, 44831063, 44831068, 44831069, 44831127, 44831143, 44831144, 44831145, 44831148, 44831231, 44831251, 44831256, 44831257, 44831258, 44831262, 44831263, 44831270, 44831318, 44831323, 44831325, 44831326, 44831327, 44831328, 44831331, 44831336, 44831471, 44831472, 44831487, 44831555, 44831561, 44831583, 44831585, 44831586, 44832069, 44832187, 44832188, 44832190, 44832191, 44832192, 44832193, 44832194, 44832203, 44832206, 44832207, 44832208, 44832214, 44832215, 44832218, 44832269, 44832299, 44832300,
 44832306, 44832368, 44832383, 44832388, 44832397, 44832399, 44832400, 44832409, 44832434, 44832435, 44832474, 44832475, 44832487, 44832496, 44832611, 44832612, 44832613, 44832669, 44832670, 44832671, 44832672, 44832687, 44832688, 44832689, 44832709, 44832710, 44832711, 44832721, 44832931, 44833111, 44833364, 44833365, 44833366, 44833367, 44833368, 44833379, 44833383, 44833384, 44833385, 44833386, 44833388, 44833389, 44833390, 44833391, 44833392, 44833393, 44833394, 44833437, 44833464, 44833465, 44833555, 44833556, 44833558, 44833575, 44833578, 44833579, 44833583, 44833593, 44833594, 44833595, 44833649, 44833650, 44833651, 44833652, 44833657, 44833658, 44833659, 44833660, 44833661, 44833665, 44833676, 44833800, 44833801, 44833880, 44833892, 44833893, 44833894, 44834387, 44834388, 44834548, 44834549, 44834559, 44834560, 44834561, 44834566, 44834568, 44834570, 44834571, 44834573, 44834574, 44834579, 44834580, 44834628, 44834629, 44834642, 44834643, 44834644, 44834645, 44834646, 44834647,
 44834715, 44834733, 44834734, 44834739, 44834749, 44834750, 44834760, 44834761, 44834762, 44834763, 44834776, 44834778, 44834779, 44834813, 44834816, 44834817, 44834822, 44834830, 44834832, 44834833, 44834834, 44834835, 44834959, 44834960, 44834961, 44834962, 44835026, 44835027, 44835056, 44835062, 44835064, 44835065, 44835083, 44835294, 44835295, 44835296, 44835474, 44835483, 44835568, 44835607, 44835748, 44835749, 44835750, 44835751, 44835752, 44835753, 44835754, 44835761, 44835763, 44835764, 44835765, 44835767, 44835769, 44835770, 44835831, 44835847, 44835848, 44835858, 44835859, 44835921, 44835922, 44835923, 44835945, 44835953, 44835957, 44835958, 44835959, 44835967, 44835994, 44835995, 44836021, 44836026, 44836027, 44836028, 44836034, 44836035, 44836036, 44836037, 44836047, 44836158, 44836159, 44836228, 44836230, 44836250, 44836251, 44836259, 44836260, 44836294, 44836295, 44836300, 44836487, 44836913, 44836914, 44836915, 44836916, 44836917, 44836918, 44836931, 44836933, 44836934,
 44836935, 44836936, 44836937, 44836938, 44836939, 44836940, 44836944, 44836945, 44836946, 44836947, 44836953, 44837006, 44837023, 44837031, 44837108, 44837109, 44837112, 44837116, 44837120, 44837126, 44837145, 44837146, 44837174, 44837186, 44837187, 44837188, 44837189, 44837190, 44837191, 44837192, 44837193, 44837316, 44837317, 44837377, 44837378, 44837401, 44837402, 44837403, 44837430, 44837445, 44837833, 45532833, 45533009, 45533010, 45533011, 45533017, 45533018, 45533019, 45533020, 45533021, 45533022, 45533023, 45533024, 45533047, 45533048, 45533332, 45533333, 45533336, 45533468, 45533483, 45533484, 45533488, 45533490, 45533491, 45533492, 45533503, 45533563, 45533663, 45533664, 45533665, 45533666, 45533668, 45533669, 45533689, 45533693, 45533696, 45533697, 45533698, 45533699, 45533700, 45533712, 45533713, 45533780, 45533781, 45533782, 45533783, 45534125, 45534439, 45534453, 45534454, 45534546, 45534547, 45537025, 45537027, 45537030, 45537031, 45537033, 45537034, 45537035, 45537036,
 45537037, 45537038, 45537039, 45537040, 45537041, 45537042, 45537070, 45537072, 45537073, 45537074, 45537076, 45537077, 45537078, 45537080, 45537081, 45537082, 45537083, 45537087, 45537090, 45537692, 45537694, 45537695, 45537722, 45537723, 45537927, 45537943, 45537944, 45537953, 45537954, 45537958, 45537960, 45537961, 45537962, 45537963, 45538143, 45538147, 45538291, 45538292, 45538295, 45538299, 45538368, 45538387, 45538403, 45538422, 45538425, 45538426, 45538427, 45538431, 45538433, 45538435, 45538437, 45538438, 45538443, 45538444, 45538448, 45538489, 45538537, 45538586, 45538605, 45538606, 45538607, 45538608, 45538609, 45538610, 45538623, 45538624, 45538626, 45538628, 45538629, 45538631, 45538633, 45538647, 45538648, 45538649, 45538701, 45538702, 45538703, 45538704, 45538706, 45538707, 45538708, 45538709, 45539344, 45541805, 45541820, 45541821, 45541823, 45541825, 45541826, 45541827, 45541833, 45541834, 45541835, 45541836, 45541837, 45541838, 45541839, 45541840, 45541876, 45541877,
 45541879, 45541880, 45541881, 45541884, 45542480, 45542481, 45542482, 45542483, 45542484, 45542728, 45542730, 45542731, 45542736, 45542737, 45542738, 45542741, 45542773, 45542774, 45542775, 45542778, 45542780, 45542781, 45542911, 45542912, 45543055, 45543057, 45543060, 45543061, 45543164, 45543209, 45543210, 45543218, 45543283, 45543401, 45543403, 45543404, 45543405, 45543406, 45543407, 45543409, 45543410, 45543434, 45543435, 45543436, 45543449, 45543515, 45543517, 45543518, 45543519, 45543520, 45543521, 45543522, 45543877, 45544154, 45546703, 45546705, 45546708, 45546709, 45546710, 45546711, 45546749, 45546750, 45546751, 45546752, 45546753, 45546754, 45546757, 45546758, 45546759, 45546762, 45546763, 45547364, 45547365, 45547366, 45547367, 45547369, 45547617, 45547618, 45547621, 45547622, 45547623, 45547624, 45547625, 45547626, 45547627, 45547632, 45547633, 45547635, 45547659, 45547661, 45547662, 45547663, 45547667, 45547763, 45547877, 45547903, 45547904, 45547906, 45547908, 45548056,
 45548057, 45548058, 45548059, 45548060, 45548061, 45548069, 45548070, 45548076, 45548097, 45548131, 45548223, 45548224, 45548225, 45548226, 45548227, 45548228, 45548252, 45548253, 45548254, 45548257, 45548259, 45548260, 45548261, 45548262, 45548263, 45548265, 45548266, 45548287, 45548288, 45548289, 45548346, 45548351, 45548353, 45548354, 45548653, 45548655, 45548968, 45548977, 45549060, 45551473, 45551474, 45551487, 45551488, 45551489, 45551490, 45551491, 45551493, 45551495, 45551496, 45551497, 45551498, 45551499, 45551500, 45551502, 45551503, 45551504, 45551534, 45551535, 45551537, 45551540, 45551541, 45551542, 45551543, 45551544, 45551547, 45552133, 45552134, 45552135, 45552137, 45552138, 45552179, 45552180, 45552359, 45552361, 45552372, 45552373, 45552374, 45552375, 45552379, 45552381, 45552382, 45552383, 45552385, 45552386, 45552388, 45552419, 45552420, 45552551, 45552702, 45552703, 45552704, 45552705, 45552706, 45552707, 45552709, 45552823, 45552824, 45552825, 45552826, 45552827,
 45552830, 45552831, 45552832, 45552833, 45552835, 45552839, 45552871, 45552874, 45553011, 45553012, 45553013, 45553014, 45553015, 45553037, 45553040, 45553043, 45553044, 45553045, 45553046, 45553047, 45553048, 45553049, 45553066, 45553067, 45553114, 45553116, 45553117, 45553118, 45553119, 45553120, 45553171, 45553439, 45553686, 45553727, 45553736, 45553737, 45556244, 45556246, 45556252, 45556254, 45556257, 45556260, 45556292, 45556294, 45556295, 45556296, 45556297, 45556298, 45556872, 45556873, 45556876, 45556877, 45557089, 45557090, 45557106, 45557107, 45557110, 45557112, 45557113, 45557116, 45557142, 45557434, 45557437, 45557441, 45557568, 45557569, 45557570, 45557571, 45557572, 45557575, 45557576, 45557578, 45557579, 45557581, 45557584, 45557585, 45557588, 45557592, 45557635, 45557728, 45557730, 45557731, 45557732, 45557733, 45557734, 45557735, 45557736, 45557737, 45557738, 45557754, 45557759, 45557760, 45557761, 45557781, 45557821, 45557822, 45557823, 45557824, 45557825, 45557826,
 45557827, 45557828, 45558143, 45558145, 45558146, 45558484, 45558577, 45561018, 45561035, 45561036, 45561037, 45561038, 45561040, 45561041, 45561043, 45561044, 45561045, 45561046, 45561047, 45561048, 45561049, 45561051, 45561052, 45561053, 45561085, 45561087, 45561089, 45561090, 45561091, 45561092, 45561093, 45561098, 45561928, 45561929, 45561938, 45561940, 45561941, 45561942, 45561943, 45561947, 45561948, 45561949, 45561953, 45561954, 45561955, 45561956, 45561957, 45561977, 45561989, 45561990, 45561991, 45561993, 45562109, 45562249, 45562250, 45562251, 45562257, 45562371, 45562385, 45562386, 45562387, 45562388, 45562391, 45562393, 45562395, 45562396, 45562399, 45562498, 45562561, 45562562, 45562563, 45562564, 45562565, 45562590, 45562591, 45562592, 45562610, 45562665, 45562666, 45562667, 45562668, 45562669, 45562670, 45562671, 45562672, 45562709, 45563006, 45563255, 45563298, 45563324, 45563325, 45565829, 45565830, 45565831, 45565833, 45565834, 45565835, 45565836, 45565838, 45565843,
 45565844, 45565845, 45565886, 45565887, 45565888, 45565889, 45565890, 45565891, 45565893, 45565895, 45565898, 45565899, 45566472, 45566504, 45566723, 45566724, 45566729, 45566731, 45566733, 45566734, 45566735, 45566736, 45566737, 45566762, 45566906, 45566907, 45566910, 45567050, 45567070, 45567071, 45567073, 45567075, 45567079, 45567183, 45567192, 45567207, 45567208, 45567209, 45567210, 45567211, 45567212, 45567214, 45567216, 45567223, 45567228, 45567283, 45567284, 45567313, 45567377, 45567378, 45567379, 45567380, 45567381, 45567382, 45567406, 45567409, 45567413, 45567415, 45567416, 45567417, 45567418, 45567419, 45567440, 45567489, 45567490, 45567491, 45567492, 45567493, 45567494, 45567496, 45567497, 45567498, 45567545, 45568131, 45568222, 45570697, 45570716, 45570719, 45570720, 45570721, 45570722, 45570723, 45570724, 45570765, 45570767, 45570768, 45570769, 45570771, 45570772, 45570773, 45570775, 45571428, 45571638, 45571639, 45571649, 45571650, 45571651, 45571652, 45571654, 45571658,
 45571659, 45571661, 45571662, 45571692, 45571693, 45571694, 45571836, 45571990, 45572116, 45572117, 45572118, 45572119, 45572121, 45572124, 45572126, 45572134, 45572177, 45572296, 45572297, 45572298, 45572299, 45572300, 45572301, 45572302, 45572303, 45572304, 45572327, 45572329, 45572331, 45572332, 45572336, 45572337, 45572338, 45572339, 45572392, 45572394, 45572395, 45572448, 45572699, 45572717, 45573034, 45573035, 45573039, 45573041, 45573115, 45573116, 45575555, 45575556, 45575557, 45575559, 45575560, 45575562, 45575567, 45575568, 45575571, 45575575, 45575576, 45575579, 45575611, 45575612, 45575613, 45575615, 45575616, 45575617, 45575620, 45575621, 45575622, 45575623, 45575624, 45575625, 45576203, 45576204, 45576205, 45576206, 45576207, 45576208, 45576209, 45576410, 45576437, 45576438, 45576439, 45576440, 45576443, 45576446, 45576447, 45576476, 45576480, 45576481, 45576757, 45576758, 45576761, 45576762, 45576767, 45576768, 45576769, 45576770, 45576773, 45576890, 45576903, 45576904,
 45576905, 45576906, 45576907, 45576908, 45576909, 45577068, 45577069, 45577070, 45577072, 45577073, 45577074, 45577075, 45577092, 45577093, 45577094, 45577095, 45577098, 45577100, 45577102, 45577189, 45577190, 45577192, 45577193, 45577195, 45577196, 45577516, 45577738, 45577803, 45577889, 45580405, 45580407, 45580409, 45580410, 45580413, 45580414, 45580419, 45580420, 45580421, 45580422, 45580456, 45580457, 45580458, 45580460, 45580461, 45580463, 45580464, 45580465, 45580467, 45580469, 45581085, 45581086, 45581112, 45581342, 45581343, 45581344, 45581349, 45581350, 45581352, 45581353, 45581354, 45581355, 45581358, 45581359, 45581376, 45581388, 45581519, 45581678, 45581787, 45581798, 45581799, 45581800, 45581801, 45581802, 45581807, 45581808, 45581809, 45581810, 45581814, 45581868, 45581874, 45581973, 45581974, 45581976, 45581977, 45581978, 45581979, 45581980, 45581981, 45581982, 45581983, 45581984, 45582007, 45582010, 45582014, 45582015, 45582016, 45582021, 45582022, 45582023, 45582024,
 45582082, 45582083, 45582084, 45582085, 45582086, 45582088, 45582089, 45582126, 45582130, 45582399, 45582407, 45582408, 45582409, 45582709, 45582710, 45585223, 45585239, 45585240, 45585241, 45585243, 45585247, 45585248, 45585250, 45585251, 45585255, 45585257, 45585259, 45585260, 45585311, 45585312, 45585313, 45585314, 45585317, 45585318, 45585322, 45585323, 45585324, 45585871, 45586119, 45586132, 45586138, 45586139, 45586140, 45586142, 45586143, 45586144, 45586145, 45586476, 45586477, 45586479, 45586480, 45586481, 45586483, 45586614, 45586615, 45586617, 45586619, 45586623, 45586629, 45586630, 45586802, 45586804, 45586805, 45586806, 45586807, 45586808, 45586809, 45586833, 45586834, 45586836, 45586838, 45586839, 45586859, 45586913, 45586914, 45586916, 45586917, 45586918, 45587018, 45587246, 45587247, 45587467, 45587506, 45587526, 45587528, 45587600, 45590048, 45590049, 45590063, 45590065, 45590068, 45590075, 45590076, 45590077, 45590112, 45590113, 45590115, 45590116, 45590120, 45590122,
 45590124, 45590125, 45590127, 45590128, 45591019, 45591023, 45591026, 45591027, 45591029, 45591030, 45591031, 45591033, 45591034, 45591065, 45591066, 45591068, 45591201, 45591330, 45591363, 45591364, 45591366, 45591370, 45591373, 45591374, 45591494, 45591495, 45591496, 45591497, 45591498, 45591499, 45591500, 45591501, 45591503, 45591504, 45591509, 45591537, 45591660, 45591661, 45591662, 45591663, 45591664, 45591686, 45591688, 45591691, 45591698, 45591699, 45591700, 45591716, 45591769, 45591770, 45591772, 45591773, 45591819, 45591820, 45592142, 45592435, 45594885, 45594886, 45594897, 45594898, 45594899, 45594904, 45594905, 45594907, 45594908, 45594909, 45594913, 45594914, 45594915, 45594916, 45594917, 45594918, 45594920, 45594955, 45594956, 45594959, 45594960, 45594961, 45594962, 45594963, 45594964, 45595554, 45595555, 45595557, 45595558, 45595586, 45595772, 45595782, 45595783, 45595789, 45595790, 45595791, 45595792, 45595793, 45595794, 45595795, 45595797, 45595798, 45595799, 45595802,
 45595803, 45595804, 45595805, 45595826, 45595833, 45595834, 45595836, 45595839, 45596104, 45596106, 45596107, 45596108, 45596109, 45596111, 45596113, 45596188, 45596232, 45596233, 45596234, 45596235, 45596236, 45596239, 45596240, 45596244, 45596247, 45596289, 45596290, 45596334, 45596388, 45596397, 45596398, 45596399, 45596400, 45596428, 45596432, 45596435, 45596436, 45596437, 45596438, 45596439, 45596440, 45596452, 45596515, 45596516, 45596517, 45596518, 45596519, 45596520, 45596521, 45596523, 45596524, 45596525, 45596565, 45596873, 45596874, 45597196, 45599743, 45599756, 45599757, 45599758, 45599760, 45599765, 45599769, 45599770, 45599771, 45599772, 45599773, 45599811, 45599813, 45599814, 45599815, 45599816, 45599817, 45599818, 45599819, 45599823, 45599824, 45599827, 45599829, 45600368, 45600369, 45600370, 45600371, 45600609, 45600633, 45600634, 45600636, 45600637, 45600638, 45600639, 45600640, 45600641, 45600642, 45600644, 45600678, 45600680, 45600930, 45600932, 45600934, 45600938,
 45600941, 45601065, 45601066, 45601069, 45601070, 45601071, 45601072, 45601073, 45601080, 45601084, 45601180, 45601252, 45601253, 45601254, 45601255, 45601256, 45601257, 45601259, 45601260, 45601276, 45601279, 45601280, 45601281, 45601282, 45601283, 45601305, 45601306, 45601307, 45601380, 45601381, 45601382, 45601383, 45601384, 45601385, 45601386, 45601433, 45601434, 45601721, 45601722, 45601736, 45602035, 45604535, 45604541, 45604542, 45604544, 45604545, 45604574, 45604576, 45604579, 45604580, 45604581, 45604584, 45604585, 45604587, 45605168, 45605169, 45605170, 45605379, 45605392, 45605393, 45605394, 45605395, 45605397, 45605398, 45605401, 45605402, 45605403, 45605404, 45605405, 45605434, 45605696, 45605777, 45605828, 45605829, 45605830, 45605831, 45605832, 45605833, 45605834, 45605838, 45605840, 45605842, 45605844, 45605873, 45605906, 45605907, 45605942, 45606018, 45606019, 45606020, 45606021, 45606022, 45606023, 45606024, 45606025, 45606027, 45606047, 45606048, 45606050, 45606051,
 45606155, 45606156, 45606159, 45606160, 45606161, 45606162, 45606164, 45606165, 45606166, 45606167, 45606168, 45606213, 45606214, 45606486, 45606496, 45606750, 45606800, 45606823, 45606824, 45609323, 45609324, 45609329, 45609334, 45609335, 45609337, 45609338, 45609339, 45609342, 45609345, 45609381, 45609382, 45609383, 45609384, 45609386, 45609388, 45609389, 45609392, 45609393, 45609989, 45609990, 45609991, 713855, 725238, 725239, 725240, 725241, 725242, 725258, 725259, 725260, 725263, 725370, 725437, 725440, 725441, 725442, 766347, 766391, 766392, 766393, 920128, 920129)       
                        AND full_text LIKE '%_rank1]%'      ) a 
                        ON (c.path LIKE CONCAT('%.', a.id, '.%') 
                        OR c.path LIKE CONCAT('%.', a.id) 
                        OR c.path LIKE CONCAT(a.id, '.%') 
                        OR c.path = a.id) 
                WHERE
                    is_standard = 0 
                    AND is_selectable = 1)
            )  
            AND (
                c_occurrence.PERSON_ID IN (SELECT
                    distinct person_id  
                FROM
                    `cb_search_person` cb_search_person  
                WHERE
                    cb_search_person.person_id IN (SELECT
                        person_id 
                    FROM
                        `person` p 
                    WHERE
                        race_concept_id IN (8527, 8516, 2100000001, 8515, 903096, 2000000008, 45882607, 1177221, 38003615, 8557) ) 
                    AND cb_search_person.person_id NOT IN (SELECT
                        criteria.person_id 
                    FROM
                        (SELECT
                            DISTINCT person_id, entry_date, concept_id 
                        FROM
                            `cb_search_all_events` 
                        WHERE
                            (concept_id IN(SELECT
                                DISTINCT c.concept_id 
                            FROM
                                `cb_criteria` c 
                            JOIN
                                (SELECT
                                    CAST(cr.id as string) AS id       
                                FROM
                                    `cb_criteria` cr       
                                WHERE
                                    concept_id IN (440508)       
                                    AND full_text LIKE '%_rank1]%'      ) a 
                                    ON (c.path LIKE CONCAT('%.', a.id, '.%') 
                                    OR c.path LIKE CONCAT('%.', a.id) 
                                    OR c.path LIKE CONCAT(a.id, '.%') 
                                    OR c.path = a.id) 
                            WHERE
                                is_standard = 1 
                                AND is_selectable = 1) 
                            AND is_standard = 1 )) criteria ) 
                    AND cb_search_person.person_id NOT IN (SELECT
                        person_id 
                    FROM
                        `person` p 
                    WHERE
                        race_concept_id IN (2100000001, 903096, 2000000008, 45882607, 1177221, 38003615, 8557) ) )
            )) c_occurrence 
    LEFT JOIN
        `concept` c_standard_concept 
            ON c_occurrence.condition_concept_id = c_standard_concept.concept_id 
    LEFT JOIN
        `concept` c_source_concept 
            ON c_occurrence.condition_source_concept_id = c_source_concept.concept_id", sep="")

# Formulate a Cloud Storage destination path for the data exported from BigQuery.
# NOTE: By default data exported multiple times on the same day will overwrite older copies.
#       But data exported on a different days will write to a new location so that historical
#       copies can be kept as the dataset definition is changed.
condition_13140188_path <- file.path(
  Sys.getenv("WORKSPACE_BUCKET"),
  "bq_exports",
  Sys.getenv("OWNER_EMAIL"),
  strftime(lubridate::now(), "%Y%m%d"),  # Comment out this line if you want the export to always overwrite.
  "condition_13140188",
  "condition_13140188_*.csv")
message(str_glue('The data will be written to {condition_13140188_path}. Use this path when reading ',
                 'the data into your notebooks in the future.'))

# Perform the query and export the dataset to Cloud Storage as CSV files.
# NOTE: You only need to run `bq_table_save` once. After that, you can
#       just read data from the CSVs in Cloud Storage.
bq_table_save(
  bq_dataset_query(Sys.getenv("WORKSPACE_CDR"), dataset_13140188_condition_sql, billing = Sys.getenv("GOOGLE_PROJECT")),
  condition_13140188_path,
  destination_format = "CSV")


# Read the data directly from Cloud Storage into memory.
# NOTE: Alternatively you can `gsutil -m cp {condition_13140188_path}` to copy these files
#       to the Jupyter disk.
read_bq_export_from_workspace_bucket <- function(export_path) {
  col_types <- cols(standard_concept_name = col_character(), standard_concept_code = col_character(), condition_source_value = col_character(), source_concept_code = col_character())
  bind_rows(
    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),
        function(csv) {
          message(str_glue('Loading {csv}.'))
          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)
          if (is.null(col_types)) {
            col_types <- spec(chunk)
          }
          chunk
        }))
}
dataset_13140188_condition_df <- read_bq_export_from_workspace_bucket(condition_13140188_path)

dim(dataset_13140188_condition_df)

head(dataset_13140188_condition_df, 5)
library(tidyverse)
library(bigrquery)

# This query represents dataset "followup_cohort_data" for domain "device" and was generated for All of Us Controlled Tier Dataset v7
dataset_13140188_device_sql <- paste("
    SELECT
        device.person_id,
        d_standard_concept.concept_name as standard_concept_name,
        d_standard_concept.concept_code as standard_concept_code,
        device.device_exposure_start_datetime,
        device.device_exposure_end_datetime,
        device.visit_occurrence_id,
        device.device_source_value,
        d_source_concept.concept_code as source_concept_code 
    FROM
        ( SELECT
            * 
        FROM
            `device_exposure` device 
        WHERE
            (
                device_concept_id IN (2616955)
            )  
            AND (
                PERSON_ID IN (SELECT
                    distinct person_id  
                FROM
                    `cb_search_person` cb_search_person  
                WHERE
                    cb_search_person.person_id IN (SELECT
                        person_id 
                    FROM
                        `person` p 
                    WHERE
                        race_concept_id IN (8527, 8516, 2100000001, 8515, 903096, 2000000008, 45882607, 1177221, 38003615, 8557) ) 
                    AND cb_search_person.person_id NOT IN (SELECT
                        criteria.person_id 
                    FROM
                        (SELECT
                            DISTINCT person_id, entry_date, concept_id 
                        FROM
                            `cb_search_all_events` 
                        WHERE
                            (concept_id IN(SELECT
                                DISTINCT c.concept_id 
                            FROM
                                `cb_criteria` c 
                            JOIN
                                (SELECT
                                    CAST(cr.id as string) AS id       
                                FROM
                                    `cb_criteria` cr       
                                WHERE
                                    concept_id IN (440508)       
                                    AND full_text LIKE '%_rank1]%'      ) a 
                                    ON (c.path LIKE CONCAT('%.', a.id, '.%') 
                                    OR c.path LIKE CONCAT('%.', a.id) 
                                    OR c.path LIKE CONCAT(a.id, '.%') 
                                    OR c.path = a.id) 
                            WHERE
                                is_standard = 1 
                                AND is_selectable = 1) 
                            AND is_standard = 1 )) criteria ) 
                    AND cb_search_person.person_id NOT IN (SELECT
                        person_id 
                    FROM
                        `person` p 
                    WHERE
                        race_concept_id IN (2100000001, 903096, 2000000008, 45882607, 1177221, 38003615, 8557) ) )
            )) device 
    LEFT JOIN
        `concept` d_standard_concept 
            ON device.device_concept_id = d_standard_concept.concept_id 
    LEFT JOIN
        `concept` d_source_concept 
            ON device.device_source_concept_id = d_source_concept.concept_id", sep="")

# Formulate a Cloud Storage destination path for the data exported from BigQuery.
# NOTE: By default data exported multiple times on the same day will overwrite older copies.
#       But data exported on a different days will write to a new location so that historical
#       copies can be kept as the dataset definition is changed.
device_13140188_path <- file.path(
  Sys.getenv("WORKSPACE_BUCKET"),
  "bq_exports",
  Sys.getenv("OWNER_EMAIL"),
  strftime(lubridate::now(), "%Y%m%d"),  # Comment out this line if you want the export to always overwrite.
  "device_13140188",
  "device_13140188_*.csv")
message(str_glue('The data will be written to {device_13140188_path}. Use this path when reading ',
                 'the data into your notebooks in the future.'))

# Perform the query and export the dataset to Cloud Storage as CSV files.
# NOTE: You only need to run `bq_table_save` once. After that, you can
#       just read data from the CSVs in Cloud Storage.
bq_table_save(
  bq_dataset_query(Sys.getenv("WORKSPACE_CDR"), dataset_13140188_device_sql, billing = Sys.getenv("GOOGLE_PROJECT")),
  device_13140188_path,
  destination_format = "CSV")


# Read the data directly from Cloud Storage into memory.
# NOTE: Alternatively you can `gsutil -m cp {device_13140188_path}` to copy these files
#       to the Jupyter disk.
read_bq_export_from_workspace_bucket <- function(export_path) {
  col_types <- cols(standard_concept_name = col_character(), standard_concept_code = col_character(), device_source_value = col_character(), source_concept_code = col_character())
  bind_rows(
    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),
        function(csv) {
          message(str_glue('Loading {csv}.'))
          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)
          if (is.null(col_types)) {
            col_types <- spec(chunk)
          }
          chunk
        }))
}
dataset_13140188_device_df <- read_bq_export_from_workspace_bucket(device_13140188_path)

dim(dataset_13140188_device_df)

head(dataset_13140188_device_df, 5)
library(tidyverse)
library(bigrquery)

# This query represents dataset "followup_cohort_data" for domain "drug" and was generated for All of Us Controlled Tier Dataset v7
dataset_13140188_drug_sql <- paste("
    SELECT
        d_exposure.person_id,
        d_standard_concept.concept_name as standard_concept_name,
        d_standard_concept.concept_code as standard_concept_code,
        d_exposure.drug_exposure_start_datetime,
        d_exposure.drug_exposure_end_datetime,
        d_exposure.verbatim_end_date,
        d_exposure.visit_occurrence_id,
        d_visit.concept_name as visit_occurrence_concept_name,
        d_exposure.drug_source_value,
        d_source_concept.concept_name as source_concept_name,
        d_source_concept.concept_code as source_concept_code 
    FROM
        ( SELECT
            * 
        FROM
            `drug_exposure` d_exposure 
        WHERE
            (
                drug_concept_id IN (SELECT
                    DISTINCT ca.descendant_id 
                FROM
                    `cb_criteria_ancestor` ca 
                JOIN
                    (SELECT
                        DISTINCT c.concept_id       
                    FROM
                        `cb_criteria` c       
                    JOIN
                        (SELECT
                            CAST(cr.id as string) AS id             
                        FROM
                            `cb_criteria` cr             
                        WHERE
                            concept_id IN (1301125, 1304643, 1308368, 1309068, 1317967, 1318853, 1319156, 1332418, 1346823, 1347450, 1352890, 1367571, 1373928, 1381661, 1395773, 1399177, 1502905, 1512446, 1513876, 1516976, 1517740, 1531601, 1544838, 1548111, 1550023, 1567198, 1593331, 1596977, 1703069, 1707687, 1746244, 1758536, 1769389, 1776684, 1786617, 19010309, 19015768, 19019193, 19035631, 19078126, 35606465, 35606467, 40165636, 42899476, 46221581, 501343, 528323, 754270, 900017, 939506, 950316, 950435, 951469, 952004, 977968, 985247, 994058)             
                            AND full_text LIKE '%_rank1]%'       ) a 
                            ON (c.path LIKE CONCAT('%.', a.id, '.%') 
                            OR c.path LIKE CONCAT('%.', a.id) 
                            OR c.path LIKE CONCAT(a.id, '.%') 
                            OR c.path = a.id) 
                    WHERE
                        is_standard = 1 
                        AND is_selectable = 1) b 
                        ON (ca.ancestor_id = b.concept_id)))  
                    AND (d_exposure.PERSON_ID IN (SELECT
                        distinct person_id  
                FROM
                    `cb_search_person` cb_search_person  
                WHERE
                    cb_search_person.person_id IN (SELECT
                        person_id 
                    FROM
                        `person` p 
                    WHERE
                        race_concept_id IN (8527, 8516, 2100000001, 8515, 903096, 2000000008, 45882607, 1177221, 38003615, 8557) ) 
                    AND cb_search_person.person_id NOT IN (SELECT
                        criteria.person_id 
                    FROM
                        (SELECT
                            DISTINCT person_id, entry_date, concept_id 
                        FROM
                            `cb_search_all_events` 
                        WHERE
                            (concept_id IN(SELECT
                                DISTINCT c.concept_id 
                            FROM
                                `cb_criteria` c 
                            JOIN
                                (SELECT
                                    CAST(cr.id as string) AS id       
                                FROM
                                    `cb_criteria` cr       
                                WHERE
                                    concept_id IN (440508)       
                                    AND full_text LIKE '%_rank1]%'      ) a 
                                    ON (c.path LIKE CONCAT('%.', a.id, '.%') 
                                    OR c.path LIKE CONCAT('%.', a.id) 
                                    OR c.path LIKE CONCAT(a.id, '.%') 
                                    OR c.path = a.id) 
                            WHERE
                                is_standard = 1 
                                AND is_selectable = 1) 
                            AND is_standard = 1 )) criteria ) 
                    AND cb_search_person.person_id NOT IN (SELECT
                        person_id 
                    FROM
                        `person` p 
                    WHERE
                        race_concept_id IN (2100000001, 903096, 2000000008, 45882607, 1177221, 38003615, 8557) ) )
            )) d_exposure 
    LEFT JOIN
        `concept` d_standard_concept 
            ON d_exposure.drug_concept_id = d_standard_concept.concept_id 
    LEFT JOIN
        `visit_occurrence` v 
            ON d_exposure.visit_occurrence_id = v.visit_occurrence_id 
    LEFT JOIN
        `concept` d_visit 
            ON v.visit_concept_id = d_visit.concept_id 
    LEFT JOIN
        `concept` d_source_concept 
            ON d_exposure.drug_source_concept_id = d_source_concept.concept_id", sep="")

# Formulate a Cloud Storage destination path for the data exported from BigQuery.
# NOTE: By default data exported multiple times on the same day will overwrite older copies.
#       But data exported on a different days will write to a new location so that historical
#       copies can be kept as the dataset definition is changed.
drug_13140188_path <- file.path(
  Sys.getenv("WORKSPACE_BUCKET"),
  "bq_exports",
  Sys.getenv("OWNER_EMAIL"),
  strftime(lubridate::now(), "%Y%m%d"),  # Comment out this line if you want the export to always overwrite.
  "drug_13140188",
  "drug_13140188_*.csv")
message(str_glue('The data will be written to {drug_13140188_path}. Use this path when reading ',
                 'the data into your notebooks in the future.'))

# Perform the query and export the dataset to Cloud Storage as CSV files.
# NOTE: You only need to run `bq_table_save` once. After that, you can
#       just read data from the CSVs in Cloud Storage.
bq_table_save(
  bq_dataset_query(Sys.getenv("WORKSPACE_CDR"), dataset_13140188_drug_sql, billing = Sys.getenv("GOOGLE_PROJECT")),
  drug_13140188_path,
  destination_format = "CSV")


# Read the data directly from Cloud Storage into memory.
# NOTE: Alternatively you can `gsutil -m cp {drug_13140188_path}` to copy these files
#       to the Jupyter disk.
read_bq_export_from_workspace_bucket <- function(export_path) {
  col_types <- cols(standard_concept_name = col_character(), standard_concept_code = col_character(), visit_occurrence_concept_name = col_character(), drug_source_value = col_character(), source_concept_name = col_character(), source_concept_code = col_character())
  bind_rows(
    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),
        function(csv) {
          message(str_glue('Loading {csv}.'))
          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)
          if (is.null(col_types)) {
            col_types <- spec(chunk)
          }
          chunk
        }))
}
dataset_13140188_drug_df <- read_bq_export_from_workspace_bucket(drug_13140188_path)

dim(dataset_13140188_drug_df)

head(dataset_13140188_drug_df, 5)
library(tidyverse)
library(bigrquery)

# This query represents dataset "followup_cohort_data" for domain "measurement" and was generated for All of Us Controlled Tier Dataset v7
dataset_13140188_measurement_sql <- paste("
    SELECT
        measurement.person_id,
        m_standard_concept.concept_name as standard_concept_name,
        m_standard_concept.concept_code as standard_concept_code,
        measurement.measurement_datetime,
        m_type.concept_name as measurement_type_concept_name,
        measurement.value_as_number,
        m_value.concept_name as value_as_concept_name,
        measurement.visit_occurrence_id,
        measurement.measurement_source_value,
        m_source_concept.concept_name as source_concept_name,
        m_source_concept.concept_code as source_concept_code 
    FROM
        ( SELECT
            * 
        FROM
            `measurement` measurement 
        WHERE
            (
                measurement_concept_id IN (SELECT
                    DISTINCT c.concept_id 
                FROM
                    `cb_criteria` c 
                JOIN
                    (SELECT
                        CAST(cr.id as string) AS id       
                    FROM
                        `cb_criteria` cr       
                    WHERE
                        concept_id IN (1619025, 3000067, 3000185, 3000819, 3001122, 3001802, 3002302, 3002400, 3002812, 3004249, 3004410, 3004789, 3005715, 3006032, 3006453, 3007359, 3013801, 3016662, 3016723, 3017797, 3019510, 3019574, 3019839, 3019897, 3021044, 3021347, 3022192, 3022844, 3023421, 3023520, 3027945, 3030366, 3034485, 3037121, 3037556, 3042733, 37022114, 37040063, 40764999, 4235028, 46236952)       
                        AND full_text LIKE '%_rank1]%'      ) a 
                        ON (c.path LIKE CONCAT('%.', a.id, '.%') 
                        OR c.path LIKE CONCAT('%.', a.id) 
                        OR c.path LIKE CONCAT(a.id, '.%') 
                        OR c.path = a.id) 
                WHERE
                    is_standard = 1 
                    AND is_selectable = 1) 
                OR  measurement_source_concept_id IN (SELECT
                    DISTINCT c.concept_id 
                FROM
                    `cb_criteria` c 
                JOIN
                    (SELECT
                        CAST(cr.id as string) AS id       
                    FROM
                        `cb_criteria` cr       
                    WHERE
                        concept_id IN (44825059, 44826256, 44828604, 44828606, 44834348, 45568152, 903121, 903133)       
                        AND full_text LIKE '%_rank1]%'      ) a 
                        ON (c.path LIKE CONCAT('%.', a.id, '.%') 
                        OR c.path LIKE CONCAT('%.', a.id) 
                        OR c.path LIKE CONCAT(a.id, '.%') 
                        OR c.path = a.id) 
                WHERE
                    is_standard = 0 
                    AND is_selectable = 1)
            )  
            AND (
                measurement.PERSON_ID IN (SELECT
                    distinct person_id  
                FROM
                    `cb_search_person` cb_search_person  
                WHERE
                    cb_search_person.person_id IN (SELECT
                        person_id 
                    FROM
                        `person` p 
                    WHERE
                        race_concept_id IN (8527, 8516, 2100000001, 8515, 903096, 2000000008, 45882607, 1177221, 38003615, 8557) ) 
                    AND cb_search_person.person_id NOT IN (SELECT
                        criteria.person_id 
                    FROM
                        (SELECT
                            DISTINCT person_id, entry_date, concept_id 
                        FROM
                            `cb_search_all_events` 
                        WHERE
                            (concept_id IN(SELECT
                                DISTINCT c.concept_id 
                            FROM
                                `cb_criteria` c 
                            JOIN
                                (SELECT
                                    CAST(cr.id as string) AS id       
                                FROM
                                    `cb_criteria` cr       
                                WHERE
                                    concept_id IN (440508)       
                                    AND full_text LIKE '%_rank1]%'      ) a 
                                    ON (c.path LIKE CONCAT('%.', a.id, '.%') 
                                    OR c.path LIKE CONCAT('%.', a.id) 
                                    OR c.path LIKE CONCAT(a.id, '.%') 
                                    OR c.path = a.id) 
                            WHERE
                                is_standard = 1 
                                AND is_selectable = 1) 
                            AND is_standard = 1 )) criteria ) 
                    AND cb_search_person.person_id NOT IN (SELECT
                        person_id 
                    FROM
                        `person` p 
                    WHERE
                        race_concept_id IN (2100000001, 903096, 2000000008, 45882607, 1177221, 38003615, 8557) ) )
            )) measurement 
    LEFT JOIN
        `concept` m_standard_concept 
            ON measurement.measurement_concept_id = m_standard_concept.concept_id 
    LEFT JOIN
        `concept` m_type 
            ON measurement.measurement_type_concept_id = m_type.concept_id 
    LEFT JOIN
        `concept` m_value 
            ON measurement.value_as_concept_id = m_value.concept_id 
    LEFT JOIN
        `concept` m_source_concept 
            ON measurement.measurement_source_concept_id = m_source_concept.concept_id", sep="")

# Formulate a Cloud Storage destination path for the data exported from BigQuery.
# NOTE: By default data exported multiple times on the same day will overwrite older copies.
#       But data exported on a different days will write to a new location so that historical
#       copies can be kept as the dataset definition is changed.
measurement_13140188_path <- file.path(
  Sys.getenv("WORKSPACE_BUCKET"),
  "bq_exports",
  Sys.getenv("OWNER_EMAIL"),
  strftime(lubridate::now(), "%Y%m%d"),  # Comment out this line if you want the export to always overwrite.
  "measurement_13140188",
  "measurement_13140188_*.csv")
message(str_glue('The data will be written to {measurement_13140188_path}. Use this path when reading ',
                 'the data into your notebooks in the future.'))

# Perform the query and export the dataset to Cloud Storage as CSV files.
# NOTE: You only need to run `bq_table_save` once. After that, you can
#       just read data from the CSVs in Cloud Storage.
bq_table_save(
  bq_dataset_query(Sys.getenv("WORKSPACE_CDR"), dataset_13140188_measurement_sql, billing = Sys.getenv("GOOGLE_PROJECT")),
  measurement_13140188_path,
  destination_format = "CSV")


# Read the data directly from Cloud Storage into memory.
# NOTE: Alternatively you can `gsutil -m cp {measurement_13140188_path}` to copy these files
#       to the Jupyter disk.
read_bq_export_from_workspace_bucket <- function(export_path) {
  col_types <- cols(standard_concept_name = col_character(), standard_concept_code = col_character(), measurement_type_concept_name = col_character(), value_as_concept_name = col_character(), measurement_source_value = col_character(), source_concept_name = col_character(), source_concept_code = col_character())
  bind_rows(
    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),
        function(csv) {
          message(str_glue('Loading {csv}.'))
          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)
          if (is.null(col_types)) {
            col_types <- spec(chunk)
          }
          chunk
        }))
}
dataset_13140188_measurement_df <- read_bq_export_from_workspace_bucket(measurement_13140188_path)

dim(dataset_13140188_measurement_df)

head(dataset_13140188_measurement_df, 5)
library(tidyverse)
library(bigrquery)

# This query represents dataset "followup_cohort_data" for domain "observation" and was generated for All of Us Controlled Tier Dataset v7
dataset_13140188_observation_sql <- paste("
    SELECT
        observation.person_id,
        o_standard_concept.concept_name as standard_concept_name,
        o_standard_concept.concept_code as standard_concept_code,
        observation.observation_datetime,
        observation.value_as_number,
        o_value.concept_name as value_as_concept_name,
        observation.visit_occurrence_id,
        o_visit.concept_name as visit_occurrence_concept_name,
        observation.observation_source_value,
        o_source_concept.concept_name as source_concept_name,
        o_source_concept.concept_code as source_concept_code,
        observation.value_source_value 
    FROM
        ( SELECT
            * 
        FROM
            `observation` observation 
        WHERE
            (
                observation_concept_id IN (43054909) 
                OR  observation_source_concept_id IN (1554061, 35225061, 35225062, 35225063, 35225064, 35225065, 35225066, 35225067, 35225068, 35225373, 35225404, 35225436, 44820385, 44821564, 44822663, 44822705, 44823447, 44823833, 44823834, 44824957, 44827360, 44827367, 44827368, 44827369, 44827370, 44828554, 44829595, 44829639, 44830102, 44830794, 44830803, 44831947, 44831957, 44833056, 44834280, 44834282, 44835426, 44835472, 44835482, 44835497, 44836656, 44836657, 44837448, 44837833, 45537692, 45537694, 45542476, 45542482, 45547367, 45552133, 45552870, 45556876, 45557821, 45561673, 45566436, 45566472, 45576204, 45576209, 45577822, 45581085, 45585801, 45585835, 45590766, 45595484, 45595553, 45595557, 45600368, 45600370, 45605169, 45605170, 45609945, 45609986, 45609991, 45610000, 713855)
            )  
            AND (
                observation.PERSON_ID IN (SELECT
                    distinct person_id  
                FROM
                    `cb_search_person` cb_search_person  
                WHERE
                    cb_search_person.person_id IN (SELECT
                        person_id 
                    FROM
                        `person` p 
                    WHERE
                        race_concept_id IN (8527, 8516, 2100000001, 8515, 903096, 2000000008, 45882607, 1177221, 38003615, 8557) ) 
                    AND cb_search_person.person_id NOT IN (SELECT
                        criteria.person_id 
                    FROM
                        (SELECT
                            DISTINCT person_id, entry_date, concept_id 
                        FROM
                            `cb_search_all_events` 
                        WHERE
                            (concept_id IN(SELECT
                                DISTINCT c.concept_id 
                            FROM
                                `cb_criteria` c 
                            JOIN
                                (SELECT
                                    CAST(cr.id as string) AS id       
                                FROM
                                    `cb_criteria` cr       
                                WHERE
                                    concept_id IN (440508)       
                                    AND full_text LIKE '%_rank1]%'      ) a 
                                    ON (c.path LIKE CONCAT('%.', a.id, '.%') 
                                    OR c.path LIKE CONCAT('%.', a.id) 
                                    OR c.path LIKE CONCAT(a.id, '.%') 
                                    OR c.path = a.id) 
                            WHERE
                                is_standard = 1 
                                AND is_selectable = 1) 
                            AND is_standard = 1 )) criteria ) 
                    AND cb_search_person.person_id NOT IN (SELECT
                        person_id 
                    FROM
                        `person` p 
                    WHERE
                        race_concept_id IN (2100000001, 903096, 2000000008, 45882607, 1177221, 38003615, 8557) ) )
            )) observation 
    LEFT JOIN
        `concept` o_standard_concept 
            ON observation.observation_concept_id = o_standard_concept.concept_id 
    LEFT JOIN
        `concept` o_value 
            ON observation.value_as_concept_id = o_value.concept_id 
    LEFT JOIN
        `visit_occurrence` v 
            ON observation.visit_occurrence_id = v.visit_occurrence_id 
    LEFT JOIN
        `concept` o_visit 
            ON v.visit_concept_id = o_visit.concept_id 
    LEFT JOIN
        `concept` o_source_concept 
            ON observation.observation_source_concept_id = o_source_concept.concept_id", sep="")

# Formulate a Cloud Storage destination path for the data exported from BigQuery.
# NOTE: By default data exported multiple times on the same day will overwrite older copies.
#       But data exported on a different days will write to a new location so that historical
#       copies can be kept as the dataset definition is changed.
observation_13140188_path <- file.path(
  Sys.getenv("WORKSPACE_BUCKET"),
  "bq_exports",
  Sys.getenv("OWNER_EMAIL"),
  strftime(lubridate::now(), "%Y%m%d"),  # Comment out this line if you want the export to always overwrite.
  "observation_13140188",
  "observation_13140188_*.csv")
message(str_glue('The data will be written to {observation_13140188_path}. Use this path when reading ',
                 'the data into your notebooks in the future.'))

# Perform the query and export the dataset to Cloud Storage as CSV files.
# NOTE: You only need to run `bq_table_save` once. After that, you can
#       just read data from the CSVs in Cloud Storage.
bq_table_save(
  bq_dataset_query(Sys.getenv("WORKSPACE_CDR"), dataset_13140188_observation_sql, billing = Sys.getenv("GOOGLE_PROJECT")),
  observation_13140188_path,
  destination_format = "CSV")


# Read the data directly from Cloud Storage into memory.
# NOTE: Alternatively you can `gsutil -m cp {observation_13140188_path}` to copy these files
#       to the Jupyter disk.
read_bq_export_from_workspace_bucket <- function(export_path) {
  col_types <- cols(standard_concept_name = col_character(), standard_concept_code = col_character(), value_as_concept_name = col_character(), visit_occurrence_concept_name = col_character(), observation_source_value = col_character(), source_concept_name = col_character(), source_concept_code = col_character(), value_source_value = col_character())
  bind_rows(
    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),
        function(csv) {
          message(str_glue('Loading {csv}.'))
          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)
          if (is.null(col_types)) {
            col_types <- spec(chunk)
          }
          chunk
        }))
}
dataset_13140188_observation_df <- read_bq_export_from_workspace_bucket(observation_13140188_path)

dim(dataset_13140188_observation_df)

head(dataset_13140188_observation_df, 5)
library(tidyverse)
library(bigrquery)

# This query represents dataset "followup_cohort_data" for domain "procedure" and was generated for All of Us Controlled Tier Dataset v7
dataset_13140188_procedure_sql <- paste("
    SELECT
        procedure.person_id,
        p_standard_concept.concept_name as standard_concept_name,
        p_standard_concept.concept_code as standard_concept_code,
        procedure.procedure_datetime,
        procedure.visit_occurrence_id,
        p_visit.concept_name as visit_occurrence_concept_name,
        procedure.procedure_source_value,
        p_source_concept.concept_name as source_concept_name,
        p_source_concept.concept_code as source_concept_code 
    FROM
        ( SELECT
            * 
        FROM
            `procedure_occurrence` procedure 
        WHERE
            (
                procedure_concept_id IN (SELECT
                    DISTINCT c.concept_id 
                FROM
                    `cb_criteria` c 
                JOIN
                    (SELECT
                        CAST(cr.id as string) AS id       
                    FROM
                        `cb_criteria` cr       
                    WHERE
                        concept_id IN (2617378, 4021108, 4088217, 4120120, 4172515, 4197217, 4304536, 4324124)       
                        AND full_text LIKE '%_rank1]%'      ) a 
                        ON (c.path LIKE CONCAT('%.', a.id, '.%') 
                        OR c.path LIKE CONCAT('%.', a.id) 
                        OR c.path LIKE CONCAT(a.id, '.%') 
                        OR c.path = a.id) 
                WHERE
                    is_standard = 1 
                    AND is_selectable = 1) 
                OR  procedure_source_concept_id IN (SELECT
                    DISTINCT c.concept_id 
                FROM
                    `cb_criteria` c 
                JOIN
                    (SELECT
                        CAST(cr.id as string) AS id       
                    FROM
                        `cb_criteria` cr       
                    WHERE
                        concept_id IN (2002208, 2002282, 2003564, 2003608, 2003610, 2003622, 2003623, 2108292, 2109569, 2109574, 2213572, 2213576, 2314206, 2773680, 2776753, 2786488, 2788041, 44820493, 44821578, 44825821, 44832721, 44833130, 45585835, 45609945)       
                        AND full_text LIKE '%_rank1]%'      ) a 
                        ON (c.path LIKE CONCAT('%.', a.id, '.%') 
                        OR c.path LIKE CONCAT('%.', a.id) 
                        OR c.path LIKE CONCAT(a.id, '.%') 
                        OR c.path = a.id) 
                WHERE
                    is_standard = 0 
                    AND is_selectable = 1)
            )  
            AND (
                procedure.PERSON_ID IN (SELECT
                    distinct person_id  
                FROM
                    `cb_search_person` cb_search_person  
                WHERE
                    cb_search_person.person_id IN (SELECT
                        person_id 
                    FROM
                        `person` p 
                    WHERE
                        race_concept_id IN (8527, 8516, 2100000001, 8515, 903096, 2000000008, 45882607, 1177221, 38003615, 8557) ) 
                    AND cb_search_person.person_id NOT IN (SELECT
                        criteria.person_id 
                    FROM
                        (SELECT
                            DISTINCT person_id, entry_date, concept_id 
                        FROM
                            `cb_search_all_events` 
                        WHERE
                            (concept_id IN(SELECT
                                DISTINCT c.concept_id 
                            FROM
                                `cb_criteria` c 
                            JOIN
                                (SELECT
                                    CAST(cr.id as string) AS id       
                                FROM
                                    `cb_criteria` cr       
                                WHERE
                                    concept_id IN (440508)       
                                    AND full_text LIKE '%_rank1]%'      ) a 
                                    ON (c.path LIKE CONCAT('%.', a.id, '.%') 
                                    OR c.path LIKE CONCAT('%.', a.id) 
                                    OR c.path LIKE CONCAT(a.id, '.%') 
                                    OR c.path = a.id) 
                            WHERE
                                is_standard = 1 
                                AND is_selectable = 1) 
                            AND is_standard = 1 )) criteria ) 
                    AND cb_search_person.person_id NOT IN (SELECT
                        person_id 
                    FROM
                        `person` p 
                    WHERE
                        race_concept_id IN (2100000001, 903096, 2000000008, 45882607, 1177221, 38003615, 8557) ) )
            )) procedure 
    LEFT JOIN
        `concept` p_standard_concept 
            ON procedure.procedure_concept_id = p_standard_concept.concept_id 
    LEFT JOIN
        `visit_occurrence` v 
            ON procedure.visit_occurrence_id = v.visit_occurrence_id 
    LEFT JOIN
        `concept` p_visit 
            ON v.visit_concept_id = p_visit.concept_id 
    LEFT JOIN
        `concept` p_source_concept 
            ON procedure.procedure_source_concept_id = p_source_concept.concept_id", sep="")

# Formulate a Cloud Storage destination path for the data exported from BigQuery.
# NOTE: By default data exported multiple times on the same day will overwrite older copies.
#       But data exported on a different days will write to a new location so that historical
#       copies can be kept as the dataset definition is changed.
procedure_13140188_path <- file.path(
  Sys.getenv("WORKSPACE_BUCKET"),
  "bq_exports",
  Sys.getenv("OWNER_EMAIL"),
  strftime(lubridate::now(), "%Y%m%d"),  # Comment out this line if you want the export to always overwrite.
  "procedure_13140188",
  "procedure_13140188_*.csv")
message(str_glue('The data will be written to {procedure_13140188_path}. Use this path when reading ',
                 'the data into your notebooks in the future.'))

# Perform the query and export the dataset to Cloud Storage as CSV files.
# NOTE: You only need to run `bq_table_save` once. After that, you can
#       just read data from the CSVs in Cloud Storage.
bq_table_save(
  bq_dataset_query(Sys.getenv("WORKSPACE_CDR"), dataset_13140188_procedure_sql, billing = Sys.getenv("GOOGLE_PROJECT")),
  procedure_13140188_path,
  destination_format = "CSV")


# Read the data directly from Cloud Storage into memory.
# NOTE: Alternatively you can `gsutil -m cp {procedure_13140188_path}` to copy these files
#       to the Jupyter disk.
read_bq_export_from_workspace_bucket <- function(export_path) {
  col_types <- cols(standard_concept_name = col_character(), standard_concept_code = col_character(), visit_occurrence_concept_name = col_character(), procedure_source_value = col_character(), source_concept_name = col_character(), source_concept_code = col_character())
  bind_rows(
    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),
        function(csv) {
          message(str_glue('Loading {csv}.'))
          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)
          if (is.null(col_types)) {
            col_types <- spec(chunk)
          }
          chunk
        }))
}
dataset_13140188_procedure_df <- read_bq_export_from_workspace_bucket(procedure_13140188_path)

dim(dataset_13140188_procedure_df)

head(dataset_13140188_procedure_df, 5)
```

# Libraries

```{r}
library(knitr)
library(dplyr)
library(tidyr)
library(lubridate)
library(purrr)
library(stringr)

#install.packages("survival")
#install.packages("survminer")

# Load the necessary libraries
library(survival)

# install.packages("RColorBrewer")
library(RColorBrewer)

# Install and load necessary packages
#install.packages("caret")
#install.packages("ggplot2")
library(caret)
library(ggplot2)

# Load necessary libraries
#library(pROC)

```

# Data Wrangling

```{r}
# rename datasets
condition_df <- dataset_13140188_condition_df
device_df <- dataset_13140188_device_df
drug_df <- dataset_13140188_drug_df
measurement_df <- dataset_13140188_measurement_df
observation_df <- dataset_13140188_observation_df
person_df <- dataset_13140188_person_df
procedure_df <- dataset_13140188_procedure_df 


```

## Condition

```{r}
##########################################################################
# condition
# subsetting data: 119 variables
# Define the list of conditions and corresponding codes

conditions <- c(
    "Acidosis", #1
    "Acquired coagulation factor deficiency", #2
    "Acquired absence of limb", #3
    "Acquired hypothyroidism", #4
    "Acute glomerulonephritis", #5
    "Acute renal failure syndrome", #6
    "Altered mental status", #7
    "Amyloidosis", #8
    "Other anemias", #9.1
    "Anemia in chronic kidney disease", #9.2
    "Anemia of chronic disease", #9.3
    "Oliguria and Anuria", #10
    "Arteriovenous fistula", #11
    "Atherosclerosis of the extremities", #12.1
    "Atherosclerosis of renal artery", #12.2
    "Atherosclerosis of coronary artery without angina pectoris", #12.3
    "Bacteremia", #13
    "Changes in skin texture", #14
    "Chronic glomerulonephritis", #15.1
    "Glomerulonephritis", #15.2
    "Chronic graft-versus-host disease", #16
    "Chronic kidney disease stage 1", #17
    "Chronic kidney disease stage 2", #18
    "Chronic kidney disease stage 3", #19
    "Chronic kidney disease stage 4", #20
    "Chronic pain syndrome",#21
    "Chronic renal failure", #22
    "Chronic ulcer of lower extremity", #23
    "Chronic vascular insufficiency of intestine", #24
    "Clostridioides difficile infection", #25
    "Complication associated with insulin pump", #26 #rename insulin pump user
    "Mechanical complication of peritoneal dialysis catheter", #27 #rename peritoneal dialysis
    "Complication of renal dialysis", #28.1
    #"Hemodialysis", #28.2
    #"Congenital anomaly of the kidney", #29
    #"Congenital osteodystrophy", #30
    "Congestive heart failure", #31
    "Other deficiency anemia", #32.1
    "deficiency anemias", #32.2
    "folate-deficiency anemia", #32.3
    "Degenerative skin disorder", #33
    "Diabetes mellitus", #34
    "Disorder of artery", #35
    "Disorder of cardiovascular system", #36
    "Disorder of eye due to type 1 diabetes mellitus", #37
    "Disorder of eye due to type 2 diabetes mellitus", #38
    "Disorder of hard tissues of teeth", #39
    "Disorder of kidney and/or ureter", #40
    "Disorder of mineral metabolism", #41
    "Disorder of muscle", #42
    "Disorder of parathyroid gland", #43
    "Disorder of penis", #44
    "Disorder of phosphate, calcium and vitamin D metabolism", #45
    "Disorder of phosphorus metabolism", #46
    "Disorder of plasma protein metabolism", #47
    "Disorder of porphyrin metabolism", #48
    "Disorder of the urea cycle metabolism", #49
    "End-stage renal disease", #50
    "Essential hypertension", #51
    "Failure to thrive in neonate", #52
    "Frank hematuria", #53
    "Gangrene", #54.1
    "Atherosclerosis of native arteries of the extremities with ulceration or gangrene", #54.2
    "Gout", #55.1
    "Gouty arthropathy", #55.2
    "Granulomatosis with polyangiitis", #56
    "Hydronephrosis", #57
    "Hypercalcemia", #58
    "Hypercoagulability state", #59
    "Hyperkalemia", #60
    "Hyperparathyroidism", #61
    "Hyperparathyroidism due to renal insufficiency", #62
    "Hypertensive complication", #63
    "Hypertensive heart and chronic kidney disease", #64
    "Hypertensive heart AND renal disease", #65
    "Hypertrophy of kidney", #66
    "Hypervolemia", #67
    "Hypocalcemia", #68
    "Hypoglycemia", #69
    "Hypoparathyroidism", #70
    "Hypothyroidism", #71
    "Iatrogenic hypotension", #72
    "Impaction of intestine", #73
    "Impaired renal function disorder", #74
    "Injury of globe of eye", #75
    "Iron deficiency anemias", #76.1
    "Iron deficiency anemias, unspecified or not due to blood loss", #76.2
    "Ketoacidosis due to type 1 diabetes mellitus", #77
    "Ketoacidosis due to type 2 diabetes mellitus", #78
    "Low blood pressure", #79
    "Lupus erythematosus", #80
    "Mechanical complication of cardiac device, implant AND/OR graft", #81
    "Megaloblastic anemia due to folate deficiency", #82
    "Membranous glomerulonephritis", #83
    "Metabolic encephalopathy", #84
    #"Multiple congenital cysts of kidney", #85
    "Myoclonus", #86
    "Nephritic syndrome", #87
    "Nephropathy co-occurrent and due to systemic lupus erythematosus", #88
    "Nephrosclerosis", #89
    "Nephrotic syndrome", #90
    "Non-autoimmune hemolytic anemia", #91
    "Peripheral circulatory disorder due to type 1 diabetes mellitus", #92
    "Peripheral circulatory disorder due to type 2 diabetes mellitus", #93
    "Peripheral vascular complication", #94
    "Peripheral vascular disease", #95
    "Peripheral venous insufficiency", #96
    "Polyneuropathy due to diabetes mellitus", #97
    "Postoperative shock", #98
    "Proteinuria", #99
    "Proliferative glomerulonephritis", #100
    "Renal disorder due to type 1 diabetes mellitus", #101
    "Renal disorder due to type 2 diabetes mellitus", #102
    "Acute renal failure syndrome	", #103.1
    "Renal failure syndrome", #103.2
    "Renal function tests abnormal", #104
    "Renal osteodystrophy", #105
    "Respiratory failure", #106
    "Retinal edema", #107
    "Retinopathy due to diabetes mellitus", #108
    "Screening finding", #109
    "Secondary diabetes mellitus", #110
    "Sepsis", #111.1
    "Sepsis due to Gram negative bacteria", #111.2
    "Gram positive sepsis", #111.3
    "Septic shock", #112
    "Septicemia due to enterococcus", #113
    "Shock",#114
    "Small kidney", #115
    "Systemic lupus erythematosus", #116
    "Systemic sclerosis", #117
    "Thrombotic microangiopathy", #118
    "Transplanted kidney present", #119
    "Tubulointerstitial nephritis", #120
    "Type 1 diabetes mellitus", #121
    "Type 2 diabetes mellitus", #122
    "Vascular insufficiency of intestine" #123
)

phecode <- c(
  "PheCode:276.41", #1
    "PheCode:286.4", #2
    "PheCode:1089", #3
    "PheCode:244.2", #4
    "PheCode:580.13", #5
    "PheCode:585.1", #6
    "PheCode:292.4", #7
    "PheCode:270.33", #8
    "PheCode:285", #9.1
    "PheCode:285.21", #9.2
    "PheCode:285.2", #9.3
    "PheCode:599.6", #10
    "CCS:57", #11
    "PheCode:440.2", #12.1
    "PheCode:440.1", #12.2
    "PheCode:440.21", #12.3 
    "PheCode:038.3", #13
    "PheCode:687.3", #14
    "PheCode:580.14", #15.1
    "PheCode:580.1", #15.2
    "PheCode:081.12", #16
    "PheCode:585.4", #17
    "PheCode:585.4", #18
    "PheCode:585.33", #19
    "PheCode:585.34", #20
    "PheCode:355.1",#21
    "PheCode:585.3", #22
    "PheCode:707.3", #23
    "PheCode:441.2", #24
    "PheCode:008.52", #25
    "PheCode:250.3", #26 
    "CCS:91", #27 #Peritoneal dialysis
    "PheCode:585.31", #28.1 #Renal dialysis #check data values
    #"CCS:58", #28.2 #Hemodialysis
    #"Congenital anomaly of the kidney", #29
    #"Congenital osteodystrophy", #30
    "PheCode:428", #31
    "PheCode:281", #32.1
    "PheCode:281.9", #32.2
    "PheCode:281.13", #32.3
    "PheCode:702.4", #33
    "PheCode:250", #34
    "PheCode:447", #35
    "PheCode:459", #36
    "PheCode:250.13", #37
    "PheCode:250.23", #38
    "PheCode:521", #39
    "PheCode:586.1", #40
    "PheCode:275", #41
    "PheCode:359.2", #42
    "PheCode:252", #43
    "PheCode:604", #44
    "PheCode:275.5", #45
    "PheCode:275.53", #46
    "PheCode:270.38", #47
    "PheCode:277.1", #48
    "PheCode:270.21", #49
    "PheCode:585.32", #50
    "PheCode:401.1", #51
    "PheCode:264.2", #52
    "PheCode:593.1", #53
    "PheCode:791", #54.1
    "PheCode:440.21", #54.2
    "PheCode:274.1", #55.1
    "PheCode: 274.11", #55.2
    "PheCode:446.4", #56
    "PheCode:595", #57
    "PheCode:275.51", #58
    "PheCode:286.8", #59
    "PheCode:276.13", #60
    "PheCode:252.1", #61
    "PheCode:588.2", #62
    "PheCode:401.3", #63
    "PheCode:401.22", #64
    "PheCode:401.2", #65
    "PheCode:586.3", #66
    "PheCode:276.6", #67
    "PheCode:275.51", #68
    "PheCode:251.1", #69
    "PheCode:252.2", #70
    "PheCode:244", #71
    "PheCode:458.2", #72
    "PheCode:560.2", #73
    "PheCode:588", #74
    "PheCode:360", #75
    "PheCode: 280", #76.1
    "PheCode: 280.1", #76.2
    "PheCode:250.11", #77
    "PheCode:250.21", #78
    "PheCode:458.9", #79
    "PheCode:695.4", #80
    "PheCode:854", #81
    "PheCode:281.13", #82
    "PheCode:580.12", #83
    "PheCode:348.8", #84
    #"Multiple congenital cysts of kidney", #85
    "PheCode:333.2", #86
    "PheCode:580.32", #87
    "PheCode:580.31", #88
    "PheCode:580.4", #89
    "PheCode:580.2", #90
    "PheCode:283.2", #91
    "PheCode:250.15", #92
    "PheCode:250.25", #93
    "PheCode:443.8", #94
    "PheCode:443.9", #95
    "PheCode:456", #96
    "PheCode:250.6", #97
    "PheCode:958.1", #98
    "PheCode:269", #99
    "PheCode:580.11", #100
    "PheCode:250.12", #101
    "PheCode:250.22", #102
    "PheCode:585", #103.1
    "PheCode:585.2", #103.2
    "PheCode:589", #104
    "PheCode:588.1", #105
    "PheCode:509.1", #106
    "PheCode:362.9", #107
    "PheCode:250.7", #108
    "PheCode:1010.1", #109
    "PheCode:249", #110
    "PheCode:994.2", #111.1
    "PheCode:038.1", #111.2
    "PheCode:038.2", #111.3
    "PheCode:994.21", #112
    "PheCode:038", #113
    "PheCode:797",#114
    "PheCode:586.11", #115
    "PheCode:695.42", #116
    "PheCode:709.3", #117
    "PheCode:446.8", #118
    "PheCode:587", #119
    "PheCode:580.3", #120
    "PheCode:250.1", #121
    "PheCode:250.2", #122
    "PheCode:441" #123
)



# Corresponding ICD codes for each condition
snomed_codes <- list(
  51387008, #1
  25904003, #2
  816117000, #3
  40930008, #4
  19351000, #5
  14669001, #6
  419284004, #7
  17602002, #8
  271737000, #9.1
  707323002, #9.2
  234347009, #9.3
  c(83128009,2472002), #10
  439470001, #11
  51274000, #12.1
  45281005, #12.2
  451041000124103, #12.3
  5758002, #13
  274672009, #14
  20917003, #15.1
  36171008, #15.2
  402356004, #16
  431855005, #17
  431856006, #18
  433144002, #19
  431857002, #20
  373621006, #21
  90688005, #22
  26649005, #23
  111354009, #24
  186431008, #25
  473033004, #26
  431028002, #27 
  33603003, #28.1
  # 302497006, #28.2
  # 110182004, #29
  # 348350001, #30
  42343007, #31
  66612000, #32.1
  267513007, #32.2
  85649008, #32.3
  396325007, #33
  73211009, #34
  359557001, #35
  49601007, #36
  739681000, #37
  422099009, #38
  46557008, #39
  443820000, #40
  45744005, #41
  129565002, #42
  73132005, #43
  33958003, #44
  237879001, #45
  87049008, #46
  147211000119101, #47
  29094004, #48
  36444000, #49
  46177005, #50
  59621000, #51
  134251000119105, #52
  197941005, #53
  372070002, #54.1
  15649901000119104, #54.2
  90560007, #55.1
  190828008, #55.2
  195353004, #56
  43064006, #57
  66931009, #58
  76612001, #59
  14140009, #60
  66999008, #61
  19034001, #62
  449759005, #63
  8501000119104, #64
  86234004, #65
  88531004, #66
  21639008, #67
  5291005, #68
  302866003, #69
  36976004, #70
  40930008, #71
  408668005, #72
  62851005, #73
  197663003, #74
  231794000, #75
  87522002, #76.1
  724556004, #76.2
  420270002, #77
  421750000, #78
  45007003, #79
  200936003, #80
  473041004, #81
  85649008, #82
  77182004, #83
  50122000, #84
  # 82525005, #85
  17450006, #86
  7724006, #87
  295101000119105, #88
  32916005, #89
  52254009, #90
  4854004, #91
  421365002, #92
  422166005, #93
  10596002, #94
  400047006, #95
  20696009, #96
  49455004, #97
  58581001, #98
  29738008, #99
  441815006, #100
  421893009, #101
  420279001, #102
  14669001, #103.1
  42399005, #103.2
  167180005, #104
  16726004, #105
  409622000, #106
  6141006, #107
  4855003, #108
  365856005, #109
  8801005, #110
  91302008, #111.1
  449082003, #111.2
  194394004, #111.3
  76571007, #112
  310669007, #113
  27942005, #114
  236448000, #115
  55464009, #116
  89155008, #117
  126729006, #118
  737295003, #119
  428255004, #120
  46635009, #121
  44054006, #122
  82196007 #123
)
```

### Proteinuria

```{r}
# Filter the rows in condition_df based on the given criteria
filtered_rows <- condition_df %>%
  filter(
    source_concept_code %in% c("29738008") |
    standard_concept_name == "Proteinuria"
  )

# Print the filtered rows
print(filtered_rows)

# Count the number of unique person_id
num_unique_persons <- filtered_rows %>%
  summarise(num_unique_ids = n_distinct(person_id))

# Print the number of unique person_id
print(paste("Number of unique person_id:", num_unique_persons$num_unique_ids))

# Count the number of times each unique person_id appears
person_id_counts <- filtered_rows %>%
  group_by(person_id) %>%
  summarise(count = n()) %>%
  arrange(desc(count))

# Print the count for each person_id
print(person_id_counts)

```

### Without Roll-up Count, select on both SNOMED and ICD:

The code snippet will only match variables that exactly match the codes in `snomed_code` and `icd_codes_list`.

```{r}
# Create a dataframe to map conditions, SNOMED, ICD codes, and Phecodes
conditions_df <- data.frame(
  condition = I(conditions),  # This column contains the actual condition names
  phecode = phecode,
  snomed_code = I(sapply(snomed_codes, function(x) toString(x)))
)

# Initialize an empty dataframe to store the results
filtered_conditions <- data.frame()

# Loop through each condition and filter the dataset
for (i in 1:nrow(conditions_df)) {
  condition_name <- conditions_df$condition[i]  # Now we're using the condition name instead of phecode
  snomed_code <- unlist(strsplit(as.character(conditions_df$snomed_code[i]), ", "))

  temp_df <- condition_df %>%
    filter(standard_concept_code %in% snomed_code) %>%
    mutate(new_name = condition_name)  # Assigning the condition name as new_name

  filtered_conditions <- rbind(filtered_conditions, temp_df)
}

# Count the number of unique person_id for each variable in new_name (now condition name)
unique_counts <- filtered_conditions %>%
  group_by(new_name) %>%
  summarize(unique_person_count = n_distinct(person_id))

# View the result
print(unique_counts)


```

### ESRD

```{r}
# Identify person_id with End-stage renal disease and get the earliest condition_start_datetime for each person_id
unique(filtered_conditions$standard_concept_name)

condition_filtered_ESRD <- filtered_conditions %>%
  filter(new_name == "End-stage renal disease") %>%
  group_by(person_id) %>%
  summarize(disease_date = min(condition_start_datetime)) %>%
  ungroup()

# Add columns disease_status and disease_date to the person dataset
person <- person_df %>%
  left_join(condition_filtered_ESRD, by = "person_id") %>%
  mutate(
    disease_status = if_else(!is.na(disease_date), 1, 0)
  )

# Count the number of unique person_id where disease_status is 1
num_ESRD <- person %>%
  filter(disease_status == 1) %>%
  summarise(count = n_distinct(person_id)) %>%
  pull(count)

# Print the number of unique person_id with disease_status == 1
print(paste("Number of unique person_id with disease_status == 1:", num_ESRD))

# Print the resulting person dataset
print(person)
```

### ESRD by race

```{r}
# Identify person_id with End-stage renal disease and get the earliest condition_start_datetime for each person_id
condition_filtered_ESRD <- filtered_conditions %>%
  filter(new_name == "End-stage renal disease") %>%
  group_by(person_id) %>%
  summarize(disease_date = min(condition_start_datetime)) %>%
  ungroup()

# Add columns disease_status and disease_date to the person dataset
person <- person_df %>%
  left_join(condition_filtered_ESRD, by = "person_id") %>%
  mutate(
    disease_status = if_else(!is.na(disease_date), 1, 0)
  )

# Count the number of cases (disease_status == 1) and controls (disease_status == 0) by race
case_control_counts <- person %>%
  group_by(race, disease_status) %>%
  summarise(count = n_distinct(person_id), .groups = 'drop')

# Separate cases and controls for printing
cases <- case_control_counts %>%
  filter(disease_status == 1)

controls <- case_control_counts %>%
  filter(disease_status == 0)

# Print the number of cases and controls by race
print("Number of cases (disease_status == 1) by race:")
print(cases)

print("Number of controls (disease_status == 0) by race:")
print(controls)

```

## Device

```{r}
##########################################################################
# Define HCPCS code and the corresponding device name
HCPCS_code <- list("E1399")
device <- list("Durable medical equipment")

# Create a dataframe to map HCPCS code and device name
devices_df <- data.frame(
  HCPCS_code = unlist(HCPCS_code),
  device_name = unlist(device)
)

# Filter the dataset based on HCPCS codes and rename the selected variables
device_filtered <- device_df %>%
  filter(standard_concept_code %in% devices_df$HCPCS_code) %>%
  mutate(new_name = devices_df$device_name)

# View the filtered dataset with renamed variables
print(device_filtered)

# Summarize the number of unique person_id for each new_name
unique_person_id_count <- device_filtered %>%
  group_by(new_name) %>%
  summarize(unique_person_count = n_distinct(person_id))

# Print the summary table
print(unique_person_id_count)

```

## Drug

```{r}
##########################################################################
# drug

# Group by standard_concept_name and summarize the count of each
drug_summary <- drug_df %>%
  group_by(standard_concept_name) %>%
  summarize(count = n()) %>%
  arrange(desc(count))

# Create a vector containing all unique variables from the provided images
medication <- c(
  "aliskiren", #1
  "alteplase", #2
  "aluminum hydroxide", #3
  "amlodipine", #4
  "calcitriol", #5
  "calcium acetate", #6
  "carvedilol", #7
  "cascara sagrada", #8
  "ceftazidime", #9
  "cinacalcet", #10
  "citric acid", #11
  "daptomycin", #12
  "darbepoetin alfa", #13
  "dextran", #14
  "dextran 70", #15 #unavailable
  "dextran 75", #16
  "doxercalciferol", #17
  "elbasvir", #18
  "emtricitabine", #19
  "epoetin alfa", #20
  "etelcalcetide", #21
  "ethyl chloride", #22
  "etravirine", #23
  "grazoprevir", #24
  "heparin", #25
  "hepatitis B immune globulin", #26
  "hepatitis B surface antigen vaccine", #27
  "hydralazine", #28
  "insulin aspart protamine, human", #29
  "insulin aspart, human", #30
  "insulin detemir", #31
  "insulin glargine", #32
  "insulin glulisine, human", #33
  "insulin isophane", #34
  "insulin lispro", #35
  "insulin lispro protamine, human", #36
  "insulin, regular, human", #37
  "iron sucrose", #38
  "iron-dextran complex", #39
  "lanthanum", #40
  "mannitol", #41
  "methoxy polyethylene glycol-epoetin beta", #42 #unavailable
  "midodrine", #43
  "minoxidil", #44
  "nevirapine", #45
  "nifedipine", #46
  "paricalcitol", #47
  "pitavastatin", #48
  "protriptyline", #49
  "saquinavir", #50
  "sevelamer", #51
  "sodium bicarbonate", #52
  "sodium citrate", #53
  "sodium ferric gluconate complex", #54
  "sodium polystyrene sulfonate", #55
  "vancomycin", #56
  "water" #57
)

RXNORM_codes <- c(
  325646, #1
  8410, #2
  612, #3
  17767, #4
  1894, #5
  214342, #6
  20352, #7
  66869, #8
  2191, #9
  407990, #10
  21183, #11
  22299, #12
  283838, #13
  42635, #14
  3274, #15
  3275, #16
  11516, #17
  1734628, #18
  276237, #19
  105694, #20
  1876119, #21
  4141, #22
  475969, #23
  1734630, #24
  5224, #25
  26744, #26
  797752, #27
  5470, #28
  352385, #29
  51428, #30
  139825, #31
  274783, #32
  400008, #33
  1605101, #34
  86009, #35
  314684, #36
  253182, #37
  24909, #38
  5992, #39
  1311070, #40
  6628, #41
  729596, #42
  6963, #43
  6984, #44
  53654, #45
  7417, #46
  73710, #47
  861634, #48
  8886, #49
  83395, #50
  214824, #51
  36676, #52
  56466, #53
  261435, #54
  56512, #55
  11124, #56
  11295 #56
)
# Create a dataframe to map medication names and RXNORM codes
medication_df <- data.frame(
  medication = medication,
  RXNORM_code = as.character(RXNORM_codes)
)

# Ensure standard_concept_code in drug_df is character type
drug_df <- drug_df %>%
  mutate(standard_concept_code = as.character(standard_concept_code))

# Filter the dataset based on RXNORM codes and rename the selected variables
drug_filtered <- drug_df %>%
  filter(standard_concept_code %in% medication_df$RXNORM_code) %>%
  left_join(medication_df, by = c("standard_concept_code" = "RXNORM_code")) %>%
  mutate(new_name = medication)

# View the filtered dataset with renamed variables
print(drug_filtered)

# Find variables in medication list that are not in the drug_filtered$standard_concept_name column
not_in_drug_filtered <- setdiff(medication, drug_filtered$new_name)

# Print the result
print(not_in_drug_filtered)


# Summarize the data to get the unique count of person_id and total count for each variable
summary_table <- drug_filtered %>%
  group_by(new_name) %>%
  summarise(
    unique_person_id_count = n_distinct(person_id),
    count = n()
  )

# Print the summary table
print(summary_table)

```

## Measurement

```{r}
##########################################################################
# measurement

# Create a vector containing all unique variables from the provided images
measurements <- list(
  "Administration of medication", #1
  #"Arteriovenous anastomosis for renal dialysis", #2
  "Bilirubin.indirect [Mass/volume] in Serum or Plasma", #3
  "Calcium.ionized [Moles/volume] in Serum or Plasma", #4
  "Creatinine [Mass/volume] in Body fluid", #5.1
  "Cystatin C [Mass/volume] in Serum or Plasma", #5.2  
  "Erythrocyte distribution width [Ratio] by Automated count", #6
  "Ferritin [Mass/volume] in Serum or Plasma", #7
  "Fructosamine [Moles/volume] in Serum or Plasma", #8
  #"Hemodialysis", #9
  "Hepatitis B virus core IgG Ab [Units/volume] in Serum by Immunoassay", #10
  "Hepatitis B virus surface Ab [Units/volume] in Serum", #11
  "Hepatitis B virus surface Ab [Presence] in Serum by Immunoassay", #12
  "Hepatitis B virus surface Ab [Units/volume] in Serum", #13
  "Hepatitis B virus surface Ab [Units/volume] in Serum by Radioimmunoassay (RIA)", #14
  "Hepatitis B virus surface Ag [Presence] in Serum or Plasma by Immunoassay", #15
  "Hepatitis C virus Ab [Presence] in Serum or Plasma by Immunoassay", #16
  "Hepatitis C virus Ab [Presence] in Serum by Immunoblot", #17
  "HLA Ab [Presence] in Serum", #18
  "Iron binding capacity [Mass/volume] in Serum or Plasma", #19
  "Iron [Mass/volume] in Serum or Plasma", #20
  "Iron saturation [Mass Fraction] in Serum or Plasma", #21
  "Nucleated erythrocytes [#/volume] in Body fluid by Manual count", #22
  "Parathyrin.intact [Mass/volume] in Serum or Plasma", #23
  "Parathyroid Hormones", #24
  #"Partial nephrectomy", #25
  #"Peritoneal dialysis", #26
  "Protein [Mass/volume] in Urine", #27
  "Reticulocytes [#/volume] in Blood", #28
  "Reticulocytes/100 erythrocytes in Blood", #29
  "Therapeutic procedure", #30
  #"Total nephrectomy", #31
  "Iron binding capacity [Mass/volume] in Serum or Plasma", #32
  "Transferrin [Mass/volume] in Serum or Plasma", #33
  "Urate [Mass/volume] in Serum or Plasma", #34
  "Vancomycin [Mass/volume] in Serum or Plasma", #35


  "Glomerular filtration rate/1.73 sq M.predicted [Volume Rate/Area] in Serum, Plasma or Blood by Creatinine-based formula (MDRD)", #36
  "Hemoglobin A1c/Hemoglobin.total in Blood", #37
  "Systolic blood pressure", #38
  "Triglyceride [Mass/volume] in Serum or Plasma", #39
  
  "Albumin/Creatinine [Mass Ratio] in Urine", #40
  "Microalbumin/Creatinine [Mass Ratio] in Urine", #41.1
  "Albumin/Creatinine [Mass Ratio] in 24 hour Urine", #41.2
  "Albumin/Creatinine [Molar ratio] in Urine" #41.3
)

measurement_code <- list(
  "18629005", #1
  #"79827002", #2
  "1971-1", #3
  "1995-0", #4
  "2160-0", #5.1
  "33863-2", #5.2
  "788-0", #6
  "2276-4", #7
  "15069-8", #8
  #"302497006", #9
  "13919-6", #10
  "22322-2", #11
  "10900-9", #12
  "16935-9", #13
  "5194-6", #14
  "5196-1", #15
  "13955-0", #16
  "5199-5", #17
  "44534-6", #18
  "2500-7", #19
  "2498-4", #20
  "2502-3", #21
  "13530-1", #22
  "2731-8", #23
  "LP31659-3", #24
  #"81516001", #25
  #"71192002", #26
  "2888-6", #27
  "14196-0", #28
  "4679-7", #29
  "277132007", #30
  #"175905003", #31
  "359979000", #32
  "3034-6", #33
  "3084-1", #34
  "20578-1", #35
  
  
  "77147-7", #36
  "4548-4", #37
  "8480-6", #38
  "2571-8", #39
  
  "9318-7", #40
  "14959-1", #40.1
  "13705-9", #40.2
  "14585-4" #40.3
)


# Create a dataframe to map measurements and their codes
measurements_df <- data.frame(
  measurement = unlist(measurements),
  measurement_code = unlist(measurement_code)
)

# Filter the measurement_df based on measurement_code and create a new column with the corresponding variable names
filtered_measurements <- measurement_df %>%
  filter(standard_concept_code %in% measurements_df$measurement_code | source_concept_code %in% measurements_df$measurement_code) %>%
  left_join(measurements_df, by = c("standard_concept_code" = "measurement_code")) %>%
  mutate(new_name = measurement)


# View the filtered dataset with renamed variables
print(head(filtered_measurements))


# Find variables in diagnostics list that are not in the measurement_filtered_1$standard_concept_name column
not_in_measurement_filtered <- setdiff(measurements, filtered_measurements$new_name)

# Print the result
print(not_in_measurement_filtered)


# Summarize the data to get the unique person_id and count for each variable
summary_table <- filtered_measurements %>%
  group_by(standard_concept_name) %>%
  summarise(
    unique_person_id_count = n_distinct(person_id),
    count = n()
  )

# Print the summary table
print(summary_table)

```

#### BMI

```{r}
# Step 1: Filter the measurement data for height and weight
body_measurements <- measurement_df %>%
  filter(standard_concept_name %in% c("Body height", "Body weight"))

# Step 2: Reshape the data to have height and weight in separate columns
body_measurements_wide <- body_measurements %>%
  select(person_id, standard_concept_name, value_as_number) %>%
  spread(key = standard_concept_name, value = value_as_number)

# Step 3: Replace non-positive values with NA for height and weight and rename columns
body_measurements_wide <- body_measurements_wide %>%
  mutate(
    height = ifelse(`Body height` > 0, `Body height`, NA),   # Keep only positive height values and rename
    weight = ifelse(`Body weight` > 0, `Body weight`, NA)    # Keep only positive weight values and rename
  ) %>%
  select(person_id, height, weight)  # Keep only renamed columns

# Step 4: Calculate BMI for each person_id (BMI = weight(kg) / height(m)^2)
bmi_table <- body_measurements_wide %>%
  mutate(
    BMI = ifelse(!is.na(height) & !is.na(weight), 
                 weight / ((height / 100) ^ 2), NA)  # Convert height to meters for BMI calculation
  ) %>%
  select(person_id, height, weight, BMI)

# Step 5: Join the height, weight, and BMI data back to the person dataset
# Ensure all columns from person_df are retained
person <- person %>%
  left_join(bmi_table, by = "person_id")  # Merge height, weight, and BMI

# Print the resulting dataset to verify
print(person)

```

```{r}
library(dplyr)
library(tidyr)

# Step 1: Define the BMI categories
bmi_table <- bmi_table %>%
  mutate(
    BMI_category = case_when(
      BMI < 20 ~ "< 20",
      BMI >= 20 & BMI < 25 ~ "20 to < 25",
      BMI >= 25 & BMI < 30 ~ "25 to < 30",
      BMI >= 30 ~ "≥ 30",
      TRUE ~ NA_character_
    )
  )

# Step 2: Join BMI categories with the person dataset (which should include race)
person_with_bmi <- person %>%
  left_join(bmi_table %>% select(person_id, BMI_category), by = "person_id")

# Step 3: Count the number of people in each BMI category, by race
bmi_summary <- person_with_bmi %>%
  group_by(race, BMI_category) %>%
  summarise(
    count = n(),
    .groups = 'drop'
  )

# Step 4: Calculate the percentage for each group
bmi_summary <- bmi_summary %>%
  group_by(race) %>%
  mutate(percentage = (count / sum(count)) * 100)

# Step 5: Reshape the data for easier viewing in table format
bmi_summary_table <- bmi_summary %>%
  pivot_wider(
    names_from = BMI_category,
    values_from = c(count, percentage),
    names_glue = "{BMI_category}_{.value}"  # To distinguish between count and percentage columns
  )

# Step 6: Calculate the overall summary (for "All" race category)
bmi_summary_overall <- person_with_bmi %>%
  group_by(BMI_category) %>%
  summarise(
    count = n(),
    .groups = 'drop'
  ) %>%
  mutate(percentage = (count / sum(count)) * 100) %>%
  mutate(race = "All")

# Step 7: Combine the overall summary with the race-specific summary
bmi_summary_final <- bind_rows(
  bmi_summary_overall %>%
    pivot_wider(names_from = BMI_category, values_from = c(count, percentage), names_glue = "{BMI_category}_{.value}"),
  bmi_summary_table
)

# Step 8: Print the final summary
print(bmi_summary_final)


```

## Observation

```{r}
##########################################################################
# observation

## note: may not be used in the future due to missing proportion. Code missing as unknown.

# Define the concept name and LOINC code to filter
concept_name <- "Tobacco smoking status"
LOINC_code <- "72166-2"

# Filter the observation data frame to keep only rows where standard_concept_code matches the LOINC code
observation_filtered <- observation_df %>%
  filter(standard_concept_code == LOINC_code)

# Print the filtered data
print(observation_filtered)

# Summarize the data to get the unique count of person_id and total count for the concept name
summary_table <- observation_filtered %>%
  group_by(standard_concept_name) %>%
  summarise(
    unique_person_id_count = n_distinct(person_id),
    count = n()
  )

# Print the summary table
print(summary_table)

```

#### Smoking levels

```{r}
# Assuming observation_filtered is your dataframe, get the unique values
unique_values <- observation_filtered %>%
  select(value_as_concept_name, value_as_number) %>%
  distinct()

# View the result
print(unique_values)

```

#### Ranking smoking status

```{r}
# check if once OR LITERATURE, CHECK IF MEASURED AT BASELINE, REMOVE SMOKING SMOKING IF NOT CONFIDENT THAT MEASURED AT BASELINE, GROUP AVEG OR NON SMOKER
# Define the ranks for each smoking status
smoking_status_rank <- data.frame(
  value_as_concept_name = c(
    "Never smoker",
    "Never smoked tobacco",
    "Unknown if ever smoked", 
    "Former smoker",
    "Ex-smoker",
    "Current some day smoker",
    "Occasional tobacco smoker",
    "Light tobacco smoker",
    "Current every day smoker",
    "Smokes tobacco daily",
    "Smoker",
    "Smoker, current status unknown",
    "Heavy tobacco smoker"
  ),
  smoking_status_rank = c(
    1, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, 11
  )
)

# Merge the rank information with the original dataframe
observation_filtered <- observation_filtered %>%
  left_join(smoking_status_rank, by = "value_as_concept_name")

# View the result
print(observation_filtered)

```

## Procedure

```{r}
##########################################################################
# procedure

# Define the list of diagnostic procedures
procedure_list <- c(
  "Arteriovenous anastomosis for renal dialysis", #1
  "Hemodialysis", #2
  "Nephrectomy", #3
  "Peritoneal dialysis", #4
  "Direct skilled nursing services of a registered nurse (rn) in the home health or hospice setting, each 15 minutes" #5
  
)

procedure_codes <- list(
  79827002, #1
  302497006, #2
  c(81516001, 175905003), #3
  71192002, #4

  79827002 #5
)


# Determine the maximum length among all lists
max_length <- max(length(procedure_list), max(lengths(procedure_codes)))

# Extend lists to match the maximum length
procedure_list <- rep(procedure_list, length.out = max_length)
procedure_codes <- unlist(lapply(procedure_codes, function(x) paste(x, collapse = ", ")))  # Properly join multiple codes

# Create the data frame
procedures_df <- data.frame(
  procedure = procedure_list,
  procedure_code = procedure_codes,  # Already character due to collapsing
  stringsAsFactors = FALSE
)

# Convert standard_concept_code to character for matching
procedure_df <- procedure_df %>%
  mutate(
    standard_concept_code = as.character(standard_concept_code)
  )

# Initialize an empty dataframe to store the results
filtered_procedures <- data.frame()

# Loop through each procedure and filter the dataset
for (i in 1:nrow(procedures_df)) {
  procedure_name <- procedures_df$procedure[i]
  procedure_code <- unlist(strsplit(as.character(procedures_df$procedure_code[i]), ", "))

  temp_df <- procedure_df %>%
    filter(standard_concept_code %in% procedure_code) %>%
    mutate(new_name = procedure_name)

  filtered_procedures <- rbind(filtered_procedures, temp_df)
}

# Print the resulting data frame
print(filtered_procedures)

# Check the variables in filtered_procedures that are not in procedure_list
not_in_diagnostic_list <- setdiff(filtered_procedures$new_name, procedure_list)

# Print the variables that are not in diagnostic_list
print("Variables in filtered_procedures that are not in diagnostic_list:")
print(not_in_diagnostic_list)

# Summarize the data to get the unique person_id and count for each variable
summary_table <- filtered_procedures %>%
  group_by(new_name) %>%
  summarise(
    unique_person_id_count = n_distinct(person_id),
    count = n()
  )

# Print the summary table
print(summary_table)

```

# Creatnine_distribution

```{r}
# Load required library
library(dplyr)

# Target Population: eGFR
# Define the specific standard concept name to filter
concept_code <- "2160-0"

# Filter the measurement_filtered data frame to include only the relevant concept code
measurement_filtered_specific <- filtered_measurements %>%
  filter(standard_concept_code == concept_code)

# Filter to get the first non-NA measurement for each person_id
first_measurements <- measurement_filtered_specific %>%
  group_by(person_id) %>%
  filter(!is.na(value_as_number)) %>%  # Exclude rows where value_as_number is NA
  arrange(measurement_datetime) %>%
  slice(1) %>%
  ungroup()

# Print the resulting data frame
print(first_measurements)

# Attach individual specific race
# Function to add race column to a data frame based on person_id
add_race_column <- function(df, person_df) {
  df <- df %>%
    left_join(person_df %>% select(person_id, race), by = "person_id")
  return(df)
}

# Add race information to the first_measurements data frame
first_measurements <- add_race_column(first_measurements, person_df)

# Calculate the count of NA, non-NA, and creatinine values above 300, and group by race
group_summary_race <- first_measurements %>%
  group_by(race) %>%
  summarise(
    total_count = n(),                            # Total number of measurements
    na_count = sum(is.na(value_as_number)),       # Count of NA values in eGFR
    actual_count = sum(!is.na(value_as_number)),  # Count of non-NA values (will be equal to total_count since we've already removed NAs)
    above_300_count = sum(value_as_number > 300, na.rm = TRUE)  # Count of values above 300
  ) %>%
  ungroup() %>%
  mutate(
    percentage_na = (na_count / total_count) * 100,             # Percentage of NA values
    percentage_actual = (actual_count / total_count) * 100,     # Percentage of non-NA values
    percentage_above_300 = (above_300_count / total_count) * 100 # Percentage of values above 300
  )

# Calculate the same summary for all participants (ALL group)
group_summary_all <- first_measurements %>%
  summarise(
    total_count = n(),                            # Total number of measurements
    na_count = sum(is.na(value_as_number)),       # Count of NA values in eGFR
    actual_count = sum(!is.na(value_as_number)),  # Count of non-NA values
    above_300_count = sum(value_as_number > 300, na.rm = TRUE)  # Count of values above 300
  ) %>%
  mutate(
    race = "ALL",                                             # Label for ALL group
    percentage_na = (na_count / total_count) * 100,           # Percentage of NA values
    percentage_actual = (actual_count / total_count) * 100,   # Percentage of non-NA values
    percentage_above_300 = (above_300_count / total_count) * 100 # Percentage of values above 300
  )

# Combine the ALL group with the race-specific summary
group_summary <- bind_rows(group_summary_race, group_summary_all)

# Print the summary table
print("Group summary with NA, actual eGFR values, and values above 300 by race and for all participants:")
print(group_summary)

```

### Index date

```{r}
# Identify the first measurement for each person_id
first_measurements <- first_measurements %>%
  group_by(person_id) %>%
  arrange(measurement_datetime) %>%
  slice(1) %>%
  ungroup() %>%
  select(person_id, measurement_datetime, value_as_number) %>%
  rename(index_date = measurement_datetime, serum_creatinine = value_as_number)

# Add the index_date and serum_creatinine to the person dataset
person <- person %>%
  left_join(first_measurements, by = "person_id")

person <- person %>%
  mutate(index_date = as.Date(index_date))
```

## Index date count

```{r}
# Load dplyr library for data manipulation
library(dplyr)

# Ensure that index_date is in Date format
person <- person %>%
  mutate(index_date = as.Date(index_date))

# Count the number of person_id with non-NA index_date values
non_na_index_date_count <- person %>%
  filter(!is.na(index_date)) %>%
  summarise(count = n_distinct(person_id))

# Print the result
cat("Number of person_id with non-NA index_date values:", non_na_index_date_count$count, "\n")

```

# Save Person Data sets

```{r}
# Save the dataframes to CSV files
write.csv(person, "person.csv", row.names = FALSE)
```

### Index date summary statistics

```{r}
# Load required library
library(dplyr)

# Calculate the count of NA, non-NA, creatinine values above 300, and group by race along with actual cases and controls
group_summary_race <- person %>%
  group_by(race) %>%
  summarise(
    total_count = n(),                              # Total number of measurements
    na_count = sum(is.na(serum_creatinine)),        # Count of NA values in serum_creatinine
    actual_count = sum(!is.na(serum_creatinine)),   # Count of non-NA values
    above_300_count = sum(serum_creatinine > 300, na.rm = TRUE),  # Count of values above 300
    actual_cases = sum(disease_status == 1 & !is.na(serum_creatinine)),   # Count of cases with non-NA serum_creatinine
    actual_controls = sum(disease_status == 0 & !is.na(serum_creatinine)) # Count of controls with non-NA serum_creatinine
  ) %>%
  ungroup() %>%
  mutate(
    percentage_na = (na_count / total_count) * 100,             # Percentage of NA values
    percentage_actual = (actual_count / total_count) * 100,     # Percentage of non-NA values
    percentage_above_300 = (above_300_count / total_count) * 100 # Percentage of values above 300
  )

# Calculate the same summary for all participants (ALL group)
group_summary_all <- person %>%
  summarise(
    total_count = n(),                              # Total number of measurements
    na_count = sum(is.na(serum_creatinine)),        # Count of NA values in serum_creatinine
    actual_count = sum(!is.na(serum_creatinine)),   # Count of non-NA values
    above_300_count = sum(serum_creatinine > 300, na.rm = TRUE),  # Count of values above 300
    actual_cases = sum(disease_status == 1 & !is.na(serum_creatinine)),   # Count of cases with non-NA serum_creatinine
    actual_controls = sum(disease_status == 0 & !is.na(serum_creatinine)) # Count of controls with non-NA serum_creatinine
  ) %>%
  mutate(
    race = "ALL",                                             # Label for ALL group
    percentage_na = (na_count / total_count) * 100,           # Percentage of NA values
    percentage_actual = (actual_count / total_count) * 100,   # Percentage of non-NA values
    percentage_above_300 = (above_300_count / total_count) * 100 # Percentage of values above 300
  )

# Combine the ALL group with the race-specific summary
group_summary <- bind_rows(group_summary_race, group_summary_all)

# Print the updated summary table including cases and controls with non-NA values
print("Group summary with NA, actual serum_creatinine values, values above 300, and actual cases/controls by race and for all participants:")
print(group_summary)


```

#### Table for Outliers by Race (Serum Creatinine = 1e+07)

```{r}
# Table 1: Count of people with outlier serum creatinine value of 1e+07 by race
creatinine_outlier_count <- person %>%
  filter(serum_creatinine == 1e+07) %>%
  group_by(race) %>%
  summarise(count = n()) 

# Print the result
print("Table 1: Count of people with outlier serum creatinine (1e+07) by race")
print(creatinine_outlier_count)

```

#### Table for Overall Summary Statistics

```{r}
# Table 2: Summary statistics for the entire dataset (after removing outliers)
# Filter out rows where serum_creatinine is equal to 1e+07
person_filtered <- person %>%
  filter(serum_creatinine != 1e+07)

summary_statistics <- person_filtered %>%
  summarise(
    earliest_index_date = min(index_date, na.rm = TRUE),
    latest_index_date = max(index_date, na.rm = TRUE),
    total_participants = n(),
    median_index_date = median(index_date, na.rm = TRUE),
    q1_index_date = quantile(as.numeric(index_date), probs = 0.25, na.rm = TRUE) %>% as.Date(origin = "1970-01-01"),
    q3_index_date = quantile(as.numeric(index_date), probs = 0.75, na.rm = TRUE) %>% as.Date(origin = "1970-01-01"),
    min_creatinine = min(serum_creatinine, na.rm = TRUE),
    max_creatinine = max(serum_creatinine, na.rm = TRUE),
    mean_creatinine = mean(serum_creatinine, na.rm = TRUE),
    q1_creatinine = quantile(serum_creatinine, probs = 0.25, na.rm = TRUE),
    q3_creatinine = quantile(serum_creatinine, probs = 0.75, na.rm = TRUE),

  )

# Print the overall summary statistics
print("Table 2: Overall Summary Statistics after Removing Outliers")
print(summary_statistics)

```

#### Table for Creatnine Distribution of Statistics by Race

```{r}
# Table 3: Breakdown of summary statistics by race (after removing outliers)
summary_by_race <- person_filtered %>%
  group_by(race) %>%
  summarise(
    earliest_index_date = min(index_date, na.rm = TRUE),
    latest_index_date = max(index_date, na.rm = TRUE),
    total_participants = n(),
    median_index_date = median(index_date, na.rm = TRUE),
    q1_index_date = quantile(as.numeric(index_date), probs = 0.25, na.rm = TRUE) %>% as.Date(origin = "1970-01-01"), # Unix epoch as the reference point for converting numbers back to dates.
    q3_index_date = quantile(as.numeric(index_date), probs = 0.75, na.rm = TRUE) %>% as.Date(origin = "1970-01-01"),
    min_creatinine = min(serum_creatinine, na.rm = TRUE),
    max_creatinine = max(serum_creatinine, na.rm = TRUE),
    mean_creatinine = mean(serum_creatinine, na.rm = TRUE),
    q1_creatinine = quantile(serum_creatinine, probs = 0.25, na.rm = TRUE),
    q3_creatinine = quantile(serum_creatinine, probs = 0.75, na.rm = TRUE),

  )

# Print the breakdown by race
print("Table 3: Summary Statistics Breakdown by Race after Removing Outliers")
print(summary_by_race)

```

#### Table for Creatnine Distribution of Statistics by Race and disease status

```{r}
# Filter out people without a valid index_date (i.e., remove rows with NA in index_date)
person_filtered <- person %>%
  filter(!is.na(index_date))

# Calculate the count of NA, non-NA, and creatinine values above 300, and group by race and disease_status (cases and controls)
group_summary_race <- person_filtered %>%
  group_by(race, disease_status) %>%
  summarise(
    total_count = n(),                            # Total number of measurements
    na_count = sum(is.na(serum_creatinine)),      # Count of NA values in serum_creatinine
    actual_count = sum(!is.na(serum_creatinine)), # Count of non-NA values
    above_300_count = sum(serum_creatinine > 300, na.rm = TRUE)  # Count of values above 300
  ) %>%
  ungroup() %>%
  mutate(
    group = if_else(disease_status == 1, "Cases", "Controls"),
    percentage_na = (na_count / total_count) * 100,             # Percentage of NA values
    percentage_actual = (actual_count / total_count) * 100,     # Percentage of non-NA values
    percentage_above_300 = (above_300_count / total_count) * 100 # Percentage of values above 300
  )

# Calculate the same summary for all participants (ALL group), split by cases and controls
group_summary_all <- person_filtered %>%
  group_by(disease_status) %>%
  summarise(
    total_count = n(),                            # Total number of measurements
    na_count = sum(is.na(serum_creatinine)),      # Count of NA values in serum_creatinine
    actual_count = sum(!is.na(serum_creatinine)), # Count of non-NA values
    above_300_count = sum(serum_creatinine > 300, na.rm = TRUE)  # Count of values above 300
  ) %>%
  ungroup() %>%
  mutate(
    race = "ALL",                                             # Label for ALL group
    group = if_else(disease_status == 1, "Cases", "Controls"),
    percentage_na = (na_count / total_count) * 100,           # Percentage of NA values
    percentage_actual = (actual_count / total_count) * 100,   # Percentage of non-NA values
    percentage_above_300 = (above_300_count / total_count) * 100 # Percentage of values above 300
  )

# Combine the ALL group with the race-specific summary
group_summary <- bind_rows(group_summary_race, group_summary_all)

# Print the summary table
print("Group summary with NA, actual serum_creatinine values, and values above 300 by race, disease_status (cases and controls), and for all participants (with valid index_date):")
print(group_summary)

```

## Follow up summary

```{r}
# Table 3: Breakdown of summary statistics by race (after removing outliers)
summary_by_race <- person_filtered %>%
  group_by(race) %>%
  summarise(
    earliest_index_date = min(index_date, na.rm = TRUE),
    latest_index_date = max(index_date, na.rm = TRUE),
    total_participants = n(),
    median_index_date = median(index_date, na.rm = TRUE),
    q1_index_date = quantile(as.numeric(index_date), probs = 0.25, na.rm = TRUE) %>% as.Date(origin = "1970-01-01"), # Unix epoch as the reference point for converting numbers back to dates.
    q3_index_date = quantile(as.numeric(index_date), probs = 0.75, na.rm = TRUE) %>% as.Date(origin = "1970-01-01"),
    min_creatinine = min(serum_creatinine, na.rm = TRUE),
    max_creatinine = max(serum_creatinine, na.rm = TRUE),
    mean_creatinine = mean(serum_creatinine, na.rm = TRUE),
    q1_creatinine = quantile(serum_creatinine, probs = 0.25, na.rm = TRUE),
    q3_creatinine = quantile(serum_creatinine, probs = 0.75, na.rm = TRUE),
    na_creatinine = sum(is.na(serum_creatinine)) # Count the number of NA values for serum creatinine
  )

# Print the breakdown by race
print("Table 3: Summary Statistics Breakdown by Race after Removing Outliers")
print(summary_by_race)

```

```{r}
# Load necessary libraries
library(ggplot2)
library(dplyr)

# Filter out non-finite values (zeros or negative values) for serum_creatinine
filtered_person <- person_filtered %>%
  filter(serum_creatinine > 0,  # Remove non-positive values
         race %in% c("Black or African American", "White", "Asian"))  # Filter relevant races

# Check if the filtering worked
summary(filtered_person$serum_creatinine)  # Should not show zeros or negative values

# Create the boxplot
ggplot(filtered_person, aes(x = race, y = serum_creatinine, fill = race)) +
  geom_boxplot(outlier.shape = 16, outlier.size = 2, notch = TRUE) +
  scale_y_log10() +  # Log transformation (only if your data is skewed)
  labs(
    title = "Comparison of Serum Creatinine Levels Across Racial Groups",
    x = "Race",
    y = "Serum Creatinine (mg/dL)",
    fill = "Race"
  ) +
  theme_minimal(base_size = 15) +  # Lancet-style minimalist theme
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    axis.title.x = element_text(face = "bold"),
    axis.title.y = element_text(face = "bold")
  ) +
  stat_summary(fun.data = "mean_cl_boot", geom = "errorbar", width = 0.2, color = "black") +
  theme(legend.position = "none")  # Remove legend if not needed

```

## Cohort

### Disease_date before index_date

```{r}
# Group by race and disease_status to get the count of cases and controls by race
cases_controls_by_race <- person_filtered %>%
  group_by(race, disease_status) %>%
  summarise(count = n(), .groups = 'drop')

# Print the result
print(cases_controls_by_race)

```

# Save Person_filtered Data sets

```{r}
# Person_filtered contains our cogort with creatnine measurements
write.csv(person_filtered, "person_filtered.csv", row.names = FALSE)
```

```{r}
# Remove rows where disease_date is before index_date for each person_id with disease_status == 1
person_filtered <- person_filtered %>%
  filter(disease_status != 1 | !(disease_date < index_date)) %>%
  ungroup()


# Count the number of people with disease_status == 1 and disease_status == 0
status_counts <- person_filtered %>%
  group_by(race) %>%
  summarise(count = n())

# Print the counts
print(status_counts)
```

#### case and control distribution by race

```{r}
# Remove rows where disease_date is before index_date for each person_id with disease_status == 1
person_filtered <- person_filtered %>%
  filter(disease_status != 1 | !(disease_date < index_date)) %>%
  ungroup()

# Count the number of people with disease_status == 1 and disease_status == 0, grouped by race
status_counts_by_race <- person_filtered %>%
  group_by(race, disease_status) %>%
  summarise(count = n()) %>%
  ungroup()

# Print the counts by race and disease status
print(status_counts_by_race)

```

### Age

```{r}
# Calculate age as index_date - date_of_birth
# Calculate precise age
person_filtered <- read.csv("person.csv")

person_filtered <- person_filtered %>%
  mutate(age_precise = as.numeric(difftime(index_date, date_of_birth, units = "days")) / 365.25)

# Round the age to the nearest whole number for reporting
person_filtered <- person_filtered %>%
  mutate(age_reported = round(age_precise))

# View the result
print(person_filtered)

colnames(person_filtered)
```

# Feature Window

## Feature Window comparison

## Checking for perfect separation

```{r}
# Join 'index_date' and 'disease_status' from 'person' to 'filtered_conditions'
filtered_conditions0 <- filtered_conditions %>%
  left_join(person %>% select(person_id, index_date, disease_status), by = "person_id")

# Function to filter and summarize data based on feature window (time before index_date)
summarize_conditions <- function(data, window_days) {
  data %>%
    filter(difftime(index_date, condition_start_datetime, units = "days") <= window_days & 
           difftime(index_date, condition_start_datetime, units = "days") >= 0) %>%  # Ensure condition is before the index_date
    group_by(new_name, disease_status) %>%
    summarize(unique_person_count = n_distinct(person_id), .groups = 'drop')
}

# Calculate the metrics for each feature window
summary_1year <- summarize_conditions(filtered_conditions0, 365)
summary_1_5years <- summarize_conditions(filtered_conditions0, 1.5 * 365.25)
summary_2years <- summarize_conditions(filtered_conditions0, 2 * 365.25)

# Add a column to indicate the feature window
summary_1year <- summary_1year %>% mutate(feature_window = "1 year")
summary_1_5years <- summary_1_5years %>% mutate(feature_window = "1.5 years")
summary_2years <- summary_2years %>% mutate(feature_window = "2 years")

# Combine the summaries into one table
summary_combined <- bind_rows(summary_1year, summary_1_5years, summary_2years)

# Pivot the table to have feature windows as columns
summary_table <- summary_combined %>%
  pivot_wider(names_from = feature_window, values_from = unique_person_count, values_fill = list(unique_person_count = 0))

# Print the resulting summary table
print(summary_table)


```

## 1 year from Index date

```{r}
feature_window <- 365

```

#### Condition count

```{r}

# Step 1: Filter rows where 'condition_start_datetime' is 365 days or less BEFORE 'index_date'
filtered_conditions <- filtered_conditions0 %>%
  filter(difftime(index_date, condition_start_datetime, units = "days") <= 365 &
         difftime(index_date, condition_start_datetime, units = "days") >= 0)  # Only before index date

# Step 2: Calculate the metrics for each unique person_id and standard_concept_name within the 1-year feature window
summary_table1 <- filtered_conditions %>%
  group_by(standard_concept_name, person_id) %>%
  summarize(
    unique_count = n_distinct(condition_start_datetime, condition_end_datetime, visit_occurrence_id),
    .groups = 'drop'
  )


# Print the resulting summary table
print(summary_table1)

# Count the number of unique person_id in the filtered_conditions dataframe
unique_person_count <- filtered_conditions %>%
  summarize(unique_person_count = n_distinct(person_id))

# Print the count
print(unique_person_count)



```

##### Appending to person dataset and addressing missing data. by deletion

```{r}
# Step 1: Create the wide format summary table
summary_wide <- summary_table1 %>%
  pivot_wider(
    id_cols = person_id,  # Include person_id as an identifier
    names_from = standard_concept_name,
    values_from = unique_count,
    values_fill = 0  # simplified values_fill since it's a scalar
  )


# Step 1: Perform a left join between person_filtered and filtered_conditions to retain all rows from person_filtered
person_with_summary <- person_filtered %>%
  left_join(summary_wide, by = "person_id")  # This retains all rows from person_filtered

# Step 2: Replace any missing values (i.e., NA) with 0 for the columns from summary_wide
person_with_summary <- person_with_summary %>%
  mutate(across(starts_with("condition"), ~replace_na(., 0)))

# Optional: Check for non-zero counts in each column
non_zero_counts <- person_with_summary %>%
  summarise(across(everything(), ~ sum(. != 0, na.rm = TRUE)))

# Print in vertical format
for (col in names(non_zero_counts)) {
  cat(paste0(col, ": ", non_zero_counts[[col]], "\n"))
}


```

```{r}
# Count the number of people with disease_status == 0 and disease_status == 1, grouped by race
status_counts_by_race <- person_with_summary %>%
  group_by(race, disease_status) %>%
  summarise(count = n(), .groups = 'drop')  # .groups = 'drop' prevents summarise from returning grouped data

# Print the counts by race and disease status
print(status_counts_by_race)

```

#### Device count

```{r}
# Load necessary libraries
library(dplyr)

# Assuming your datasets `device_df` and `person` dataframe are already loaded

# Step 1: Join 'index_date' from 'person' to 'device_df'
device_filtered <- device_df %>%
  left_join(person %>% select(person_id, index_date), by = "person_id")


# Step 2: Filter rows where 'device_exposure_start_datetime' is within the feature window (365 days or less before 'index_date')
device_filtered <- device_filtered %>%
  filter(difftime(index_date, device_exposure_start_datetime, units = "days") <= 365 & 
         difftime(index_date, device_exposure_start_datetime, units = "days") > 0)  # Only keep rows where the start is before index date but within 365 days

# Step 3: Calculate the metrics for each unique device_type and person_id within the feature window
summary_table2 <- device_filtered %>%
  group_by(standard_concept_name, person_id) %>%
  summarize(
    unique_count = n_distinct(device_exposure_start_datetime, device_exposure_end_datetime, visit_occurrence_id),
    .groups = 'drop'
  )

# Print the resulting summary table
print(summary_table2)

# Count the number of unique person_id in the filtered_conditions dataframe
unique_person_count <- device_filtered %>%
  summarize(unique_person_count = n_distinct(person_id))

# Print the count
print(unique_person_count)

```

##### Appending to person dataset

```{r}
# Step 1: Create the wide format summary table
summary_wide <- summary_table2 %>%
  pivot_wider(
    id_cols = person_id,  # Include person_id as an identifier
    names_from = standard_concept_name,
    values_from = unique_count,
    values_fill = 0  # simplified values_fill since it's a scalar
  )

# Step 2: Join with person_filtered
person_with_summary <- person_with_summary %>%
  left_join(summary_wide, by = "person_id")


# Optional: Check for non-zero counts in each column
non_zero_counts <- person_with_summary %>%
  summarise(across(everything(), ~ sum(. != 0, na.rm = TRUE)))

print(non_zero_counts)
```

#### Drug count

```{r}
# Load necessary libraries
library(dplyr)


# Assuming your datasets `drug_df` and `person` dataframe are already loaded

# Step 1: Join 'index_date' from 'person' to 'drug_filtered'
drug_filtered <- drug_filtered %>%
  left_join(person %>% select(person_id, index_date), by = "person_id")


# Step 2: Filter rows where 'drug_exposure_start_datetime' is within the feature window (365 days or less before 'index_date')
drug_filtered <- drug_filtered %>%
  filter(difftime(index_date, drug_exposure_start_datetime, units = "days") <= 365 & 
         difftime(index_date, drug_exposure_start_datetime, units = "days") > 0)  # Only keep rows where the start is before index date but within 365 days

# Step 3: Calculate the metrics for each unique drug_concept_id and person_id within the feature window
summary_table3 <- drug_filtered %>%
  group_by(new_name, person_id) %>%
  summarize(
    unique_count = n_distinct(drug_exposure_start_datetime, drug_exposure_end_datetime, visit_occurrence_id),
    .groups = 'drop'
  )

# Print the resulting summary table
print(summary_table3)


```

##### Appending to person dataset

```{r}
# Step 1: Create the wide format summary table
summary_wide <- summary_table3 %>%
  pivot_wider(
    id_cols = person_id,  # Include person_id as an identifier
    names_from = new_name,
    values_from = unique_count,
    values_fill = 0  # simplified values_fill since it's a scalar
  )

# Step 2: Join with person_filtered
person_with_summary <- person_with_summary %>%
  left_join(summary_wide, by = "person_id")


# Optional: Check for non-zero counts in each column
non_zero_counts <- person_with_summary %>%
  summarise(across(everything(), ~ sum(. != 0, na.rm = TRUE)))

print(non_zero_counts)
```

#### Measurement count

```{r}
# Load necessary libraries
library(data.table)

# Ensure both data frames are data.table objects
setDT(filtered_measurements)
setDT(person)
setDT(measurement_df)

# Step 1: Join 'index_date' from 'person' to 'filtered_measurements' using data.table
filtered_measurements <- filtered_measurements[person, on = "person_id", nomatch = 0]

# Step 2: Filter rows where 'measurement_datetime' is within the feature window (365 days or less before 'index_date')
filtered_measurements <- filtered_measurements[
  difftime(index_date, measurement_datetime, units = "days") <= 365 &
  difftime(index_date, measurement_datetime, units = "days") > 0
]

# Step 3: Calculate the metrics for each unique measurement_concept_id and person_id within the feature window
summary_table4 <- filtered_measurements[, .(
  unique_count = uniqueN(.SD, by = c("measurement_datetime", "visit_occurrence_id"))
), by = .(new_name, person_id)]

# Print the resulting summary table
print(summary_table4)

```

##### Appending to person dataset

```{r}
# Step 1: Create the wide format summary table
summary_wide <- summary_table4 %>%
  pivot_wider(
    id_cols = person_id,  # Include person_id as an identifier
    names_from = new_name,
    values_from = unique_count,
    values_fill = 0  # simplified values_fill since it's a scalar
  )

# Step 2: Join with person_filtered
person_with_summary <- person_with_summary %>%
  left_join(summary_wide, by = "person_id")


# Optional: Check for non-zero counts in each column
non_zero_counts <- person_with_summary %>%
  summarise(across(everything(), ~ sum(. != 0, na.rm = TRUE)))

print(non_zero_counts)
colnames(person_with_summary) 
```

#### Observation count

```{r}
# Load necessary libraries
library(dplyr)

# Step 1: Join 'index_date' from 'person' to 'observation_filtered'
observation_filtered <- observation_filtered %>%
  left_join(person %>% select(person_id, index_date), by = "person_id")

# Step 2: Filter rows where 'observation_datetime' is within the feature window (365 days or less before 'index_date')
observation_filtered <- observation_filtered %>%
  filter(
    difftime(index_date, observation_datetime, units = "days") <= 365 &
    difftime(index_date, observation_datetime, units = "days") > 0  # Only include rows before index date within 365 days
  )

# Step 3: Calculate the metrics for each unique standard_concept_name and person_id within the feature window
summary_table5 <- observation_filtered %>%
  group_by(standard_concept_name, person_id) %>%
  summarize(
    unique_count = n_distinct(observation_datetime, visit_occurrence_id),  # Calculate unique occurrences
    .groups = 'drop'
  )


# Print the resulting summary table
print(summary_table5)


```

##### Appending to person dataset

```{r}
# Custom function to handle NA values in max
max_na <- function(x) {
  if (all(is.na(x))) {
    return(NA)
  } else {
    return(max(x, na.rm = TRUE))
  }
}

# Step 1: Select the highest smoking status rank for each person
highest_smoking_status <- observation_filtered %>%
  group_by(person_id) %>%
  summarize(highest_smoking_status_rank = max_na(smoking_status_rank), .groups = 'drop')

# Step 2: Pivot the summary_table5 to wide format
summary_wide <- summary_table5 %>%
  pivot_wider(
    id_cols = person_id,  # Include person_id as an identifier
    names_from = standard_concept_name,
    values_from = unique_count,
    values_fill = list(unique_count = NA),  # Fill missing values with NA
    values_fn = list(unique_count = first)  # Use the first value in case of duplicates
  )

# Step 3: Join the highest smoking status rank with person_with_summary
person_with_summary <- person_with_summary %>%
  left_join(highest_smoking_status, by = "person_id") %>%
  left_join(summary_wide, by = "person_id")

# Optional: Check for non-zero counts in each column
non_zero_counts <- person_with_summary %>%
  summarise(across(everything(), ~ sum(. != 0, na.rm = TRUE)))

# Print the non-zero counts
print(non_zero_counts)

# Optional: Print the person_with_summary to verify
print(person_with_summary)

```

#### Procedure count

```{r}
# Load necessary libraries
library(dplyr)

# Step 1: Join 'index_date' from 'person' to 'filtered_procedures'
filtered_procedures <- filtered_procedures %>%
  left_join(person %>% select(person_id, index_date), by = "person_id")

# Step 2: Filter rows where 'procedure_datetime' is within the feature window (365 days or less before 'index_date')
filtered_procedures_window <- filtered_procedures %>%
  filter(
    difftime(index_date, procedure_datetime, units = "days") <= 365 &  # Within 365 days before index_date
    difftime(index_date, procedure_datetime, units = "days") > 0      # Ensure only before the index date
  )

# Step 3: Calculate the metrics for each unique procedure_code and person_id within the feature window
summary_table6 <- filtered_procedures_window %>%
  group_by(new_name, person_id) %>%
  summarize(
    unique_count = n_distinct(procedure_datetime, visit_occurrence_id),  # Calculate unique occurrences
    .groups = 'drop'
  )

# Print the resulting summary table
print(summary_table6)

```

##### Appending to person dataset

```{r}
# Step 1: Create the wide format summary table
summary_wide <- summary_table6 %>%
  pivot_wider(
    id_cols = person_id,  # Include person_id as an identifier
    names_from = new_name,
    values_from = unique_count,
    values_fill = 0  # simplified values_fill since it's a scalar
  )

# Step 2: Join with person_filtered
person_with_summary <- person_with_summary %>%
  left_join(summary_wide, by = "person_id")


# Optional: Check for non-zero counts in each column
# Function to count non-zero values
count_non_zero <- function(df) {
  df %>%
    summarise(across(-contains("Tobacco smoking status"), ~ sum(. != 0, na.rm = TRUE)))
}

# Calculate non-zero counts for each column
non_zero_counts <- count_non_zero(person_with_summary)

# Print the resulting counts
# Transpose the non_zero_counts dataframe for vertical printing
non_zero_counts_vertical <- t(non_zero_counts)

# Print the transposed dataframe vertically
kable(non_zero_counts_vertical)
```

### Disease Status (after index date)

```{r}
# Calculate the count of NA, non-NA, and creatinine values above 300, and group by race and disease_status (cases and controls)
group_summary_race <- person_with_summary %>%
  group_by(race, disease_status) %>%
  summarise(
    total_count = n(),                            # Total number of participants
    na_count = sum(is.na(serum_creatinine)),      # Count of NA values in serum_creatinine
    actual_count = sum(!is.na(serum_creatinine)), # Count of non-NA values
    above_300_count = sum(serum_creatinine > 300, na.rm = TRUE)  # Count of values above 300
  ) %>%
  ungroup() %>%
  mutate(
    group = if_else(disease_status == 1, "Cases", "Controls"),  # Label for cases and controls
    percentage_na = (na_count / total_count) * 100,             # Percentage of NA values
    percentage_actual = (actual_count / total_count) * 100,     # Percentage of non-NA values
    percentage_above_300 = (above_300_count / total_count) * 100 # Percentage of values above 300
  )

# Calculate the same summary for all participants (ALL group), split by cases and controls
group_summary_all <- person_with_summary %>%
  group_by(disease_status) %>%
  summarise(
    total_count = n(),                            # Total number of participants
    na_count = sum(is.na(serum_creatinine)),      # Count of NA values in serum_creatinine
    actual_count = sum(!is.na(serum_creatinine)), # Count of non-NA values
    above_300_count = sum(serum_creatinine > 300, na.rm = TRUE)  # Count of values above 300
  ) %>%
  ungroup() %>%
  mutate(
    race = "ALL",                                             # Label for ALL group
    group = if_else(disease_status == 1, "Cases", "Controls"), # Label for cases and controls
    percentage_na = (na_count / total_count) * 100,           # Percentage of NA values
    percentage_actual = (actual_count / total_count) * 100,   # Percentage of non-NA values
    percentage_above_300 = (above_300_count / total_count) * 100 # Percentage of values above 300
  )

# Combine the ALL group with the race-specific summary
group_summary <- bind_rows(group_summary_race, group_summary_all)

# Print the summary table
print("Group summary with NA, actual serum_creatinine values, and values above 300 by race, disease_status (cases and controls), and for all participants:")
print(group_summary)

```

### Time to disease

```{r}
# Ensure 'disease_date' and 'index_date' are in Date format
person_with_summary <- person_with_summary %>%
  mutate(
    disease_date = as.Date(disease_date),
    index_date = as.Date(index_date),
    time_to_event = as.numeric(difftime(disease_date, index_date, units = "days")) / 365.25  # Convert to years
  )

# Function to summarize time_to_event statistics
summarize_time_to_event <- function(data) {
  data %>%
    filter(!is.na(time_to_event)) %>%
    summarize(
      unique_person_count = n_distinct(person_id),  # Count unique person_id
      count = n(),  # Total count of events
      mean_time = mean(time_to_event, na.rm = TRUE),  # Mean time to event
      median_time = median(time_to_event, na.rm = TRUE),  # Median time to event
      min_time = min(time_to_event, na.rm = TRUE),  # Minimum time to event
      max_time = max(time_to_event, na.rm = TRUE),  # Maximum time to event
      sd_time = sd(time_to_event, na.rm = TRUE),  # Standard deviation of time to event
      q1_time = quantile(time_to_event, 0.25, na.rm = TRUE),  # 1st quartile time to event
      q3_time = quantile(time_to_event, 0.75, na.rm = TRUE)  # 3rd quartile time to event
    )
}

# Summarize for Black participants
summary_black <- summarize_time_to_event(person_with_summary %>% filter(race == "Black or African American"))

# Summarize for White participants
summary_white <- summarize_time_to_event(person_with_summary %>% filter(race == "White"))

# Summarize for Asian participants
summary_asian <- summarize_time_to_event(person_with_summary %>% filter(race == "Asian"))

# Summarize for All participants
summary_all <- summarize_time_to_event(person_with_summary)

# Combine the results into a table
final_summary_table <- bind_rows(
  summary_black %>% mutate(group = "Black Participants"),
  summary_white %>% mutate(group = "White Participants"),
  summary_asian %>% mutate(group = "Asian Participants"),
  summary_all %>% mutate(group = "All Participants")
) %>%
  select(group, unique_person_count, count, mean_time, median_time, min_time, max_time, sd_time, q1_time, q3_time)

# Print the final summary table
print("Summary of Time to Event Statistics by Race:")
print(final_summary_table)

# Optional: Display the table in a nice format (if using RMarkdown or similar)
library(knitr)
kable(final_summary_table, caption = "Summary Statistics of Time to Event by Race")

# Remove 'time_to_event' from 'person_with_summary'
person_with_summary <- person_with_summary %>% select(-time_to_event)
colnames(person_with_summary)

```

## Checking for perfect separation

```{r}
# Identify numeric columns
numeric_columns <- sapply(person_with_summary, is.numeric)
numeric_columns <- names(numeric_columns[numeric_columns])

# Filter out non-numeric columns and pivot longer
person_long <- person_with_summary %>%
  select(person_id, disease_status, all_of(numeric_columns)) %>%
  pivot_longer(
    cols = -c(person_id, disease_status),
    names_to = "variable",
    values_to = "value"
  ) %>%
  filter(!is.na(value) & value != 0)  # Remove rows where the value is NA or zero

# Count occurrences of disease_status for each variable
status_counts <- person_long %>%
  group_by(variable, disease_status) %>%
  summarise(count = n_distinct(person_id), .groups = 'drop')

# Pivot the summary table to a wide format
summary_wide <- status_counts %>%
  pivot_wider(
    names_from = disease_status,
    values_from = count,
    names_prefix = "disease_status_",
    values_fill = list(count = 0)
  )

# Print the resulting summary table
print(summary_wide)
```

### Censored Date

```{r}
library(dplyr)

# Step 1: Combine the relevant date columns from all the datasets
combined_dates <- bind_rows(
  condition_df %>% select(person_id, date = condition_end_datetime),
  device_df %>% select(person_id, date = device_exposure_end_datetime),
  drug_df %>% select(person_id, date = drug_exposure_end_datetime),
  measurement_df %>% select(person_id, date = measurement_datetime),
  observation_df %>% select(person_id, date = observation_datetime),
  procedure_df %>% select(person_id, date = procedure_datetime)
)

# Step 2: Group by person_id and find the maximum date for each person
latest_censored_dates <- combined_dates %>%
  group_by(person_id) %>%
  summarise(censored_date = if (all(is.na(date))) NA else max(date, na.rm = TRUE)) %>%
  ungroup()

# Step 3: Merge the latest censored dates back into person_with_summary
person_with_summary <- person_with_summary %>%
  left_join(latest_censored_dates, by = "person_id")

# Step 4: Print the updated number of people with non-NA censored_date
non_na_censored_count <- person_with_summary %>%
  filter(!is.na(censored_date)) %>%
  summarise(count = n_distinct(person_id))

cat("Number of people with non-NA censored_date:", non_na_censored_count$count, "\n")

# Step 5: Print the number of people with NA censored dates
na_censored_count <- person_with_summary %>%
  filter(is.na(censored_date)) %>%
  summarise(count = n_distinct(person_id))

cat("Number of people with NA censored_date:", na_censored_count$count, "\n")

```

## Add censored dates to person dataset

```{r}
library(dplyr)

# Step 1: Combine the relevant date columns from all the datasets
combined_dates <- bind_rows(
  condition_df %>% select(person_id, date = condition_end_datetime),
  device_df %>% select(person_id, date = device_exposure_end_datetime),
  drug_df %>% select(person_id, date = drug_exposure_end_datetime),
  measurement_df %>% select(person_id, date = measurement_datetime),
  observation_df %>% select(person_id, date = observation_datetime),
  procedure_df %>% select(person_id, date = procedure_datetime)
)

# Step 2: Group by person_id and find the maximum date for each person
latest_censored_dates <- combined_dates %>%
  group_by(person_id) %>%
  summarise(censored_date = if (all(is.na(date))) NA else max(date, na.rm = TRUE)) %>%
  ungroup()

# Step 3: Merge the latest censored dates back into person_with_summary
person <- person %>%
  left_join(latest_censored_dates, by = "person_id")

# Step 4: Print the updated number of people with non-NA censored_date
non_na_censored_count <- person %>%
  filter(!is.na(censored_date)) %>%
  summarise(count = n_distinct(person_id))

cat("Number of people with non-NA censored_date:", non_na_censored_count$count, "\n")

# Step 5: Print the number of people with NA censored dates
na_censored_count <- person %>%
  filter(is.na(censored_date)) %>%
  summarise(count = n_distinct(person_id))

cat("Number of people with NA censored_date:", na_censored_count$count, "\n")

# Step 5: Print the column names of the updated person dataset
colnames(person)

```

### Time to event (first of disease or censoring)

```{r}
library(dplyr)

# Convert censored_date, disease_date, and index_date to Date format
person_with_summary <- person_with_summary %>%
  mutate(
    censored_date = as.Date(censored_date),
    disease_date = as.Date(disease_date),
    index_date = as.Date(index_date) # Ensure index_date is in Date format
  )

# Determine the event_date for cases and controls
# Use disease_date for cases (disease_status == 1) and censored_date for controls (disease_status == 0)
person_with_summary <- person_with_summary %>%
  mutate(
    event_date = case_when(
      disease_status == 1 ~ disease_date,  # Cases should use disease_date
      disease_status == 0 ~ censored_date  # Controls should use censored_date
    ),
    event_date = as.Date(event_date) # Ensure the event_date is in Date format
  )

# Step 1: Identify cases with disease_date before index_date
cases_removed <- person_with_summary %>%
  filter(disease_status == 1 & disease_date < index_date)

# Step 2: Identify controls with censored_date before index_date
controls_removed <- person_with_summary %>%
  filter(disease_status == 0 & censored_date < index_date)

# Print the counts of cases and controls that were removed
cat("Number of cases removed with disease_date before index_date:", nrow(cases_removed), "\n")
cat("Number of controls removed with censored_date before index_date:", nrow(controls_removed), "\n")

# Step 3: Filter the dataset to keep only the valid cases and controls
person_with_summary_inrange_indexdate <- person_with_summary %>%
  filter(
    (disease_status == 1 & disease_date >= index_date) |   # Keep cases where disease_date is after or on index_date
    (disease_status == 0 & censored_date >= index_date)    # Keep controls where censored_date is after or on index_date
  )

# Calculate the time_to_event for the filtered dataset
person_with_summary_inrange_indexdate <- person_with_summary_inrange_indexdate %>%
  mutate(
    time_to_event = as.numeric(difftime(event_date, index_date, units = "days")) # Calculate the time_to_event in days
  )

# Print the total number of people in the filtered dataset
cat("Total number of people in the filtered dataset (inrange):", nrow(person_with_summary_inrange_indexdate), "\n")

# Display the first few rows to verify changes
print(head(person_with_summary_inrange_indexdate))

# Count the number of cases and controls in the filtered dataset
cases_controls_count <- person_with_summary_inrange_indexdate %>%
  group_by(disease_status) %>%
  summarise(count = n())

# Rename the `disease_status` values for clarity
cases_controls_count <- cases_controls_count %>%
  mutate(status = ifelse(disease_status == 1, "Cases", "Controls")) %>%
  select(status, count)

# Print the number of cases and controls
print(cases_controls_count)


```

# Save Person_with_summary Data sets

```{r}

# Removed disease date before index date
write.csv(person_with_summary_inrange_indexdate, "person_with_summary.csv", row.names = FALSE)

#Full cohort
write.csv(person, "person.csv", row.names = FALSE)

```

### Time to event summary statistics by race ()

```{r}
library(dplyr)

# Function to summarize time_to_event statistics by group and convert days to years
summarize_time_to_event <- function(data) {
  data %>%
    #filter(!is.na(serum_creatinine)) %>%
    summarize(
      unique_person_count = n_distinct(person_id),  # Count unique person_id
      count = n(),  # Total count of events
      mean_time = mean(time_to_event, na.rm = TRUE) / 365.25,  # Convert mean time to years
      median_time = median(time_to_event, na.rm = TRUE) / 365.25,  # Convert median time to years
      min_time = min(time_to_event, na.rm = TRUE) / 365.25,  # Convert minimum time to years
      max_time = max(time_to_event, na.rm = TRUE) / 365.25,  # Convert maximum time to years
      sd_time = sd(time_to_event, na.rm = TRUE) / 365.25,  # Convert standard deviation of time to years
      q1_time = quantile(time_to_event, 0.25, na.rm = TRUE) / 365.25,  # Convert 1st quartile time to years
      q3_time = quantile(time_to_event, 0.75, na.rm = TRUE) / 365.25  # Convert 3rd quartile time to years
    )
}

# Summarize for all participants
summary_all <- summarize_time_to_event(person_with_summary_inrange_indexdate)

# Summarize for Black participants
summary_black <- summarize_time_to_event(person_with_summary_inrange_indexdate %>% filter(race == "Black or African American"))

# Summarize for White participants
summary_white <- summarize_time_to_event(person_with_summary_inrange_indexdate %>% filter(race == "White"))

# Summarize for Asian participants
summary_asian <- summarize_time_to_event(person_with_summary_inrange_indexdate %>% filter(race == "Asian"))

# Combine the summaries into one table
final_summary_table <- bind_rows(
  summary_all %>% mutate(group = "All Participants"),
  summary_black %>% mutate(group = "Black Participants"),
  summary_white %>% mutate(group = "White Participants"),
  summary_asian %>% mutate(group = "Asian Participants")
) %>%
  select(group, unique_person_count, count, mean_time, median_time, min_time, max_time, sd_time, q1_time, q3_time)

# Print the final summary table
print("Summary of Time to Event Statistics by Race (in years):")
print(final_summary_table)

```

```{r}
# Count the number of entries with disease_status == 0
num_disease_status_0 <- person_with_summary_inrange_indexdate %>%
  filter(disease_status == 0) %>%
  nrow()

# Count the number of entries with disease_status == 1
num_disease_status_1 <- person_with_summary_inrange_indexdate %>%
  filter(disease_status == 1) %>%
  nrow()

# Print the counts
print(paste("Number of entries with disease_status == 0:", num_disease_status_0))
print(paste("Number of entries with disease_status == 1:", num_disease_status_1))

```

# Survival Analysis

Data Preparation

```{r}
# Load necessary libraries
library(dplyr)
person_with_summary_inrange_indexdate <- read.csv("person_with_summary.csv")

# Step 1: Ensure date columns are in Date format
person <- person_with_summary_inrange_indexdate %>%
  mutate(
    disease_date = as.Date(disease_date),
    censored_date = as.Date(censored_date),
    index_date = as.Date(index_date)
  )

# Step 2: Calculate time-to-event as the min of disease_date or censored_date
person <- person_with_summary_inrange_indexdate %>%
  mutate(
    # Calculate the minimum of disease_date or censored_date (excluding NAs)
    event_date = pmin(disease_date, censored_date, na.rm = TRUE),
    
    # If both dates are NA, we can't calculate time-to-event
    time_to_event = as.numeric(difftime(event_date, index_date, units = "days")),
    
    # For clarity, indicate whether the event was censored or if disease occurred
    event_type = case_when(
      !is.na(disease_date) & (disease_date <= censored_date | is.na(censored_date)) ~ "Disease",
      !is.na(censored_date) & (censored_date < disease_date | is.na(disease_date)) ~ "Censored",
      TRUE ~ "Unknown"  # In case both dates are NA
    )
  )

# Step 3: Review the updated dataset with time_to_event and event_type
head(person[, c("person_id", "index_date", "disease_date", "censored_date", "time_to_event", "event_type")])

# 279,614 participants, 17 variables, contains full cohort
write.csv(person, "survival_data.csv", row.names = FALSE)



```

### KP plot: creatnine cohort

```{r}
# Load necessary library
library(dplyr)

# Assuming person_with_summary_inrange_indexdate has columns: person_id, race, and disease_status
# where disease_status is 1 for cases and 0 for controls

# Calculate the number of cases and controls for each race
cases_controls_summary <- person_with_summary_inrange_indexdate %>%
  group_by(race) %>%
  summarise(
    total_individuals = n(),                           # Total number of individuals for each race
    cases = sum(disease_status == 1, na.rm = TRUE),    # Count of cases
    controls = sum(disease_status == 0, na.rm = TRUE)  # Count of controls
  )

# Display the results
print(cases_controls_summary)

```

## Creatnine Cohort without sex filter, and outlier filter KP Curve

Asian: 4170, 58 Black: 30941, 816 White: 94661, 805

```{r}
# Load necessary libraries
library(survival)
library(survminer)
library(dplyr)

# Ensure that the race column is treated as a factor with appropriate labels
person_with_summary_inrange_indexdate <- person_with_summary_inrange_indexdate %>%
  mutate(
    race = factor(race, levels = c("White", "Black or African American", "Asian")),
    time_to_event_years = time_to_event / 365 # Convert time to years
  )

# Create a Kaplan-Meier survival object stratified by race
km_fit <- survfit(Surv(time_to_event_years, disease_status == 1) ~ race, data = person_with_summary_inrange_indexdate)

# Update legend labels with the correct names
legend_labels <- c("White Participants", "Black Participants", "Asian Participants")

# Plot the Kaplan-Meier curves for each race with dashed confidence intervals
km_plot <- ggsurvplot(
  km_fit,
  data = person_with_summary_inrange_indexdate,
  pval = TRUE,                     # Include p-value for the log-rank test
  conf.int = TRUE,                 # Include confidence intervals
  conf.int.style = "step",         # Dashed lines for confidence intervals
  risk.table = TRUE,               # Include risk table below the plot
  legend.title = "Race",
  legend.labs = legend_labels,     # Correct labels for publication
  palette = c("green3", "red", "blue"),  # White = green, Black = red, Asian = blue
  linetype = "solid",              # Use a solid line for all groups
  ggtheme = theme_minimal() + theme(panel.grid.major = element_blank(), 
                                    panel.grid.minor = element_blank()), # Remove grid lines
  xlab = "Time in Years",          # Update X-axis label to years
  ylab = "Survival Probability",   # Y-axis label
  size = 2,                        # Further increase line thickness for visibility
  font.main = c(22, "bold"),       # Increase title font size and bold for emphasis
  font.x = c(20),                  # X-axis label font size
  font.y = c(20),                  # Y-axis label font size
  font.tickslab = c(18),           # Increase axis ticks label size
  font.legend = c(23),             # Increase the font size of the legend labels
  risk.table.fontsize = 6,         # Adjust font size of the risk table for visibility
  legend = "right",                # Position the legend on the right side
  legend.linetype = "solid",       # Ensure that all legend lines are solid
  legend.size = 2                  # Increase the thickness of lines in the legend
)

# Save the Kaplan-Meier plot to a file with improved visibility
ggsave("km_plot_race_with_dashed_CI.png", plot = km_plot$plot, dpi = 900, width = 20, height = 15)

# Print the Kaplan-Meier plot to the R console for a final check
print(km_plot)

# Save the plot again as a high-resolution image suitable for a Lancet publication
ggsave("Enhanced_Kaplan_Meier_Survival_Plot_with_Dashed_CI_Lancet.png", plot = km_plot$plot, dpi = 900, width = 20, height = 15)

```

KP Zoomed in for presentation

```{r}
# Load necessary libraries
library(survival)
library(survminer)
library(dplyr)

# Ensure that the race column is treated as a factor with appropriate labels and order
person_with_summary_inrange_indexdate <- person_with_summary_inrange_indexdate %>%
  mutate(
    race = factor(race, levels = c("Asian", "Black or African American", "White")),
    time_to_event_years = time_to_event / 365 # Convert time to years
  )

# Create a Kaplan-Meier survival object stratified by race
km_fit <- survfit(Surv(time_to_event_years, disease_status == 1) ~ race, data = person_with_summary_inrange_indexdate)

# Update legend labels with the correct names
legend_labels <- c("Asian Participants", "Black Participants", "White Participants")

# Plot the Kaplan-Meier curves for each race with enhanced readability
km_plot <- ggsurvplot(
  km_fit,
  data = person_with_summary_inrange_indexdate,
  pval = TRUE,                     # Include p-value for the log-rank test
  pval.coord = c(0, 0.82),         # Manually position the p-value within the visible range
  conf.int = TRUE,                 # Include confidence intervals
  conf.int.style = "step",         # Dashed lines for confidence intervals
  risk.table = FALSE,              # Remove risk table for simplicity
  legend.title = "Race: ",
  legend.labs = legend_labels,     # Correct labels for publication
  palette = c("blue", "red", "green3"),  # Update colors: Asian = blue, Black = red, White = green
  linetype = "solid",              # Use a solid line for all groups
  ggtheme = theme_minimal() +
    theme(
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank(),
      legend.position = "top",         # Move legend to the top
      legend.text = element_text(size = 30, face = "bold"), # Make legend text bold and large
      legend.title = element_text(size = 30, face = "bold"), # Make legend title bold and large
      legend.key.size = unit(2, "lines"),  # Make the legend lines thicker
      axis.title = element_text(size = 30, face = "bold"),   # Bold and large axis titles
      axis.text = element_text(size = 27, face = "bold")     # Bold and large axis tick labels
    ),
  xlab = "Time in Years",          # Update X-axis label to years
  ylab = "Survival Probability",   # Y-axis label
  ylim = c(0.80, 1),               # Set y-axis limits to zoom in from 0.80 to 1
  size = 2                         # Further increase line thickness for visibility
)

# Save the Kaplan-Meier plot to a file with improved visibility
ggsave("km_plot_race_updated_colors.png", plot = km_plot$plot, dpi = 900, width = 20, height = 15)

# Print the Kaplan-Meier plot to the R console for a final check
print(km_plot)

# Save the plot again as a high-resolution image suitable for publication
ggsave("Enhanced_Kaplan_Meier_Survival_Plot_with_Updated_Colors_Lancet.png", plot = km_plot$plot, dpi = 900, width = 20, height = 15)

```

### The Asians left after the last event in the Asian Population

```{r}
# Load necessary libraries
library(survival)
library(survminer)
library(dplyr)

# Ensure that the race column is treated as a factor with appropriate labels
person_with_summary_inrange_indexdate <- person_with_summary_inrange_indexdate %>%
  mutate(
    race = factor(race, levels = c("White", "Black or African American", "Asian")),
    time_to_event_years = time_to_event / 365 # Convert time to years
  )

# Create a Kaplan-Meier survival object stratified by race
km_fit <- survfit(Surv(time_to_event_years, disease_status == 1) ~ race, data = person_with_summary_inrange_indexdate)

# Extract information for Asian participants
asian_data <- person_with_summary_inrange_indexdate %>% filter(race == "Asian")

# Calculate the time of the last event for Asians
last_event_time <- max(asian_data$time_to_event_years[asian_data$disease_status == 1])

# Calculate the number of individuals remaining in the cohort after the last event
remaining_after_last_event <- sum(asian_data$time_to_event_years >= last_event_time)

# Create annotation text
annotation_text <- paste0(
  "Last event: ", round(last_event_time, 2), " years\n",
  "Remaining: ", remaining_after_last_event
)

# Plot the Kaplan-Meier curves for each race
km_plot <- ggsurvplot(
  km_fit,
  data = person_with_summary_inrange_indexdate,
  pval = TRUE,                     # Include p-value for the log-rank test
  conf.int = TRUE,                 # Include confidence intervals
  conf.int.style = "step",         # Dashed lines for confidence intervals
  risk.table = TRUE,               # Include risk table below the plot
  legend.title = "Race",
  legend.labs = c("White Participants", "Black Participants", "Asian Participants"),
  palette = c("green3", "red", "blue"),  # Using distinct colors
  linetype = "solid",              # Use a solid line for all groups
  ggtheme = theme_minimal() + theme(panel.grid.major = element_blank(), 
                                    panel.grid.minor = element_blank()), # Remove grid lines
  xlab = "Time in Years",          # Update X-axis label to years
  ylab = "Survival Probability",   # Y-axis label
  size = 2,                        # Further increase line thickness for visibility
  font.main = c(22, "bold"),       # Increase title font size and bold for emphasis
  font.x = c(20),                  # X-axis label font size
  font.y = c(20),                  # Y-axis label font size
  font.tickslab = c(18),           # Increase axis ticks label size
  font.legend = c(23),             # Increase the font size of the legend labels
  risk.table.fontsize = 6,         # Adjust font size of the risk table for visibility
  legend = "right",                # Position the legend on the right side
  legend.linetype = "solid",       # Ensure that all legend lines are solid
  legend.size = 2                  # Increase the thickness of lines in the legend
)

# Add annotation for Asians
km_plot$plot <- km_plot$plot +
  annotate("text", x = last_event_time, y = 0.85, label = annotation_text, color = "blue", size = 6, hjust = 0)

# Save the Kaplan-Meier plot to a file
ggsave("km_plot_with_annotation.png", plot = km_plot$plot, dpi = 900, width = 20, height = 15)

# Print the Kaplan-Meier plot to the R console for a final check
print(km_plot)

```

# Table 1

```{r}

# # Load necessary libraries
# library(dplyr)
# library(tidyr)
# 
# # Deduplicate the person_with_summary dataset by person_id
# table1_data <- person_with_summary %>%
#   distinct(person_id, .keep_all = TRUE)
# 
# # Combine the relevant PheCode columns for each condition into a single column
# table1_data <- table1_data %>%
#   mutate(
#     hypertension = ifelse(`PheCode:401.3` == 1 | `PheCode:401.22` == 1 | `PheCode:401.2` == 1, 1, 0),
#     `Chronic kidney disease` = ifelse(`PheCode:585.4` == 1 | `PheCode:585.33` == 1 | `PheCode:585.34` == 1 | `PheCode:585.32` == 1, 1, 0),
#     `Congestive heart failure` = ifelse(`PheCode:428` == 1, 1, 0),
#     `Peripheral vascular disease` = ifelse(`PheCode:443.9` == 1, 1, 0),
#     `Acute renal failure syndrome` = ifelse(`PheCode:585.1` == 1, 1, 0),
#     `Diabetes mellitus` = ifelse(`PheCode:250` == 1, 1, 0)
#   )
# 
# # Create the table with case and control labels
# create_baseline_table <- function(data) {
#   numeric_summary <- data %>%
#     group_by(disease_status) %>%
#     summarise(
#       `Black or African American` = sum(race == "Black or African American"),
#       `White` = sum(race == "White"),
#       `18-44` = sum(age_precise >= 18 & age_precise <= 44),
#       `45-64` = sum(age_precise >= 45 & age_precise <= 64),
#       `> 65` = sum(age_precise > 65),
#       #`≤30` = sum(eGFR_value <= 30, na.rm = TRUE),
#       #`31–60` = sum(eGFR_value > 30 & eGFR_value <= 60, na.rm = TRUE),
#       #`61–90` = sum(eGFR_value > 60 & eGFR_value <= 90, na.rm = TRUE),
#       #`>90` = sum(eGFR_value > 90, na.rm = TRUE),
#       Female = sum(sex_at_birth == "Female"),
#       Male = sum(sex_at_birth == "Male"),
#       `Underweight (< 18.5)` = sum(BMI < 18.5, na.rm = TRUE),
#       `Normal (18.5–24.9)` = sum(BMI >= 18.5 & BMI <= 24.9, na.rm = TRUE),
#       `Overweight (25–29.9)` = sum(BMI >= 25 & BMI <= 29.9, na.rm = TRUE),
#       `Obese (30+)` = sum(BMI >= 30, na.rm = TRUE),
#       hypertension = sum(hypertension == 1),
#       `Chronic kidney disease` = sum(`Chronic kidney disease` == 1),
#       `Congestive heart failure` = sum(`Congestive heart failure` == 1),
#       `Peripheral vascular disease` = sum(`Peripheral vascular disease` == 1),
#       `Acute renal failure syndrome` = sum(`Acute renal failure syndrome` == 1),
#       `Diabetes mellitus` = sum(`Diabetes mellitus` == 1),
#       `Participants with Albumin/Creatinine` = sum(!is.na(`Albumin/Creatinine`))
#     ) %>%
#     pivot_longer(-disease_status, names_to = "Characteristics", values_to = "Counts") %>%
#     mutate(Counts = as.character(Counts))  # Convert Counts to character
#   
#   character_summary <- data %>%
#     group_by(disease_status) %>%
#     summarise(
#       #`eGFR, median (IQR)` = round(median(eGFR_value, na.rm = TRUE), 2),
#       `BMI, median (IQR)` = round(median(BMI, na.rm = TRUE), 2)
#     ) %>%
#     pivot_longer(-disease_status, names_to = "Characteristics", values_to = "Counts") %>%
#     mutate(Counts = as.character(Counts))  # Convert Counts to character
# 
#   summary_table <- bind_rows(numeric_summary, character_summary) %>%
#     pivot_wider(names_from = disease_status, values_from = Counts, names_prefix = "ESRD_") %>%
#     mutate(
#       Total = as.character(as.numeric(coalesce(ESRD_0, "0")) + as.numeric(coalesce(ESRD_1, "0"))),
#       Control = paste0(ESRD_0, " (", round(as.numeric(ESRD_0) / as.numeric(Total) * 100, 1), "%)"),
#       Case = paste0(ESRD_1, " (", round(as.numeric(ESRD_1) / as.numeric(Total) * 100, 1), "%)")
#     ) %>%
#     select(Characteristics, Case, Control, Total)
#   
#   return(summary_table)
# }
# 
# # Create the table
# baseline_table <- create_baseline_table(table1_data)
# 
# # Print the table
# print(baseline_table)
```

## Data prep for eGFR calculation

### Individuals with 0 creatnine

```{r}
# Load necessary library
library(dplyr)

# Assuming 'person_with_summary_inrange_indexdate' dataset is already loaded

# Calculate the number of people in each race with serum_creatinine == 0
creatinine_zero_counts <- person_with_summary_inrange_indexdate %>%
  filter(serum_creatinine == 0) %>%
  group_by(race, disease_status) %>%
  summarise(count = n())

# Print the result
print(creatinine_zero_counts)

```

```{r}
library(dplyr)
library(knitr)

# Step 1: Filter out observations with serum_creatinine > 0 & serum_creatinine <= 300 and sex_at_birth is "Male" or "Female"
person_with_summary_inrange_indexdate_creatnine <- person_with_summary_inrange_indexdate %>%
  filter(serum_creatinine > 0 & serum_creatinine <= 300) %>%
  filter(sex_at_birth %in% c("Male", "Female"))

# Step 2: Function to calculate the number of cases and controls by group
summarize_by_group <- function(data, group_var) {
  data %>%
    group_by(!!sym(group_var), disease_status) %>%
    summarise(count = n(), .groups = 'drop') %>%
    mutate(group = if_else(disease_status == 1, "Cases", "Controls")) %>%
    select(!!sym(group_var), group, count)  # Return selected columns
}

# Summaries by race (Black, White, Asian)
black_summary <- person_with_summary_inrange_indexdate_creatnine %>%
  filter(race == "Black or African American") %>%
  summarize_by_group("race") %>%
  mutate(race_group = "Black Participants")

white_summary <- person_with_summary_inrange_indexdate_creatnine %>%
  filter(race == "White") %>%
  summarize_by_group("race") %>%
  mutate(race_group = "White Participants")

asian_summary <- person_with_summary_inrange_indexdate_creatnine %>%
  filter(race == "Asian") %>%
  summarize_by_group("race") %>%
  mutate(race_group = "Asian Participants")

# Summary for all participants
all_participants_summary <- person_with_summary_inrange_indexdate_creatnine %>%
  group_by(disease_status) %>%
  summarise(count = n(), .groups = 'drop') %>%
  mutate(group = if_else(disease_status == 1, "Cases", "Controls"),
         race = "All Participants",
         race_group = "All Participants") %>% 
  select(race, group, count, race_group) # Align columns correctly

# Step 3: Combine all summaries into a single table
combined_summary <- bind_rows(black_summary, white_summary, asian_summary, all_participants_summary)

# Print the summary table for number of cases and controls
print("Summary of Cases and Controls by Race (Filtered for Serum Creatinine and Sex):")
print(combined_summary)

# Optional: Display the table in a nice format (if using RMarkdown or similar)
kable(combined_summary, caption = "Summary of Cases and Controls by Race (Filtered for Serum Creatinine and Sex)")

```

```{r}
# Load necessary libraries
library(dplyr)

# Calculate the minimum and maximum serum_creatinine values
creatinine_summary <- person_with_summary_inrange_indexdate_creatnine %>%
  summarise(
    min_serum_creatinine = min(serum_creatinine, na.rm = TRUE),
    max_serum_creatinine = max(serum_creatinine, na.rm = TRUE)
  )

# Print the summary
print(creatinine_summary)

```

# eGFR MDRD CALCULATION

```{r}
library(dplyr)

# Step 1: Define the function to calculate eGFR using the MDRD equation
calculate_eGFR_MDRD <- function(serum_creatinine, age, sex_factor, race_factor) {
  # Calculate eGFR using the MDRD equation
  eGFR <- 186 * (serum_creatinine ^ -1.154) * (age ^ -0.203) * sex_factor * race_factor
  return(eGFR)
}

# Step 2: Calculate eGFR directly using serum creatinine in mg/dL
person_with_summary_inrange_indexdate_creatnine <- person_with_summary_inrange_indexdate_creatnine %>%
  mutate(
    # Ensure serum_creatinine is converted to numeric
    serum_creatinine_numeric = as.numeric(serum_creatinine),
    
    # Apply the appropriate sex factor (Female = 0.742, Male = 1)
    sex_factor = ifelse(sex_at_birth == "Female", 0.742, 1),
    
    # Apply the appropriate race factor (Black = 1.21, others = 1)
    race_factor = ifelse(race == "Black or African American", 1.21, 1),
    
    # Calculate eGFR_MDRD
    eGFR_MDRD = ifelse(
      is.na(serum_creatinine_numeric) | serum_creatinine_numeric <= 0 | is.na(age_precise) | age_precise <= 0,
      NA,
      calculate_eGFR_MDRD(serum_creatinine_numeric, age_precise, sex_factor, race_factor)
    )
  ) %>%
  # Drop the temporary variables used for the eGFR calculation
  select(-serum_creatinine_numeric, -sex_factor, -race_factor)

# Step 3: Print the number of cases and controls that have a valid eGFR_MDRD value
valid_eGFR_counts <- person_with_summary_inrange_indexdate_creatnine %>%
  filter(!is.na(eGFR_MDRD)) %>%
  group_by(disease_status) %>%
  summarise(count = n())

cat("Number of cases (disease_status == 1) with valid eGFR_MDRD:", 
    valid_eGFR_counts$count[valid_eGFR_counts$disease_status == 1], "\n")
cat("Number of controls (disease_status == 0) with valid eGFR_MDRD:", 
    valid_eGFR_counts$count[valid_eGFR_counts$disease_status == 0], "\n")

# Step 4: Print the summary statistics of the eGFR_MDRD values
summary_eGFR_MDRD <- summary(person_with_summary_inrange_indexdate_creatnine$eGFR_MDRD)

cat("\nSummary statistics for eGFR_MDRD:\n")
print(summary_eGFR_MDRD)


```

### eGFR_MDRD summary

```{r}
library(dplyr)
library(knitr)

# Function to generate summary and ensure it always has 7 rows (including NA's)
generate_summary <- function(data, column) {
  summary_vector <- summary(data[[column]])
  
  # Check if the 'NA's' row is missing, and if so, append it
  if (length(summary_vector) == 6) {
    summary_vector <- c(summary_vector, "NA's" = sum(is.na(data[[column]])))
  }
  
  return(as.numeric(summary_vector))
}

# Step 1: Summary for all participants
summary_all <- generate_summary(person_with_summary_inrange_indexdate_creatnine, "eGFR_MDRD")

# Step 2: Summary for Black participants
summary_black <- generate_summary(person_with_summary_inrange_indexdate_creatnine %>% 
                                   filter(race == "Black or African American"), 
                                 "eGFR_MDRD")

# Step 3: Summary for White participants
summary_white <- generate_summary(person_with_summary_inrange_indexdate_creatnine %>% 
                                   filter(race == "White"), 
                                 "eGFR_MDRD")

# Step 4: Summary for Asian participants
summary_asian <- generate_summary(person_with_summary_inrange_indexdate_creatnine %>% 
                                   filter(race == "Asian"), 
                                 "eGFR_MDRD")

# Step 5: Create a summary table for all four groups
summary_table <- data.frame(
  Statistic = c("Min", "1st Qu.", "Median", "Mean", "3rd Qu.", "Max", "NA's"),
  All_Participants = summary_all,
  Black_Participants = summary_black,
  White_Participants = summary_white,
  Asian_Participants = summary_asian
)

# Step 6: Print the summary table
print("Summary of eGFR_MDRD for All Participants, Black Participants, White Participants, and Asian Participants:")
kable(summary_table, caption = "Summary Statistics of eGFR_MDRD by Race Group")

```

# CKD-EPI 2009 CALCULATION

```{r}
# Step 1: Extract relevant columns and set up the CKD-EPI 2009 equation variables
person_with_summary_inrange_indexdate_creatnine <- person_with_summary_inrange_indexdate_creatnine %>%
  mutate(
    # Define the kappa and alpha parameters based on sex
    kappa = ifelse(sex_at_birth == "Female", 0.7, 0.9),
    alpha = ifelse(sex_at_birth == "Female", -0.329, -0.411),
    sex_factor = ifelse(sex_at_birth == "Female", 1.018, 1),

    # Define the race factor (1 for White, 1.159 for Black or African American)
    race_factor = ifelse(race == "Black or African American", 1.159, 1),

    # Apply the CKD-EPI 2009 equation for eGFR calculation
    eGFR_ckd_epi_2009 = 141 * pmin(serum_creatinine / kappa, 1)^alpha * 
                        pmax(serum_creatinine / kappa, 1)^(-1.209) * 
                        (0.993^age_precise) * sex_factor * race_factor
  ) %>%
  # Step 2: Remove all temporary variables used in the calculation
  select(-kappa, -alpha, -sex_factor, -race_factor)

# Step 3: Print the number of cases and controls that have a valid eGFR_ckd_epi_2009 value
valid_eGFR_counts <- person_with_summary_inrange_indexdate_creatnine %>%
  filter(!is.na(eGFR_ckd_epi_2009)) %>%
  group_by(disease_status) %>%
  summarise(count = n())

cat("Number of cases (disease_status == 1) with valid eGFR_ckd_epi_2009:", 
    valid_eGFR_counts$count[valid_eGFR_counts$disease_status == 1], "\n")
cat("Number of controls (disease_status == 0) with valid eGFR_ckd_epi_2009:", 
    valid_eGFR_counts$count[valid_eGFR_counts$disease_status == 0], "\n")

# Step 4: Print the summary statistics of the eGFR_ckd_epi_2009 values
summary_eGFR <- summary(person_with_summary_inrange_indexdate_creatnine$eGFR_ckd_epi_2009)

cat("\nSummary statistics for eGFR_ckd_epi_2009:\n")
print(summary_eGFR)

# Optional: Print a few rows of the dataset to verify the eGFR values are appended
head(person_with_summary_inrange_indexdate_creatnine, 10)

```

## eGFR_ckd_epi_2009 summary

```{r}
library(dplyr)
library(knitr)

# Function to generate summary and ensure it always has 7 rows (including NA's)
generate_summary <- function(data, column) {
  summary_vector <- summary(data[[column]])
  
  # Check if the 'NA's' row is missing, and if so, append it
  if (length(summary_vector) == 6) {
    summary_vector <- c(summary_vector, "NA's" = sum(is.na(data[[column]])))
  }
  
  return(as.numeric(summary_vector))
}

# Step 1: Summary for all participants
summary_all <- generate_summary(person_with_summary_inrange_indexdate_creatnine, "eGFR_ckd_epi_2009")

# Step 2: Summary for Black participants
summary_black <- generate_summary(person_with_summary_inrange_indexdate_creatnine %>% 
                                   filter(race == "Black or African American"), 
                                 "eGFR_ckd_epi_2009")

# Step 3: Summary for White participants
summary_white <- generate_summary(person_with_summary_inrange_indexdate_creatnine %>% 
                                   filter(race == "White"), 
                                 "eGFR_ckd_epi_2009")

# Step 4: Summary for Asian participants
summary_asian <- generate_summary(person_with_summary_inrange_indexdate_creatnine %>% 
                                   filter(race == "Asian"), 
                                 "eGFR_ckd_epi_2009")

# Step 5: Create a summary table for all four groups
summary_table <- data.frame(
  Statistic = c("Min", "1st Qu.", "Median", "Mean", "3rd Qu.", "Max", "NA's"),
  All_Participants = summary_all,
  Black_Participants = summary_black,
  White_Participants = summary_white,
  Asian_Participants = summary_asian
)

# Step 6: Print the summary table
print("Summary of eGFR_ckd_epi_2009 for All Participants, Black Participants, White Participants, and Asian Participants:")
kable(summary_table, caption = "Summary Statistics of eGFR_ckd_epi_2009 by Race Group")

```

# CKD-EPI 2021 CALCULATION

```{r}
# Load necessary libraries
library(dplyr)

# Step 1: Extract relevant columns and set up the CKD-EPI 2021 equation variables
person_with_summary_inrange_indexdate_creatnine <- person_with_summary_inrange_indexdate_creatnine %>%
  mutate(
    # Define the kappa and alpha parameters based on sex (CKD-EPI 2021 without race)
    kappa = ifelse(sex_at_birth == "Female", 0.7, 0.9),
    alpha = ifelse(sex_at_birth == "Female", -0.241, -0.302),
    sex_factor = ifelse(sex_at_birth == "Female", 1.012, 1),

    # Apply the CKD-EPI 2021 equation for eGFR calculation (without race)
    eGFR_ckd_epi_2021 = 142 * pmin(serum_creatinine / kappa, 1)^alpha * 
                        pmax(serum_creatinine / kappa, 1)^(-1.2) * 
                        (0.9938^age_precise) * sex_factor
  ) %>%
  # Step 2: Remove the temporary variables (kappa, alpha, sex_factor)
  select(-kappa, -alpha, -sex_factor)

# Step 3: Print the number of cases and controls that have a valid eGFR_ckd_epi_2021 value
# Assuming disease_status == 1 means case and disease_status == 0 means control
valid_eGFR_counts_2021 <- person_with_summary_inrange_indexdate_creatnine %>%
  filter(!is.na(eGFR_ckd_epi_2021)) %>%
  group_by(disease_status) %>%
  summarise(count = n())

cat("Number of cases (disease_status == 1) with valid eGFR_ckd_epi_2021:", 
    valid_eGFR_counts_2021$count[valid_eGFR_counts_2021$disease_status == 1], "\n")
cat("Number of controls (disease_status == 0) with valid eGFR_ckd_epi_2021:", 
    valid_eGFR_counts_2021$count[valid_eGFR_counts_2021$disease_status == 0], "\n")

# Step 4: Print the summary statistics of the eGFR_ckd_epi_2021 values
summary_eGFR_2021 <- summary(person_with_summary_inrange_indexdate_creatnine$eGFR_ckd_epi_2021)

cat("\nSummary statistics for eGFR_ckd_epi_2021:\n")
print(summary_eGFR_2021)

# Optional: Print a few rows of the dataset to verify the eGFR_ckd_epi_2021 values are appended
head(person_with_summary_inrange_indexdate_creatnine, 10)

```

## eGFR_ckd_epi_2021 summary

```{r}
library(dplyr)
library(knitr)

# Function to generate summary and ensure it always has 7 rows (including NA's)
generate_summary <- function(data, column) {
  summary_vector <- summary(data[[column]])
  
  # Check if the 'NA's' row is missing, and if so, append it
  if (length(summary_vector) == 6) {
    summary_vector <- c(summary_vector, "NA's" = sum(is.na(data[[column]])))
  }
  
  return(as.numeric(summary_vector))
}

# Step 1: Summary for all participants
summary_all <- generate_summary(person_with_summary_inrange_indexdate_creatnine, "eGFR_ckd_epi_2021")

# Step 2: Summary for Black participants
summary_black <- generate_summary(person_with_summary_inrange_indexdate_creatnine %>% 
                                   filter(race == "Black or African American"), 
                                 "eGFR_ckd_epi_2021")

# Step 3: Summary for White participants
summary_white <- generate_summary(person_with_summary_inrange_indexdate_creatnine %>% 
                                   filter(race == "White"), 
                                 "eGFR_ckd_epi_2021")

# Step 4: Summary for Asian participants
summary_asian <- generate_summary(person_with_summary_inrange_indexdate_creatnine %>% 
                                   filter(race == "Asian"), 
                                 "eGFR_ckd_epi_2021")

# Step 5: Create a summary table for all four groups
summary_table <- data.frame(
  Statistic = c("Min", "1st Qu.", "Median", "Mean", "3rd Qu.", "Max", "NA's"),
  All_Participants = summary_all,
  Black_Participants = summary_black,
  White_Participants = summary_white,
  Asian_Participants = summary_asian
)

# Step 6: Print the summary table
print("Summary of eGFR_ckd_epi_2021 for All Participants, Black Participants, White Participants, and Asian Participants:")
kable(summary_table, caption = "Summary Statistics of eGFR_ckd_epi_2021 by Race Group")


```

# Save Analysis Data sets

```{r}
# Save the dataframes to CSV files
# 125,098 participants, 222 variables, contains creatnine cohort with creatnine value > 0 and  disease_date after index date and control with censored date after index date
write.csv(person_with_summary_inrange_indexdate_creatnine, "person_with_summary_inrange_indexdate_creatnine.csv", row.names = FALSE)

```

# eGFR Analysis

```{r}
person_with_summary_inrange_indexdate_creatnine <- read.csv("person_with_summary_inrange_indexdate_creatnine.csv")

# Load required libraries
library(ggplot2)
library(dplyr)

# Define the function to calculate eGFR categories
calculate_eGFR_categories <- function(data, eGFR_column) {
  data %>%
    mutate(
      eGFR_category = case_when(
        !!sym(eGFR_column) < 15 ~ "Kidney failure (<15)",
        !!sym(eGFR_column) >= 15 & !!sym(eGFR_column) < 60 ~ "Kidney disease (15-59)",
        !!sym(eGFR_column) >= 60 & !!sym(eGFR_column) < 90 ~ "Early-stage (60-89)",
        !!sym(eGFR_column) >= 90 ~ "Normal (>=90)",
        TRUE ~ NA_character_
      )
    ) %>%
    group_by(eGFR_category) %>%
    summarise(
      count = n(),
      percentage = (n() / nrow(data)) * 100
    ) %>%
    ungroup()
}

# Combine the eGFR category summaries for each race and method
combine_data_for_plot <- function(data, eGFR_method, race) {
  data %>%
    mutate(Method = eGFR_method, Race = race) %>%
    rename(Count = count, Percentage = percentage)
}

# Prepare data for each method and race
eGFR_MDRD_plot_data_black <- combine_data_for_plot(
  calculate_eGFR_categories(person_with_summary_inrange_indexdate_creatnine %>% filter(race == "Black or African American"), "eGFR_MDRD"),
  "MDRD", "Black"
)
eGFR_ckd_epi_2009_plot_data_black <- combine_data_for_plot(
  calculate_eGFR_categories(person_with_summary_inrange_indexdate_creatnine %>% filter(race == "Black or African American"), "eGFR_ckd_epi_2009"),
  "CKD-EPI 2009", "Black"
)
eGFR_ckd_epi_2021_plot_data_black <- combine_data_for_plot(
  calculate_eGFR_categories(person_with_summary_inrange_indexdate_creatnine %>% filter(race == "Black or African American"), "eGFR_ckd_epi_2021"),
  "CKD-EPI 2021", "Black"
)

eGFR_MDRD_plot_data_white <- combine_data_for_plot(
  calculate_eGFR_categories(person_with_summary_inrange_indexdate_creatnine %>% filter(race == "White"), "eGFR_MDRD"),
  "MDRD", "White"
)
eGFR_ckd_epi_2009_plot_data_white <- combine_data_for_plot(
  calculate_eGFR_categories(person_with_summary_inrange_indexdate_creatnine %>% filter(race == "White"), "eGFR_ckd_epi_2009"),
  "CKD-EPI 2009", "White"
)
eGFR_ckd_epi_2021_plot_data_white <- combine_data_for_plot(
  calculate_eGFR_categories(person_with_summary_inrange_indexdate_creatnine %>% filter(race == "White"), "eGFR_ckd_epi_2021"),
  "CKD-EPI 2021", "White"
)

eGFR_MDRD_plot_data_asian <- combine_data_for_plot(
  calculate_eGFR_categories(person_with_summary_inrange_indexdate_creatnine %>% filter(race == "Asian"), "eGFR_MDRD"),
  "MDRD", "Asian"
)
eGFR_ckd_epi_2009_plot_data_asian <- combine_data_for_plot(
  calculate_eGFR_categories(person_with_summary_inrange_indexdate_creatnine %>% filter(race == "Asian"), "eGFR_ckd_epi_2009"),
  "CKD-EPI 2009", "Asian"
)
eGFR_ckd_epi_2021_plot_data_asian <- combine_data_for_plot(
  calculate_eGFR_categories(person_with_summary_inrange_indexdate_creatnine %>% filter(race == "Asian"), "eGFR_ckd_epi_2021"),
  "CKD-EPI 2021", "Asian"
)

# Combine data for all methods and races
combined_plot_data <- bind_rows(
  eGFR_MDRD_plot_data_black, eGFR_ckd_epi_2009_plot_data_black, eGFR_ckd_epi_2021_plot_data_black,
  eGFR_MDRD_plot_data_white, eGFR_ckd_epi_2009_plot_data_white, eGFR_ckd_epi_2021_plot_data_white,
  eGFR_MDRD_plot_data_asian, eGFR_ckd_epi_2009_plot_data_asian, eGFR_ckd_epi_2021_plot_data_asian
)

# Reorder the eGFR methods so MDRD (1999) appears first
combined_plot_data$Method <- factor(combined_plot_data$Method, 
                                    levels = c("MDRD", "CKD-EPI 2009", "CKD-EPI 2021"))

# Reorder the CKD stages for plotting
combined_plot_data$eGFR_category <- factor(combined_plot_data$eGFR_category, 
                                           levels = c("Kidney failure (<15)", 
                                                      "Kidney disease (15-59)", 
                                                      "Early-stage (60-89)", 
                                                      "Normal (>=90)"))

# Define a custom color palette suitable for a Lancet-style presentation
lancet_palette <- c(
  "Kidney failure (<15)" = "#D73027",  # Dark red
  "Kidney disease (15-59)" = "#FC8D59",  # Salmon
  "Early-stage (60-89)" = "#FEE090",  # Light orange
  "Normal (>=90)" = "#4575B4"  # Soft blue
)

# Create a stacked bar chart where colors represent CKD stages
plot <- ggplot(combined_plot_data, aes(x = Method, y = Percentage, fill = eGFR_category)) +
  geom_bar(stat = "identity", position = "fill") +
  facet_wrap(~ Race) +
  labs(title = "CKD Stage Distribution by Race and eGFR Method",
       x = "eGFR Method",
       y = "Proportion",
       fill = "CKD Stage: ") +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_manual(values = lancet_palette) +  # Apply Lancet-style color palette
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 18, face = "bold"), 
    axis.text.y = element_text(size = 18, face = "bold"), 
    axis.title.x = element_text(size = 18, face = "bold"), 
    axis.title.y = element_text(size = 18, face = "bold"), 
    plot.title = element_text(size = 18, face = "bold", hjust = 0.5), 
    legend.title = element_text(size = 16, face = "bold"), 
    legend.text = element_text(size = 16, face = "bold"), 
    legend.position = "top",  # Move legend to the top
    strip.text = element_text(size = 20, face = "bold")
  )

# Save the plot as PNG and PDF
ggsave("CKD_Stage_Distribution_by_Race_and_Method_Lancet.png", plot, dpi = 300, width = 12, height = 8)
ggsave("CKD_Stage_Distribution_by_Race_and_Method_Lancet.pdf", plot, width = 12, height = 8)

# Display the plot
plot

```

## eGFR Model Misclassification analysis

```{r}
# Load required libraries
library(ggplot2)
library(dplyr)

# Combine the eGFR category summaries for each race and method
combine_data_for_plot <- function(data, eGFR_method, race) {
  data %>%
    mutate(Method = eGFR_method, Race = race) %>%
    rename(Count = count, Percentage = percentage)
}

# Prepare data for each method and race
eGFR_MDRD_plot_data_black <- combine_data_for_plot(
  calculate_eGFR_categories(person_with_summary_inrange_indexdate_creatnine %>% filter(race == "Black or African American"), "eGFR_MDRD"),
  "MDRD", "Black"
)
eGFR_ckd_epi_2009_plot_data_black <- combine_data_for_plot(
  calculate_eGFR_categories(person_with_summary_inrange_indexdate_creatnine %>% filter(race == "Black or African American"), "eGFR_ckd_epi_2009"),
  "CKD-EPI 2009", "Black"
)
eGFR_ckd_epi_2021_plot_data_black <- combine_data_for_plot(
  calculate_eGFR_categories(person_with_summary_inrange_indexdate_creatnine %>% filter(race == "Black or African American"), "eGFR_ckd_epi_2021"),
  "CKD-EPI 2021", "Black"
)

eGFR_MDRD_plot_data_white <- combine_data_for_plot(
  calculate_eGFR_categories(person_with_summary_inrange_indexdate_creatnine %>% filter(race == "White"), "eGFR_MDRD"),
  "MDRD", "White"
)
eGFR_ckd_epi_2009_plot_data_white <- combine_data_for_plot(
  calculate_eGFR_categories(person_with_summary_inrange_indexdate_creatnine %>% filter(race == "White"), "eGFR_ckd_epi_2009"),
  "CKD-EPI 2009", "White"
)
eGFR_ckd_epi_2021_plot_data_white <- combine_data_for_plot(
  calculate_eGFR_categories(person_with_summary_inrange_indexdate_creatnine %>% filter(race == "White"), "eGFR_ckd_epi_2021"),
  "CKD-EPI 2021", "White"
)

eGFR_MDRD_plot_data_asian <- combine_data_for_plot(
  calculate_eGFR_categories(person_with_summary_inrange_indexdate_creatnine %>% filter(race == "Asian"), "eGFR_MDRD"),
  "MDRD", "Asian"
)
eGFR_ckd_epi_2009_plot_data_asian <- combine_data_for_plot(
  calculate_eGFR_categories(person_with_summary_inrange_indexdate_creatnine %>% filter(race == "Asian"), "eGFR_ckd_epi_2009"),
  "CKD-EPI 2009", "Asian"
)
eGFR_ckd_epi_2021_plot_data_asian <- combine_data_for_plot(
  calculate_eGFR_categories(person_with_summary_inrange_indexdate_creatnine %>% filter(race == "Asian"), "eGFR_ckd_epi_2021"),
  "CKD-EPI 2021", "Asian"
)

# Combine data for all methods and races
combined_plot_data <- bind_rows(
  eGFR_MDRD_plot_data_black, eGFR_ckd_epi_2009_plot_data_black, eGFR_ckd_epi_2021_plot_data_black,
  eGFR_MDRD_plot_data_white, eGFR_ckd_epi_2009_plot_data_white, eGFR_ckd_epi_2021_plot_data_white,
  eGFR_MDRD_plot_data_asian, eGFR_ckd_epi_2009_plot_data_asian, eGFR_ckd_epi_2021_plot_data_asian
)

# Reorder the eGFR methods so MDRD appears first
combined_plot_data$Method <- factor(combined_plot_data$Method, 
                                    levels = c("MDRD", "CKD-EPI 2009", "CKD-EPI 2021"))

# Reorder the CKD stages for plotting
combined_plot_data$eGFR_category <- factor(combined_plot_data$eGFR_category, 
                                           levels = c("Kidney failure (<15)", 
                                                      "Kidney disease (15-59)", 
                                                      "Early-stage (60-89)", 
                                                      "Normal (>=90)"))

# Define a custom color palette suitable for a Lancet-style presentation
lancet_palette <- c(
  "Kidney failure (<15)" = "#D73027",  # Dark red
  "Kidney disease (15-59)" = "#FC8D59",  # Salmon
  "Early-stage (60-89)" = "#FEE090",  # Light orange
  "Normal (>=90)" = "#4575B4"  # Soft blue
)

# Create a stacked bar chart where colors represent CKD stages
plot <- ggplot(combined_plot_data, aes(x = Method, y = Percentage, fill = eGFR_category)) +
  geom_bar(stat = "identity", position = "fill") +
  facet_wrap(~ Race) +
  labs(title = "CKD Stage Distribution by Race and eGFR Method",
       x = "eGFR Method",
       y = "Proportion",
       fill = "CKD Stage") +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_manual(values = lancet_palette) +  # Apply Lancet-style color palette
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 14, face = "bold"), 
    axis.text.y = element_text(size = 14, face = "bold"), 
    axis.title.x = element_text(size = 16, face = "bold"), 
    axis.title.y = element_text(size = 16, face = "bold"), 
    plot.title = element_text(size = 18, face = "bold", hjust = 0.5), 
    legend.title = element_text(size = 16, face = "bold"), 
    legend.text = element_text(size = 14, face = "bold"), 
    legend.position = "top",  # Move legend to the top
    strip.text = element_text(size = 20, face = "bold")
  )

# Save the plot as PNG and PDF
ggsave("CKD_Stage_Distribution_by_Race_and_Method_Lancet.png", plot, dpi = 300, width = 12, height = 8)
ggsave("CKD_Stage_Distribution_by_Race_and_Method_Lancet.pdf", plot, width = 12, height = 8)

# Display the plot
plot

```

### eGFR analysis with weight added to the kidney failure group

```{r}
# Define the function to calculate eGFR categories
calculate_eGFR_categories <- function(data, eGFR_column, adjustment = 20) {
  data %>%
    mutate(
      eGFR_category = case_when(
        !!sym(eGFR_column) < 15 ~ "Kidney failure (<15)",
        !!sym(eGFR_column) >= 15 & !!sym(eGFR_column) < 60 ~ "Kidney disease (15-59)",
        !!sym(eGFR_column) >= 60 & !!sym(eGFR_column) < 90 ~ "Early-stage (60-89)",
        !!sym(eGFR_column) >= 90 ~ "Normal (>=90)",
        TRUE ~ NA_character_
      )
    ) %>%
    group_by(eGFR_category) %>%
    summarise(
      count = n(),
      percentage = (n() / nrow(data)) * 100
    ) %>%
    ungroup() %>%
    # Proportionally adjust the "Kidney failure (<15)" group
    mutate(percentage = if_else(
      eGFR_category == "Kidney failure (<15)",
      percentage + (percentage / sum(percentage[eGFR_category == "Kidney failure (<15)"], na.rm = TRUE)) * adjustment,
      percentage
    ))
}

# Combine the eGFR category summaries for each race and method
combine_data_for_plot <- function(data, eGFR_method, race) {
  data %>%
    mutate(Method = eGFR_method, Race = race) %>%
    rename(Count = count, Percentage = percentage)
}

# Prepare data for each method and race
eGFR_MDRD_plot_data_black <- combine_data_for_plot(
  calculate_eGFR_categories(person_with_summary_inrange_indexdate_creatnine %>% filter(race == "Black or African American"), "eGFR_MDRD"),
  "MDRD", "Black"
)
eGFR_ckd_epi_2009_plot_data_black <- combine_data_for_plot(
  calculate_eGFR_categories(person_with_summary_inrange_indexdate_creatnine %>% filter(race == "Black or African American"), "eGFR_ckd_epi_2009"),
  "CKD-EPI 2009", "Black"
)
eGFR_ckd_epi_2021_plot_data_black <- combine_data_for_plot(
  calculate_eGFR_categories(person_with_summary_inrange_indexdate_creatnine %>% filter(race == "Black or African American"), "eGFR_ckd_epi_2021"),
  "CKD-EPI 2021", "Black"
)

eGFR_MDRD_plot_data_white <- combine_data_for_plot(
  calculate_eGFR_categories(person_with_summary_inrange_indexdate_creatnine %>% filter(race == "White"), "eGFR_MDRD"),
  "MDRD", "White"
)
eGFR_ckd_epi_2009_plot_data_white <- combine_data_for_plot(
  calculate_eGFR_categories(person_with_summary_inrange_indexdate_creatnine %>% filter(race == "White"), "eGFR_ckd_epi_2009"),
  "CKD-EPI 2009", "White"
)
eGFR_ckd_epi_2021_plot_data_white <- combine_data_for_plot(
  calculate_eGFR_categories(person_with_summary_inrange_indexdate_creatnine %>% filter(race == "White"), "eGFR_ckd_epi_2021"),
  "CKD-EPI 2021", "White"
)

eGFR_MDRD_plot_data_asian <- combine_data_for_plot(
  calculate_eGFR_categories(person_with_summary_inrange_indexdate_creatnine %>% filter(race == "Asian"), "eGFR_MDRD"),
  "MDRD", "Asian"
)
eGFR_ckd_epi_2009_plot_data_asian <- combine_data_for_plot(
  calculate_eGFR_categories(person_with_summary_inrange_indexdate_creatnine %>% filter(race == "Asian"), "eGFR_ckd_epi_2009"),
  "CKD-EPI 2009", "Asian"
)
eGFR_ckd_epi_2021_plot_data_asian <- combine_data_for_plot(
  calculate_eGFR_categories(person_with_summary_inrange_indexdate_creatnine %>% filter(race == "Asian"), "eGFR_ckd_epi_2021"),
  "CKD-EPI 2021", "Asian"
)

# Combine data for all methods and races
combined_plot_data <- bind_rows(
  eGFR_MDRD_plot_data_black, eGFR_ckd_epi_2009_plot_data_black, eGFR_ckd_epi_2021_plot_data_black,
  eGFR_MDRD_plot_data_white, eGFR_ckd_epi_2009_plot_data_white, eGFR_ckd_epi_2021_plot_data_white,
  eGFR_MDRD_plot_data_asian, eGFR_ckd_epi_2009_plot_data_asian, eGFR_ckd_epi_2021_plot_data_asian
)

# Reorder the eGFR methods so MDRD (1999) appears first
combined_plot_data$Method <- factor(combined_plot_data$Method, 
                                    levels = c("MDRD", "CKD-EPI 2009", "CKD-EPI 2021"))

# Reorder the CKD stages for plotting
combined_plot_data$eGFR_category <- factor(combined_plot_data$eGFR_category, 
                                           levels = c("Kidney failure (<15)", 
                                                      "Kidney disease (15-59)", 
                                                      "Early-stage (60-89)", 
                                                      "Normal (>=90)"))

# Define a custom color palette suitable for a Lancet-style presentation
lancet_palette <- c(
  "Kidney failure (<15)" = "#D73027",  # Dark red
  "Kidney disease (15-59)" = "#FC8D59",  # Salmon
  "Early-stage (60-89)" = "#FEE090",  # Light orange
  "Normal (>=90)" = "#4575B4"  # Soft blue
)

# Create a stacked bar chart where colors represent CKD stages
plot <- ggplot(combined_plot_data, aes(x = Method, y = Percentage, fill = eGFR_category)) +
  geom_bar(stat = "identity", position = "fill") +
  facet_wrap(~ Race) +
  labs(title = "CKD Stage Distribution by Race and eGFR Method",
       x = "eGFR Method",
       y = "Proportion",
       fill = "CKD Stage: ") +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_manual(values = lancet_palette) +  # Apply Lancet-style color palette
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 18, face = "bold"), 
    axis.text.y = element_text(size = 18, face = "bold"), 
    axis.title.x = element_text(size = 18, face = "bold"), 
    axis.title.y = element_text(size = 18, face = "bold"), 
    plot.title = element_text(size = 18, face = "bold", hjust = 0.5), 
    legend.title = element_text(size = 16, face = "bold"), 
    legend.text = element_text(size = 16, face = "bold"), 
    legend.position = "top",  # Move legend to the top
    strip.text = element_text(size = 20, face = "bold")
  )

# Save the plot as PNG and PDF
ggsave("CKD_Stage_Distribution_by_Race_and_Method_Lancet.png", plot, dpi = 300, width = 12, height = 8)
ggsave("CKD_Stage_Distribution_by_Race_and_Method_Lancet.pdf", plot, width = 12, height = 8)

# Display the plot
plot

```

```{r}
library(dplyr)

# Calculate the number of cases and controls for each race
case_control_summary <- person_with_summary_inrange_indexdate_creatnine %>%
  group_by(race, disease_status) %>%   # Group by race and disease status
  summarise(count = n(), .groups = 'drop') %>%  # Count the number of occurrences for each group
  mutate(status = ifelse(disease_status == 1, "Cases", "Controls")) %>%  # Label cases and controls
  select(race, status, count)  # Select relevant columns

# Print the summary
print(case_control_summary)

```

```{r}
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(tidyr)


# Step 1: Reshape the data into a long format for easy plotting
person_long <- person_with_summary_inrange_indexdate_creatnine %>%
  select(race, eGFR_MDRD, eGFR_ckd_epi_2009, eGFR_ckd_epi_2021) %>%
  pivot_longer(
    cols = c(eGFR_MDRD, eGFR_ckd_epi_2009, eGFR_ckd_epi_2021),
    names_to = "eGFR_Method",
    values_to = "eGFR_Value"
  )

# Set factor levels for eGFR_Method to ensure the correct order
person_long$eGFR_Method <- factor(person_long$eGFR_Method, 
                                  levels = c("eGFR_MDRD", "eGFR_ckd_epi_2009", "eGFR_ckd_epi_2021"),
                                  labels = c("MDRD", "CKD-EPI 2009", "CKD-EPI 2021"))

# Remove rows with NA values in eGFR_Value
person_long <- person_long %>% filter(!is.na(eGFR_Value))

# Step 2: Calculate mean, standard deviation, min, and max for each group
mean_sd_data <- person_long %>%
  group_by(race, eGFR_Method) %>%
  summarise(
    mean_value = mean(eGFR_Value, na.rm = TRUE),
    sd_value = sd(eGFR_Value, na.rm = TRUE),
    min_value = min(eGFR_Value, na.rm = TRUE),
    max_value = max(eGFR_Value, na.rm = TRUE),
    .groups = "drop"  # To suppress the grouping message
  )

# Find the maximum value for the y-axis range
max_value <- max(person_long$eGFR_Value, na.rm = TRUE)

# Step 3: Create the density plot with mean, SD lines, and annotations
density_plot <- ggplot(person_long, aes(x = eGFR_Value, fill = race)) +
  geom_density(alpha = 0.4, adjust = 1.5) +  # Density plot with adjusted smoothing
  geom_vline(data = mean_sd_data, aes(xintercept = mean_value, color = race), linetype = "dashed", linewidth = 1.2) +
  # Using geom_segment to display the mean ± SD ranges
  geom_segment(data = mean_sd_data,
               aes(x = mean_value - sd_value, xend = mean_value + sd_value,
                   y = 0, yend = 0, color = race),
               linewidth = 1.5) +
  # Adding annotations for mean, min, and max values with bold text and increased size
  geom_text(data = mean_sd_data, aes(x = mean_value, y = 0.01, label = paste("Mean:", round(mean_value, 1))),
            color = "black", size = 6, vjust = -1.5, fontface = "bold", angle = 90) +
  geom_text(data = mean_sd_data, aes(x = min_value, y = 0.01, label = paste("Min:", round(min_value, 1))),
            color = "black", size = 6, vjust = -1.5, fontface = "bold", angle = 90) +
  geom_text(data = mean_sd_data, aes(x = max_value, y = 0.01, label = paste("Max:", round(max_value, 1))),
            color = "black", size = 6, vjust = -1.5, fontface = "bold", angle = 90) +
  # Apply custom colors
  scale_fill_manual(values = c("White" = "#1f78b4", "Black or African American" = "#e31a1c", "Asian" = "#33a02c")) +
  scale_color_manual(values = c("White" = "#1f78b4", "Black or African American" = "#e31a1c", "Asian" = "#33a02c")) +
  facet_wrap(~ race + eGFR_Method, scales = "free") +  # Allow free x and y scales for each facet
  theme_minimal() +
  labs(
    title = "",
    x = "eGFR Value (mL/min/1.73 m²)",
    y = "Density",
    fill = "Race",
    color = "Race"
  ) +
  theme(
    plot.title = element_text(hjust = 0.5, size = 24, face = "bold"),
    axis.title.x = element_text(size = 20, face = "bold"),
    axis.title.y = element_text(size = 20, face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1, size = 16, face = "bold"),
    axis.text.y = element_text(size = 16, face = "bold"),
    legend.position = "bottom",
    legend.title = element_text(size = 18, face = "bold"),
    legend.text = element_text(size = 16, face = "bold"),
    strip.text = element_text(size = 18, face = "bold")
  )

# Print the density plot
print(density_plot)

# Save the density plot with increased size for better visibility
ggsave("Annotated_Density_eGFR_Plot_Free_Scales.png", plot = density_plot, dpi = 600, width = 20, height = 12)

```

# Read-in data

```{r}
library(knitr)
library(dplyr)
library(tidyr)
library(lubridate)
library(purrr)
library(stringr)

#install.packages("survival")
#install.packages("survminer")

# Load the necessary libraries
library(survival)

# install.packages("RColorBrewer")
library(RColorBrewer)

# Install and load necessary packages
#install.packages("caret")
#install.packages("ggplot2")
library(caret)
library(ggplot2)

# Load necessary libraries
#library(pROC)

#person_with_summary_inrange_indexdate_creatnine <- read.csv("person_with_summary_inrange_indexdate_creatnine.csv")
# Load necessary library
library(dplyr)

# Calculate summary statistics for time_to_event in years
summary_stats <- person_with_summary_inrange_indexdate_creatnine %>%
  summarise(
    Min = min(time_to_event) / 365.25,
    Q1 = quantile(time_to_event, 0.25) / 365.25,
    Median = median(time_to_event) / 365.25,
    Mean = mean(time_to_event) / 365.25,
    Q3 = quantile(time_to_event, 0.75) / 365.25,
    Max = max(time_to_event) / 365.25,
    SD = sd(time_to_event) / 365.25
  )

# Create a nicely formatted table
library(knitr)
kable(summary_stats, col.names = c("Min (years)", "Q1 (years)", "Median (years)", 
                                   "Mean (years)", "Q3 (years)", "Max (years)", "SD (years)"),
      caption = "Summary Statistics for Time-to-Event Data (in Years)")

```

# InRange eGFR data

```{r}
disease_status_count <- person_with_summary_inrange_indexdate_creatnine %>%
  group_by(disease_status) %>%
  summarise(count = n())

# Print the counts
print(disease_status_count)
```

## Albumin.Creatinine Analysis

# Albumin Data Import

```{r}
library(tidyverse)
library(bigrquery)

# This query represents dataset "Albumin data" for domain "measurement" and was generated for All of Us Controlled Tier Dataset v7
dataset_92268616_measurement_sql <- paste("
    SELECT
        measurement.person_id,
        measurement.measurement_concept_id,
        m_standard_concept.concept_name as standard_concept_name,
        m_standard_concept.concept_code as standard_concept_code,
        m_standard_concept.vocabulary_id as standard_vocabulary,
        measurement.measurement_datetime,
        measurement.measurement_type_concept_id,
        m_type.concept_name as measurement_type_concept_name,
        measurement.operator_concept_id,
        m_operator.concept_name as operator_concept_name,
        measurement.value_as_number,
        measurement.value_as_concept_id,
        m_value.concept_name as value_as_concept_name,
        measurement.unit_concept_id,
        m_unit.concept_name as unit_concept_name,
        measurement.range_low,
        measurement.range_high,
        measurement.visit_occurrence_id,
        m_visit.concept_name as visit_occurrence_concept_name,
        measurement.measurement_source_value,
        measurement.measurement_source_concept_id,
        m_source_concept.concept_name as source_concept_name,
        m_source_concept.concept_code as source_concept_code,
        m_source_concept.vocabulary_id as source_vocabulary,
        measurement.unit_source_value,
        measurement.value_source_value 
    FROM
        ( SELECT
            * 
        FROM
            `measurement` measurement 
        WHERE
            (
                measurement_concept_id IN (SELECT
                    DISTINCT c.concept_id 
                FROM
                    `cb_criteria` c 
                JOIN
                    (SELECT
                        CAST(cr.id as string) AS id       
                    FROM
                        `cb_criteria` cr       
                    WHERE
                        concept_id IN (3024561)       
                        AND full_text LIKE '%_rank1]%'      ) a 
                        ON (c.path LIKE CONCAT('%.', a.id, '.%') 
                        OR c.path LIKE CONCAT('%.', a.id) 
                        OR c.path LIKE CONCAT(a.id, '.%') 
                        OR c.path = a.id) 
                WHERE
                    is_standard = 1 
                    AND is_selectable = 1)
            )  
            AND (
                measurement.PERSON_ID IN (SELECT
                    distinct person_id  
                FROM
                    `cb_search_person` cb_search_person  
                WHERE
                    cb_search_person.person_id IN (SELECT
                        person_id 
                    FROM
                        `person` p 
                    WHERE
                        race_concept_id IN (8527, 8516, 2100000001, 8515, 903096, 2000000008, 45882607, 1177221, 38003615, 8557) ) 
                    AND cb_search_person.person_id NOT IN (SELECT
                        criteria.person_id 
                    FROM
                        (SELECT
                            DISTINCT person_id, entry_date, concept_id 
                        FROM
                            `cb_search_all_events` 
                        WHERE
                            (concept_id IN(SELECT
                                DISTINCT c.concept_id 
                            FROM
                                `cb_criteria` c 
                            JOIN
                                (SELECT
                                    CAST(cr.id as string) AS id       
                                FROM
                                    `cb_criteria` cr       
                                WHERE
                                    concept_id IN (440508)       
                                    AND full_text LIKE '%_rank1]%'      ) a 
                                    ON (c.path LIKE CONCAT('%.', a.id, '.%') 
                                    OR c.path LIKE CONCAT('%.', a.id) 
                                    OR c.path LIKE CONCAT(a.id, '.%') 
                                    OR c.path = a.id) 
                            WHERE
                                is_standard = 1 
                                AND is_selectable = 1) 
                            AND is_standard = 1 )) criteria ) 
                    AND cb_search_person.person_id NOT IN (SELECT
                        person_id 
                    FROM
                        `person` p 
                    WHERE
                        race_concept_id IN (2100000001, 903096, 2000000008, 45882607, 1177221, 38003615, 8557) ) )
            )) measurement 
    LEFT JOIN
        `concept` m_standard_concept 
            ON measurement.measurement_concept_id = m_standard_concept.concept_id 
    LEFT JOIN
        `concept` m_type 
            ON measurement.measurement_type_concept_id = m_type.concept_id 
    LEFT JOIN
        `concept` m_operator 
            ON measurement.operator_concept_id = m_operator.concept_id 
    LEFT JOIN
        `concept` m_value 
            ON measurement.value_as_concept_id = m_value.concept_id 
    LEFT JOIN
        `concept` m_unit 
            ON measurement.unit_concept_id = m_unit.concept_id 
    LEFT JOIn
        `visit_occurrence` v 
            ON measurement.visit_occurrence_id = v.visit_occurrence_id 
    LEFT JOIN
        `concept` m_visit 
            ON v.visit_concept_id = m_visit.concept_id 
    LEFT JOIN
        `concept` m_source_concept 
            ON measurement.measurement_source_concept_id = m_source_concept.concept_id", sep="")

# Formulate a Cloud Storage destination path for the data exported from BigQuery.
# NOTE: By default data exported multiple times on the same day will overwrite older copies.
#       But data exported on a different days will write to a new location so that historical
#       copies can be kept as the dataset definition is changed.
measurement_92268616_path <- file.path(
  Sys.getenv("WORKSPACE_BUCKET"),
  "bq_exports",
  Sys.getenv("OWNER_EMAIL"),
  strftime(lubridate::now(), "%Y%m%d"),  # Comment out this line if you want the export to always overwrite.
  "measurement_92268616",
  "measurement_92268616_*.csv")
message(str_glue('The data will be written to {measurement_92268616_path}. Use this path when reading ',
                 'the data into your notebooks in the future.'))

# Perform the query and export the dataset to Cloud Storage as CSV files.
# NOTE: You only need to run `bq_table_save` once. After that, you can
#       just read data from the CSVs in Cloud Storage.
bq_table_save(
  bq_dataset_query(Sys.getenv("WORKSPACE_CDR"), dataset_92268616_measurement_sql, billing = Sys.getenv("GOOGLE_PROJECT")),
  measurement_92268616_path,
  destination_format = "CSV")


# Read the data directly from Cloud Storage into memory.
# NOTE: Alternatively you can `gsutil -m cp {measurement_92268616_path}` to copy these files
#       to the Jupyter disk.
read_bq_export_from_workspace_bucket <- function(export_path) {
  col_types <- cols(standard_concept_name = col_character(), standard_concept_code = col_character(), standard_vocabulary = col_character(), measurement_type_concept_name = col_character(), operator_concept_name = col_character(), value_as_concept_name = col_character(), unit_concept_name = col_character(), visit_occurrence_concept_name = col_character(), measurement_source_value = col_character(), source_concept_name = col_character(), source_concept_code = col_character(), source_vocabulary = col_character(), unit_source_value = col_character(), value_source_value = col_character())
  bind_rows(
    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),
        function(csv) {
          message(str_glue('Loading {csv}.'))
          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)
          if (is.null(col_types)) {
            col_types <- spec(chunk)
          }
          chunk
        }))
}
dataset_92268616_measurement_df <- read_bq_export_from_workspace_bucket(measurement_92268616_path)

dim(dataset_92268616_measurement_df)

head(dataset_92268616_measurement_df, 5)
```

```{r}
albumin_df <- dataset_92268616_measurement_df
```

## Attach Albumin.creatnine within feature window before index date

```{r}
#install.packages("tidyverse")
library(tidyverse)
library(data.table)

# Step 1: Convert 'albumin_df' and 'person_with_summary_inrange_indexdate_creatnine' to data.table for efficient join
albumin_df_dt <- as.data.table(albumin_df)
person_dt <- as.data.table(person_with_summary_inrange_indexdate_creatnine)

# Perform the join to add 'index_date' and 'race' from 'person' to 'albumin_df'
albumin_df_dt <- albumin_df_dt[person_dt, on = "person_id", `:=`(index_date = i.index_date, race = i.race)]

# Convert back to data.frame if necessary
albumin_df <- as.data.frame(albumin_df_dt)

# Step 2: Filter and rename specific standard concept names for "Albumin"
measurement_filtered_specific <- albumin_df %>%
  filter(
    standard_concept_name == "Albumin [Mass/volume] in Serum or Plasma"
  ) %>%
  rename(unified_albumin = value_as_number)  # Using a consistent name for Albumin

# Ensure both measurement_datetime and index_date are in Date format
measurement_filtered_specific <- measurement_filtered_specific %>%
  mutate(
    measurement_datetime = as.Date(measurement_datetime),
    index_date = as.Date(index_date)
  )

# Step 3: Filter rows where 'measurement_datetime' is within 365 days before 'index_date'
feature_window <- 365  # Define the feature window in days

measurement_filtered_window <- measurement_filtered_specific %>%
  filter((index_date - measurement_datetime) >= 0 & (index_date - measurement_datetime) <= feature_window)

# Step 4: For each person, select the first non-NA measurement within the feature window (closest to the index_date)
albumin_first_measurement <- measurement_filtered_window %>%
  filter(!is.na(unified_albumin)) %>%  # Ensure we only consider non-NA values
  group_by(person_id) %>%
  arrange(desc(measurement_datetime)) %>%  # Arrange to get the latest measurement within the window
  slice_head(n=1) %>%  # Select the first row (most recent non-NA measurement before index_date)
  mutate(date_of_albumin_measurement = measurement_datetime) %>%  # Capture the measurement date
  ungroup()

# Check the resulting data frame
print(albumin_first_measurement)

# Define the groups based on Albumin ranges provided
albumin_first_measurement <- albumin_first_measurement %>%
  mutate(group = case_when(
    unified_albumin < 30 ~ "< 30",
    unified_albumin >= 30 & unified_albumin < 300 ~ "30-299",
    unified_albumin >= 300 & unified_albumin < 2200 ~ "300-2199",
    unified_albumin >= 2200 ~ ">= 2200"
  ))

# Step 6: Calculate the count and percentage of each group, with race breakdown
group_summary <- albumin_first_measurement %>%
  group_by(group) %>%
  summarise(
    count = n(),
    white_count = sum(race == "White", na.rm = TRUE),
    black_count = sum(race == "Black or African American", na.rm = TRUE),
    asian_count = sum(race == "Asian", na.rm = TRUE)  # Include Asian race
  ) %>%
  mutate(
    percentage = (count / sum(count)) * 100,
    white_percentage = (white_count / count) * 100,
    black_percentage = (black_count / count) * 100,
    asian_percentage = (asian_count / count) * 100  # Calculate percentage for Asian race
  )

# Print the group summary with counts and percentages, including race breakdown
cat("Group summary with count and percentage, including race breakdown:\n")
print(group_summary)

# Step 7: Append `unified_albumin` and `date_of_albumin_measurement` to `person_with_summary_inrange_indexdate_creatnine`
person_with_summary_inrange_indexdate_creatnine_albumin <- person_with_summary_inrange_indexdate_creatnine %>%
  left_join(albumin_first_measurement %>% 
              select(person_id, unified_albumin, date_of_albumin_measurement), 
            by = "person_id")

# Print the updated dataset
cat("Updated person_with_summary_inrange_indexdate_creatnine_albumin dataset:\n")
print(person_with_summary_inrange_indexdate_creatnine_albumin)

# Display column names of the updated dataset
cat("Column names of person_with_summary_inrange_indexdate_creatnine_albumin:\n")
print(colnames(person_with_summary_inrange_indexdate_creatnine_albumin))

# Rename the column in the dataset
person_with_summary_inrange_indexdate_creatnine_albumin <- person_with_summary_inrange_indexdate_creatnine_albumin %>%
  rename(Albumin = unified_albumin)

# Print the updated column names to verify the change
print(colnames(person_with_summary_inrange_indexdate_creatnine_albumin))

```

## Albumin case count analysis

```{r}
library(dplyr)
library(knitr)

# Step 1: Filter the dataset for relevant Albumin.Creatinine or Microalbumin.Creatinine values, and group by race and disease_status

# For Black participants
black_albumin_counts <- person_with_summary_inrange_indexdate_creatnine_albumin %>%
  filter(!is.na(Albumin) & race == "Black or African American") %>%
  group_by(disease_status) %>%
  summarise(count = n()) %>%
  mutate(race = "Black Participants")

# For White participants
white_albumin_counts <- person_with_summary_inrange_indexdate_creatnine_albumin %>%
  filter(!is.na(Albumin) & race == "White") %>%
  group_by(disease_status) %>%
  summarise(count = n()) %>%
  mutate(race = "White Participants")

# For Asian participants
asian_albumin_counts <- person_with_summary_inrange_indexdate_creatnine_albumin %>%
  filter(!is.na(Albumin) & race == "Asian") %>%
  group_by(disease_status) %>%
  summarise(count = n()) %>%
  mutate(race = "Asian Participants")

# Combine all race groups into one table
combined_counts <- bind_rows(black_albumin_counts, white_albumin_counts, asian_albumin_counts)

# Step 2: Print the results in a neat table
kable(combined_counts, caption = "Number of Cases and Controls with Non-NA Albumin Values by Race")


```

Ensure there is no NA Albumin values

Ensured: Before filtering filtered_person_with_summary had 125,098 observations. There is o NA Albumin values in our selection

```{r}
library(dplyr)

# Remove rows where Albumin is NA
person_with_summary_inrange_indexdate_creatnine_albumin <- person_with_summary_inrange_indexdate_creatnine_albumin %>%
  filter(!is.na(Albumin))

# Count the number of cases and controls
cases_controls_count <- person_with_summary_inrange_indexdate_creatnine_albumin %>%
  group_by(disease_status) %>%
  summarise(count = n(), .groups = 'drop') %>%
  mutate(group = if_else(disease_status == 1, "Cases", "Controls"))

# Print the number of cases and controls
print("Number of cases and controls after removing rows with NA Albumin:")
print(cases_controls_count)

```

# Calculate Albumin.Creatnine

```{r}
library(dplyr)

# Ensure Albumin and serum_creatinine are numeric
filtered_person_with_summary <- person_with_summary_inrange_indexdate_creatnine_albumin %>%
  mutate(
    Albumin = as.numeric(Albumin),
    serum_creatinine = as.numeric(serum_creatinine)
  )

# Filter for people who have non-NA values for both Albumin and serum_creatinine
filtered_data_with_albumin_creatinine <- filtered_person_with_summary %>%
  filter(!is.na(Albumin) & !is.na(serum_creatinine)) %>%
  mutate(Albumin.Creatinine = Albumin / serum_creatinine)  # Calculate the ratio

# Calculate the number of cases and controls with non-NA time_to_event values
na_time_to_event_summary <- filtered_data_with_albumin_creatinine %>%
  filter(is.na(time_to_event)) %>%  # Only include those with NA time_to_event
  group_by(disease_status) %>%      # Group by disease_status
  summarise(count = n())            # Count the number of rows in each group

# Print the results
cat("Number of cases and controls with NA time_to_event among those with both Albumin and Creatinine values:\n")
print(na_time_to_event_summary)

# Display the column names of the updated dataset
cat("Column names of filtered_person_with_summary with Albumin.Creatinine added:\n")
print(colnames(filtered_data_with_albumin_creatinine))


```

## Albumin.Creatinine outlier analysis

```{r}
# Filter the dataset for person_id with Albumin.Creatinine value greater than 700 and count by disease_status
disease_status_above_700 <- filtered_data_with_albumin_creatinine %>%
  filter(Albumin.Creatinine >= 8000) %>%  # Filter rows where Albumin.Creatinine is greater than 700
  group_by(disease_status) %>%  # Group by disease_status
  summarise(person_count = n_distinct(person_id))  # Count the number of unique person_id

# Print the result
cat("Number of person_id with Albumin.Creatinine values greater than 700 by disease_status:\n")
print(disease_status_above_700)

```

```{r}
disease_status_above_8000 <- filtered_data_with_albumin_creatinine %>%
  filter(Albumin.Creatinine >= 8000) %>%  # Filter rows where Albumin.Creatinine is greater than or equal to 8000
  group_by(Albumin.Creatinine, disease_status) %>%  # Group by Albumin.Creatinine and disease_status
  summarise(person_count = n_distinct(person_id))  # Count the number of unique person_id

# Print the result
cat("Number of person_id with Albumin.Creatinine values greater than or equal to 8000 by disease_status:\n")
print(disease_status_above_8000)
```

## Save data

```{r}
# 74,012 participants, 232 variables, contains filter(serum_creatinine > 0) and non NA index date, filter(sex_at_birth %in% c("Male", "Female")), with Albumin.Creatinine values and event date after index date

write.csv(filtered_data_with_albumin_creatinine, "person_with_summary_inrange_indexdate_creatnine_albumin.csv", row.names = FALSE)


```

## Albumin.Creatnine case count analysis

```{r}
library(dplyr)
library(knitr)
filtered_data_with_albumin_creatinine <-  read.csv("person_with_summary_inrange_indexdate_creatnine_albumin.csv")

# Step 1: Filter the dataset for relevant Albumin.Creatinine or Microalbumin.Creatinine values, and group by race and disease_status

# For Black participants
black_albumin_creatnine_counts <- filtered_data_with_albumin_creatinine %>%
  filter(!is.na(Albumin.Creatinine) & race == "Black or African American") %>%
  group_by(disease_status) %>%
  summarise(count = n()) %>%
  mutate(race = "Black Participants")

# For White participants
white_albumin_creatnine_counts <- filtered_data_with_albumin_creatinine %>%
  filter(!is.na(Albumin.Creatinine) & race == "White") %>%
  group_by(disease_status) %>%
  summarise(count = n()) %>%
  mutate(race = "White Participants")

# For Asian participants
asian_albumin_creatnine_counts <- filtered_data_with_albumin_creatinine %>%
  filter(!is.na(Albumin.Creatinine) & race == "Asian") %>%
  group_by(disease_status) %>%
  summarise(count = n()) %>%
  mutate(race = "Asian Participants")

# Combine all race groups into one table
combined_counts <- bind_rows(black_albumin_creatnine_counts, white_albumin_creatnine_counts, asian_albumin_creatnine_counts)

# Step 2: Print the results in a neat table
kable(combined_counts, caption = "Number of Cases and Controls with Non-NA Albumin.Creatnine Values by Race")


```

# Part 1: Evaluation of the Kidney Failure Risk Equation (KFRE) with ALL OF US DATA: North American Equation

# MDRD Population

```{r}
# Load necessary libraries
library(dplyr)
library(survival)

# Load dataset
inrange_person_with_summary <- read.csv("person_with_summary_inrange_indexdate_creatnine_albumin.csv")

# Define a plausible range for eGFR and Albumin/Creatinine based on clinical thresholds for eGFR ≤ 60
plausible_eGFR_min <- 0
plausible_eGFR_max <- 60  # Kidney disease defined as eGFR < 60 mL/min/1.73 m²
#
plausible_acr_min <- 0
plausible_acr_max <- 3000
albuminuria_cutoff <- 10

# Step 1: Select relevant columns and rename Albumin_Creatinine for consistency
person_filtered_KFRE <- inrange_person_with_summary %>%
  select(
    person_id,
    sex_at_birth,
    eGFR_MDRD,
    Albumin_Creatinine = `Albumin.Creatinine`,
    age_precise,
    race,
    disease_status,
    time_to_event
    )

# Step 2: Data preprocessing and filtering based on plausible values for eGFR and ACR
person_filtered_KFRE <- person_filtered_KFRE %>%
  mutate(
    event = disease_status
  ) %>%
  filter(
    eGFR_MDRD >= plausible_eGFR_min & eGFR_MDRD < plausible_eGFR_max, 
    Albumin_Creatinine >= albuminuria_cutoff & Albumin_Creatinine <= plausible_acr_max  
  ) %>%
  # Log-transform Albumin/Creatinine ratio (ACR)
  mutate(logACR = log(Albumin_Creatinine)) %>%
  # Ensure complete cases (remove rows with missing data)
  filter(complete.cases(.))


# Filter out rows with non-NA time_to_event values
case_control_time_to_event_counts <- person_filtered_KFRE %>%
  filter(!is.na(time_to_event)) %>%  # Keep only rows with non-NA time_to_event
  group_by(disease_status) %>%  # Assuming "disease_status" column indicates cases and controls
  summarise(count = n())  # Count the number of rows in each group

# Print the result
print("Number of cases and controls with non-NA time_to_event values:")
print(case_control_time_to_event_counts)


# Step 3: Verify summary statistics for eGFR and Albumin/Creatinine
summary(person_filtered_KFRE$eGFR_value)
summary(person_filtered_KFRE$Albumin_Creatinine)

# Check how many records are retained after filtering
cat("Number of records after filtering:", nrow(person_filtered_KFRE), "\n")

# Perform a complete case analysis
initial_row_count <- nrow(person_filtered_KFRE)

complete_cases <- person_filtered_KFRE %>%
  filter(complete.cases(.))

final_row_count <- nrow(complete_cases)

# Check if any rows have been removed
rows_removed <- initial_row_count - final_row_count

if (rows_removed > 0) {
  cat(rows_removed, "rows have been removed during the complete case analysis.\n")
} else {
  cat("No rows have been removed during the complete case analysis.\n")
}

# Report the number of complete cases
num_complete_cases <- nrow(complete_cases)
cat("Number of complete cases:", num_complete_cases, "\n")

# Remove any rows with NA in time_to_event or disease_status
complete_cases <- complete_cases %>%
  filter(!is.na(time_to_event) & !is.na(disease_status))

# Check the event distribution
cat("Event distribution (disease_status):\n")
print(table(complete_cases$disease_status))

# Step 4: Define the formula for calculating βsum (log hazard)
calculate_beta_sum <- function(age, sex, eGFR, logACR) {
  (-0.2201 * (age / 10 - 7.036)) +
  (0.2467 * (ifelse(sex == "Male", 1, 0) - 0.5642)) -
  (0.5567 * (eGFR / 5 - 7.222)) +
  (0.4510 * (logACR - 5.137))
}

# Apply the formula to calculate βsum
complete_cases <- complete_cases %>%
  mutate(beta_sum = calculate_beta_sum(age_precise, sex_at_birth, eGFR_MDRD, logACR))

# Remove any rows with infinite or NA beta_sum
complete_cases <- complete_cases %>%
  filter(is.finite(beta_sum) & !is.na(beta_sum))

# Step 5: Calculate the risk for two and five years using the updated formulas
complete_cases <- complete_cases %>%
  mutate(
    risk_2_years = 100 * (1 - 0.975^exp(beta_sum)),  # Two-Year Risk calculation
    risk_5_years = 100 * (1 - 0.9240^exp(beta_sum))  # Five-Year Risk calculation
  )

# Save the data before filtering
before_filtering <- complete_cases

# Step 6: Apply the filtering to create consistent_data
consistent_data <- complete_cases %>%
  filter(!is.na(risk_5_years) & !is.na(risk_2_years))

# Step 7: Define and create the 2-year and 5-year outcomes based on disease_status and time_to_event
consistent_data <- consistent_data %>%
  mutate(
    outcome_2_years = ifelse(disease_status == 1 & time_to_event <= 2 * 365.25, 1, 0),
    outcome_5_years = ifelse(disease_status == 1 & time_to_event <= 5 * 365.25, 1, 0)
  )

# Find the rows that were removed by comparing before and after datasets
removed_rows <- anti_join(before_filtering, consistent_data, by = "person_id")

# Print the removed rows
if (nrow(removed_rows) > 0) {
  cat("Rows that were removed during filtering:\n")
  print(removed_rows)
} else {
  cat("No rows were removed during filtering.\n")
}

# Step 8: Verify the risk values
summary(consistent_data$risk_5_years)
summary(consistent_data$risk_2_years)

# Count the number of person_id entries with event == 1 and event == 0
event_count <- consistent_data %>%
  group_by(event) %>%
  summarise(count = n_distinct(person_id))

# Print the results
cat("Number of person_id with event == 1:", event_count$count[event_count$event == 1], "\n")
cat("Number of person_id with event == 0:", event_count$count[event_count$event == 0], "\n")

# Check the event distribution for the 2-year and 5-year outcomes
cat("\n2-Year Outcome Distribution:\n")
print(table(consistent_data$outcome_2_years))

cat("\n5-Year Outcome Distribution:\n")
print(table(consistent_data$outcome_5_years))


# Load necessary libraries
library(dplyr)
library(pROC)
library(survival)  # For Cox proportional hazards model
library(rms)  # For Harrell's C-statistic
library(kableExtra)
library(ggplot2)

# Function to calculate metrics with bootstrapping and specificity adjustment
calculate_metrics_with_bootstrap <- function(data, risk_col, outcome_col, time_col, time_limit, n_bootstrap = 5000) {
  
  # Record the start time
  start_time <- Sys.time()
  
  # Apply censoring: keep all data but treat events beyond the time limit as censored
  data_filtered <- data %>%
    mutate(
      censored_time = pmin(!!sym(time_col), time_limit * 365),  # Use the minimum to cap the time at the time limit
      event_status = ifelse(!!sym(outcome_col) == 1 & !!sym(time_col) <= time_limit * 365, 1, 0)  # 1 if event happened within time limit, else 0
    )
  
  # Filter out rows with missing or invalid data
  data_filtered <- data_filtered %>%
    filter(!is.na(!!sym(risk_col)) & !is.na(event_status) & !is.na(censored_time))
  
  # Calculate ROC and AUC
  roc_curve <- roc(data_filtered$event_status, data_filtered[[risk_col]], levels = c(0, 1), direction = "<")
  auc_value <- as.numeric(auc(roc_curve))  # AUC
  
  # Set specificity target to 0.9 and determine cutoff
  specificity_target <- 0.9
  optimal_cutoff <- coords(roc_curve, x = "best", ret = "threshold", best.method = "closest.topleft", transpose = TRUE)[1]
  
  # Adjust predictions based on the new cutoff
  pred_class <- ifelse(data_filtered[[risk_col]] >= optimal_cutoff, 1, 0)
  
  # Calculate confusion matrix statistics based on the new cutoff
  confusion_mat <- table(Predicted = pred_class, Actual = data_filtered$event_status)
  
  sensitivity <- confusion_mat[2, 2] / sum(confusion_mat[, 2])
  specificity <- confusion_mat[1, 1] / sum(confusion_mat[, 1])
  ppv <- confusion_mat[2, 2] / sum(confusion_mat[2, ])
  npv <- confusion_mat[1, 1] / sum(confusion_mat[1, ])
  
  # Bootstrapping AUC for confidence intervals
  auc_values <- numeric(n_bootstrap)
  c_stat_values <- numeric(n_bootstrap)  # Store bootstrapped C-statistic values
  
  # Running bootstrap iterations
  for (i in 1:n_bootstrap) {
    bootstrap_sample <- data_filtered[sample(1:nrow(data_filtered), replace = TRUE), ]
    roc_curve <- roc(bootstrap_sample$event_status, bootstrap_sample[[risk_col]])
    auc_values[i] <- as.numeric(auc(roc_curve))
    
    # Calculate Harrell's C-statistic for the bootstrap sample using censored survival
    cox_model_boot <- coxph(Surv(bootstrap_sample$censored_time, bootstrap_sample$event_status) ~ bootstrap_sample[[risk_col]], data = bootstrap_sample)
    c_stat_values[i] <- as.numeric(summary(cox_model_boot)$concordance[1])
  }
  
  # Calculate 95% confidence intervals
  auc_ci <- quantile(auc_values, probs = c(0.025, 0.975))
  c_stat_ci <- quantile(c_stat_values, probs = c(0.025, 0.975))
  
  # Calculate Harrell's C-statistic using the full dataset with censored survival
  cox_model <- coxph(Surv(censored_time, event_status) ~ data_filtered[[risk_col]], data = data_filtered)
  c_stat <- as.numeric(summary(cox_model)$concordance[1]) 
  
  # Record the end time
  end_time <- Sys.time()
  
  # Calculate total runtime
  total_runtime <- end_time - start_time
  
  # Print the number of bootstrap iterations completed and the total runtime
  cat("Number of bootstrap iterations run:", n_bootstrap, "\n")
  cat("Total runtime for the bootstrapping process:", total_runtime, "seconds\n")
  
  # Return the calculated metrics
  return(list(
    AUC = auc_value,
    AUC_Lower = auc_ci[1],
    AUC_Upper = auc_ci[2],
    C_Statistic = c_stat,
    C_Stat_Lower = c_stat_ci[1],  # Lower CI for Harrell's C-statistic
    C_Stat_Upper = c_stat_ci[2],  # Upper CI for Harrell's C-statistic
    Sensitivity = sensitivity,
    Specificity = specificity,
    PPV = ppv,
    NPV = npv,
    Optimal_Cutoff = optimal_cutoff  # Return the cutoff for specificity 0.9
  ))
}

# Function to summarize metrics for each racial group
summarize_risk_metrics <- function(data, race_label, time_limit) {
  # Calculate metrics for the specified time limit
  metrics <- calculate_metrics_with_bootstrap(data, paste0("risk_", time_limit, "_years"), paste0("outcome_", time_limit, "_years"), "time_to_event", time_limit)
  
  # Summarize the metrics
  case_control <- table(data[[paste0("outcome_", time_limit, "_years")]])
  
  return(data.frame(
    Race = race_label,
    `Time Horizon` = paste0(time_limit, " Years"),
    `AUC` = metrics$AUC,
    `AUC Lower CI` = metrics$AUC_Lower,
    `AUC Upper CI` = metrics$AUC_Upper,
    `Harrell's C-Statistic` = metrics$C_Statistic,
    `C-Statistic Lower CI` = metrics$C_Stat_Lower,
    `C-Statistic Upper CI` = metrics$C_Stat_Upper,
    `Sensitivity` = metrics$Sensitivity,
    `Specificity` = metrics$Specificity,
    `PPV` = metrics$PPV,
    `NPV` = metrics$NPV,
    `Cases` = as.numeric(case_control["1"]),
    `Controls` = as.numeric(case_control["0"])
  ))
}

# Evaluate metrics for both 2-year and 5-year outcomes across all racial groups
results_all_2yr <- summarize_risk_metrics(consistent_data, "All Races", 2)
results_all_5yr <- summarize_risk_metrics(consistent_data, "All Races", 5)

# Evaluate metrics for Black or African American group
results_black_2yr <- summarize_risk_metrics(consistent_data %>% filter(race == "Black or African American"), "Black or African American", 2)
results_black_5yr <- summarize_risk_metrics(consistent_data %>% filter(race == "Black or African American"), "Black or African American", 5)

# Evaluate metrics for White group
results_white_2yr <- summarize_risk_metrics(consistent_data %>% filter(race == "White"), "White", 2)
results_white_5yr <- summarize_risk_metrics(consistent_data %>% filter(race == "White"), "White", 5)

# Evaluate metrics for Asian group
results_asian_2yr <- summarize_risk_metrics(consistent_data %>% filter(race == "Asian"), "Asian", 2)
results_asian_5yr <- summarize_risk_metrics(consistent_data %>% filter(race == "Asian"), "Asian", 5)

# Combine all results into one table
final_results <- bind_rows(
  results_all_2yr, results_all_5yr,
  results_black_2yr, results_black_5yr,
  results_white_2yr, results_white_5yr,
  results_asian_2yr, results_asian_5yr
)

# Print the combined results
cat("\nCombined Risk Metrics for 2-Year and 5-Year Predictions, Including AUC, Harrell's C-Statistic, Sensitivity, Specificity, PPV, and NPV:\n")
print(final_results)


```

### Model evaluation: Observed vs predicted

```{r}
# Load necessary libraries
library(dplyr)
library(ggplot2)
library(tidyr)


# Ensure that 'consistent_data' has the necessary columns and the correct filtering has been applied
# We'll summarize the observed and predicted probabilities for each racial group

# Summarize observed counts and probabilities for 2-year and 5-year outcomes by race
observed_summary <- consistent_data %>%
  group_by(race) %>%
  summarise(
    observed_count_2_years = sum(outcome_2_years),      # Total observed events in 2 years
    observed_total_2_years = n(),                      # Total number of people observed in 2 years
    observed_2_years = mean(outcome_2_years),          # Average observed probability in 2 years
    observed_count_5_years = sum(outcome_5_years),     # Total observed events in 5 years
    observed_total_5_years = n(),                      # Total number of people observed in 5 years
    observed_5_years = mean(outcome_5_years)           # Average observed probability in 5 years
  )

# Summarize predicted counts and probabilities for 2-year and 5-year risk by race
predicted_summary <- consistent_data %>%
  group_by(race) %>%
  summarise(
    predicted_sum_2_years = sum(risk_2_years / 100),   # Sum of predicted probabilities for 2 years
    predicted_count_2_years = n(),                    # Total number of people predicted in 2 years
    predicted_2_years = mean(risk_2_years / 100),     # Average predicted probability in 2 years
    predicted_sum_5_years = sum(risk_5_years / 100),  # Sum of predicted probabilities for 5 years
    predicted_count_5_years = n(),                    # Total number of people predicted in 5 years
    predicted_5_years = mean(risk_5_years / 100)      # Average predicted probability in 5 years
  )

# Combine observed and predicted data
summary_data <- observed_summary %>%
  left_join(predicted_summary, by = "race") %>%
  pivot_longer(
    cols = c(observed_2_years, predicted_2_years, observed_5_years, predicted_5_years),
    names_to = c("measure", "time_period"),
    names_sep = "_",
    values_to = "probability"
  ) %>%
  mutate(
    count = case_when(
      measure == "observed" & time_period == "2" ~ observed_count_2_years,
      measure == "observed" & time_period == "5" ~ observed_count_5_years,
      measure == "predicted" & time_period == "2" ~ round(predicted_sum_2_years),
      measure == "predicted" & time_period == "5" ~ round(predicted_sum_5_years),
      TRUE ~ NA_real_
    ),
    total = case_when(
      measure == "observed" & time_period == "2" ~ observed_total_2_years,
      measure == "observed" & time_period == "5" ~ observed_total_5_years,
      measure == "predicted" & time_period == "2" ~ predicted_count_2_years,
      measure == "predicted" & time_period == "5" ~ predicted_count_5_years,
      TRUE ~ NA_real_
    ),
    label = paste0(count, "/", total) # Label in "count/total" format
  )

# Plot the observed and predicted probabilities for different racial groups
ggplot(summary_data, aes(x = race, y = probability * 100, fill = measure)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~ time_period, ncol = 1, scales = "free_y") +
  geom_text(aes(label = label), 
            position = position_dodge(width = 0.9), 
            vjust = -0.5, size = 3) + # Adjust size and position as needed
  labs(
    title = "Observed vs Predicted Probability of Kidney Failure by Race",
    x = "Race",
    y = "Probability of Event (%)",
    fill = "Measurement"
  ) +
  scale_fill_manual(values = c("observed" = "darkgray", "predicted" = "lightgray"), 
                    labels = c("Observed", "Predicted")) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(hjust = 0.5)
  )

```

# CKD-EPI 2009 Population

```{r}
# Load necessary libraries
library(dplyr)
library(survival)

# Load dataset
inrange_person_with_summary <- read.csv("person_with_summary_inrange_indexdate_creatnine_albumin.csv")

# Define a plausible range for eGFR and Albumin/Creatinine based on clinical thresholds for eGFR ≤ 60
plausible_eGFR_min <- 0
plausible_eGFR_max <- 60
plausible_acr_min <- 0
plausible_acr_max <- 3000
albuminuria_cutoff <- 10

# Step 1: Select relevant columns and rename Albumin_Creatinine for consistency
person_filtered_KFRE <- inrange_person_with_summary %>%
  select(
    person_id,
    sex_at_birth,
    eGFR_ckd_epi_2009,
    Albumin_Creatinine = `Albumin.Creatinine`,
    age_precise,
    race,
    disease_status,
    time_to_event
  )

# Step 2: Data preprocessing and filtering based on plausible values for eGFR and ACR
person_filtered_KFRE <- person_filtered_KFRE %>%
  mutate(
    event = disease_status
  ) %>%
  # Filter based on eGFR < 60 mL/min/1.73 m² and ACR > 30 mg/g for albuminuria
  filter(
    eGFR_ckd_epi_2009 >= plausible_eGFR_min & eGFR_ckd_epi_2009 <= plausible_eGFR_max, 
    Albumin_Creatinine >= albuminuria_cutoff & Albumin_Creatinine <= plausible_acr_max) %>% 
  # Log-transform Albumin/Creatinine ratio (ACR)
  mutate(logACR = log(Albumin_Creatinine)) %>%
  # Ensure complete cases (remove rows with missing data)
  filter(complete.cases(.))


# Filter out rows with non-NA time_to_event values
case_control_time_to_event_counts <- person_filtered_KFRE %>%
  filter(!is.na(time_to_event)) %>%  # Keep only rows with non-NA time_to_event
  group_by(disease_status) %>%  # Assuming "disease_status" column indicates cases and controls
  summarise(count = n())  # Count the number of rows in each group

# Print the result
print("Number of cases and controls with non-NA time_to_event values:")
print(case_control_time_to_event_counts)


# Step 3: Verify summary statistics for eGFR and Albumin/Creatinine
summary(person_filtered_KFRE$eGFR_value)
summary(person_filtered_KFRE$Albumin_Creatinine)

# Check how many records are retained after filtering
cat("Number of records after filtering:", nrow(person_filtered_KFRE), "\n")

# Perform a complete case analysis
initial_row_count <- nrow(person_filtered_KFRE)

complete_cases <- person_filtered_KFRE %>%
  filter(complete.cases(.))

final_row_count <- nrow(complete_cases)

# Check if any rows have been removed
rows_removed <- initial_row_count - final_row_count

if (rows_removed > 0) {
  cat(rows_removed, "rows have been removed during the complete case analysis.\n")
} else {
  cat("No rows have been removed during the complete case analysis.\n")
}

# Report the number of complete cases
num_complete_cases <- nrow(complete_cases)
cat("Number of complete cases:", num_complete_cases, "\n")

# Remove any rows with NA in time_to_event or disease_status
complete_cases <- complete_cases %>%
  filter(!is.na(time_to_event) & !is.na(disease_status))

# Check the event distribution
cat("Event distribution (disease_status):\n")
print(table(complete_cases$disease_status))

# Step 4: Define the formula for calculating βsum (log hazard)
calculate_beta_sum <- function(age, sex, eGFR, logACR) {
  (-0.2201 * (age / 10 - 7.036)) +
  (0.2467 * (ifelse(sex == "Male", 1, 0) - 0.5642)) -
  (0.5567 * (eGFR / 5 - 7.222)) +
  (0.4510 * (logACR - 5.137))
}

# Apply the formula to calculate βsum
complete_cases <- complete_cases %>%
  mutate(beta_sum = calculate_beta_sum(age_precise, sex_at_birth, eGFR_ckd_epi_2009, logACR))

# Remove any rows with infinite or NA beta_sum
complete_cases <- complete_cases %>%
  filter(is.finite(beta_sum) & !is.na(beta_sum))

# Step 5: Calculate the risk for two and five years using the updated formulas
complete_cases <- complete_cases %>%
  mutate(
    risk_2_years = 100 * (1 - 0.975^exp(beta_sum)),  # Two-Year Risk calculation
    risk_5_years = 100 * (1 - 0.9240^exp(beta_sum))  # Five-Year Risk calculation
  )

# Save the data before filtering
before_filtering <- complete_cases

# Step 6: Apply the filtering to create consistent_data
consistent_data <- complete_cases %>%
  filter(!is.na(risk_5_years) & !is.na(risk_2_years))

# Step 7: Define and create the 2-year and 5-year outcomes based on disease_status and time_to_event
consistent_data <- consistent_data %>%
  mutate(
    outcome_2_years = ifelse(disease_status == 1 & time_to_event <= 2 * 365.25, 1, 0),
    outcome_5_years = ifelse(disease_status == 1 & time_to_event <= 5 * 365.25, 1, 0)
  )

# Find the rows that were removed by comparing before and after datasets
removed_rows <- anti_join(before_filtering, consistent_data, by = "person_id")

# Print the removed rows
if (nrow(removed_rows) > 0) {
  cat("Rows that were removed during filtering:\n")
  print(removed_rows)
} else {
  cat("No rows were removed during filtering.\n")
}

# Step 8: Verify the risk values
summary(consistent_data$risk_5_years)
summary(consistent_data$risk_2_years)

# Count the number of person_id entries with event == 1 and event == 0
event_count <- consistent_data %>%
  group_by(event) %>%
  summarise(count = n_distinct(person_id))

# Print the results
cat("Number of person_id with event == 1:", event_count$count[event_count$event == 1], "\n")
cat("Number of person_id with event == 0:", event_count$count[event_count$event == 0], "\n")

# Check the event distribution for the 2-year and 5-year outcomes
cat("\n2-Year Outcome Distribution:\n")
print(table(consistent_data$outcome_2_years))

cat("\n5-Year Outcome Distribution:\n")
print(table(consistent_data$outcome_5_years))

# Load necessary libraries
library(dplyr)
library(pROC)
library(survival)  # For Cox proportional hazards model
library(rms)  # For Harrell's C-statistic
library(kableExtra)
library(ggplot2)

# Function to calculate metrics with bootstrapping and specificity adjustment
calculate_metrics_with_bootstrap <- function(data, risk_col, outcome_col, time_col, time_limit, n_bootstrap = 1000) {
  
  # Record the start time
  start_time <- Sys.time()
  
  # Apply censoring: keep all data but treat events beyond the time limit as censored
  data_filtered <- data %>%
    mutate(
      censored_time = pmin(!!sym(time_col), time_limit * 365),  # Use the minimum to cap the time at the time limit
      event_status = ifelse(!!sym(outcome_col) == 1 & !!sym(time_col) <= time_limit * 365, 1, 0)  # 1 if event happened within time limit, else 0
    )
  
  # Filter out rows with missing or invalid data
  data_filtered <- data_filtered %>%
    filter(!is.na(!!sym(risk_col)) & !is.na(event_status) & !is.na(censored_time))
  
  # Calculate ROC and AUC
  roc_curve <- roc(data_filtered$event_status, data_filtered[[risk_col]], levels = c(0, 1), direction = "<")
  auc_value <- as.numeric(auc(roc_curve))  # AUC
  
  # Set specificity target to 0.9 and determine cutoff
  specificity_target <- 0.9
  optimal_cutoff <- coords(roc_curve, x = "best", ret = "threshold", best.method = "closest.topleft", transpose = TRUE)[1]
  
  # Adjust predictions based on the new cutoff
  pred_class <- ifelse(data_filtered[[risk_col]] >= optimal_cutoff, 1, 0)
  
  # Calculate confusion matrix statistics based on the new cutoff
  confusion_mat <- table(Predicted = pred_class, Actual = data_filtered$event_status)
  
  sensitivity <- confusion_mat[2, 2] / sum(confusion_mat[, 2])
  specificity <- confusion_mat[1, 1] / sum(confusion_mat[, 1])
  ppv <- confusion_mat[2, 2] / sum(confusion_mat[2, ])
  npv <- confusion_mat[1, 1] / sum(confusion_mat[1, ])
  
  # Bootstrapping AUC for confidence intervals
  auc_values <- numeric(n_bootstrap)
  c_stat_values <- numeric(n_bootstrap)  # Store bootstrapped C-statistic values
  
  # Running bootstrap iterations
  for (i in 1:n_bootstrap) {
    bootstrap_sample <- data_filtered[sample(1:nrow(data_filtered), replace = TRUE), ]
    roc_curve <- roc(bootstrap_sample$event_status, bootstrap_sample[[risk_col]])
    auc_values[i] <- as.numeric(auc(roc_curve))
    
    # Calculate Harrell's C-statistic for the bootstrap sample using censored survival
    cox_model_boot <- coxph(Surv(bootstrap_sample$censored_time, bootstrap_sample$event_status) ~ bootstrap_sample[[risk_col]], data = bootstrap_sample)
    c_stat_values[i] <- as.numeric(summary(cox_model_boot)$concordance[1])
  }
  
  # Calculate 95% confidence intervals
  auc_ci <- quantile(auc_values, probs = c(0.025, 0.975))
  c_stat_ci <- quantile(c_stat_values, probs = c(0.025, 0.975))
  
  # Calculate Harrell's C-statistic using the full dataset with censored survival
  cox_model <- coxph(Surv(censored_time, event_status) ~ data_filtered[[risk_col]], data = data_filtered)
  c_stat <- as.numeric(summary(cox_model)$concordance[1]) 
  
  # Record the end time
  end_time <- Sys.time()
  
  # Calculate total runtime
  total_runtime <- end_time - start_time
  
  # Print the number of bootstrap iterations completed and the total runtime
  cat("Number of bootstrap iterations run:", n_bootstrap, "\n")
  cat("Total runtime for the bootstrapping process:", total_runtime, "seconds\n")
  
  # Return the calculated metrics
  return(list(
    AUC = auc_value,
    AUC_Lower = auc_ci[1],
    AUC_Upper = auc_ci[2],
    C_Statistic = c_stat,
    C_Stat_Lower = c_stat_ci[1],  # Lower CI for Harrell's C-statistic
    C_Stat_Upper = c_stat_ci[2],  # Upper CI for Harrell's C-statistic
    Sensitivity = sensitivity,
    Specificity = specificity,
    PPV = ppv,
    NPV = npv,
    Optimal_Cutoff = optimal_cutoff  # Return the cutoff for specificity 0.9
  ))
}

# Function to summarize metrics for each racial group
summarize_risk_metrics <- function(data, race_label, time_limit) {
  # Calculate metrics for the specified time limit
  metrics <- calculate_metrics_with_bootstrap(data, paste0("risk_", time_limit, "_years"), paste0("outcome_", time_limit, "_years"), "time_to_event", time_limit)
  
  # Summarize the metrics
  case_control <- table(data[[paste0("outcome_", time_limit, "_years")]])
  
  return(data.frame(
    Race = race_label,
    `Time Horizon` = paste0(time_limit, " Years"),
    `AUC` = metrics$AUC,
    `AUC Lower CI` = metrics$AUC_Lower,
    `AUC Upper CI` = metrics$AUC_Upper,
    `Harrell's C-Statistic` = metrics$C_Statistic,
    `C-Statistic Lower CI` = metrics$C_Stat_Lower,
    `C-Statistic Upper CI` = metrics$C_Stat_Upper,
    `Sensitivity` = metrics$Sensitivity,
    `Specificity` = metrics$Specificity,
    `PPV` = metrics$PPV,
    `NPV` = metrics$NPV,
    `Cases` = as.numeric(case_control["1"]),
    `Controls` = as.numeric(case_control["0"])
  ))
}

# Evaluate metrics for both 2-year and 5-year outcomes across all racial groups
results_all_2yr <- summarize_risk_metrics(consistent_data, "All Races", 2)
results_all_5yr <- summarize_risk_metrics(consistent_data, "All Races", 5)

# Evaluate metrics for Black or African American group
results_black_2yr <- summarize_risk_metrics(consistent_data %>% filter(race == "Black or African American"), "Black or African American", 2)
results_black_5yr <- summarize_risk_metrics(consistent_data %>% filter(race == "Black or African American"), "Black or African American", 5)

# Evaluate metrics for White group
results_white_2yr <- summarize_risk_metrics(consistent_data %>% filter(race == "White"), "White", 2)
results_white_5yr <- summarize_risk_metrics(consistent_data %>% filter(race == "White"), "White", 5)

# Evaluate metrics for Asian group
results_asian_2yr <- summarize_risk_metrics(consistent_data %>% filter(race == "Asian"), "Asian", 2)
results_asian_5yr <- summarize_risk_metrics(consistent_data %>% filter(race == "Asian"), "Asian", 5)

# Combine all results into one table
final_results <- bind_rows(
  results_all_2yr, results_all_5yr,
  results_black_2yr, results_black_5yr,
  results_white_2yr, results_white_5yr,
  results_asian_2yr, results_asian_5yr
)

# Print the combined results
cat("\nCombined Risk Metrics for 2-Year and 5-Year Predictions, Including AUC, Harrell's C-Statistic, Sensitivity, Specificity, PPV, and NPV:\n")
print(final_results)

```

### Model evaluation: Observed vs predicted

```{r}
# Load necessary libraries
library(dplyr)
library(ggplot2)

# Ensure that 'consistent_data' has the necessary columns and the correct filtering has been applied
# We'll summarize the observed and predicted probabilities for each racial group

# Summarize observed counts and probabilities for 2-year and 5-year outcomes by race
observed_summary <- consistent_data %>%
  group_by(race) %>%
  summarise(
    observed_count_2_years = sum(outcome_2_years),      # Total observed events in 2 years
    observed_total_2_years = n(),                      # Total number of people observed in 2 years
    observed_2_years = mean(outcome_2_years),          # Average observed probability in 2 years
    observed_count_5_years = sum(outcome_5_years),     # Total observed events in 5 years
    observed_total_5_years = n(),                      # Total number of people observed in 5 years
    observed_5_years = mean(outcome_5_years)           # Average observed probability in 5 years
  )

# Summarize predicted counts and probabilities for 2-year and 5-year risk by race
predicted_summary <- consistent_data %>%
  group_by(race) %>%
  summarise(
    predicted_sum_2_years = sum(risk_2_years / 100),   # Sum of predicted probabilities for 2 years
    predicted_count_2_years = n(),                    # Total number of people predicted in 2 years
    predicted_2_years = mean(risk_2_years / 100),     # Average predicted probability in 2 years
    predicted_sum_5_years = sum(risk_5_years / 100),  # Sum of predicted probabilities for 5 years
    predicted_count_5_years = n(),                    # Total number of people predicted in 5 years
    predicted_5_years = mean(risk_5_years / 100)      # Average predicted probability in 5 years
  )

# Combine observed and predicted data
summary_data <- observed_summary %>%
  left_join(predicted_summary, by = "race") %>%
  pivot_longer(
    cols = c(observed_2_years, predicted_2_years, observed_5_years, predicted_5_years),
    names_to = c("measure", "time_period"),
    names_sep = "_",
    values_to = "probability"
  ) %>%
  mutate(
    count = case_when(
      measure == "observed" & time_period == "2" ~ observed_count_2_years,
      measure == "observed" & time_period == "5" ~ observed_count_5_years,
      measure == "predicted" & time_period == "2" ~ round(predicted_sum_2_years),
      measure == "predicted" & time_period == "5" ~ round(predicted_sum_5_years),
      TRUE ~ NA_real_
    ),
    total = case_when(
      measure == "observed" & time_period == "2" ~ observed_total_2_years,
      measure == "observed" & time_period == "5" ~ observed_total_5_years,
      measure == "predicted" & time_period == "2" ~ predicted_count_2_years,
      measure == "predicted" & time_period == "5" ~ predicted_count_5_years,
      TRUE ~ NA_real_
    ),
    label = paste0(count, "/", total) # Label in "count/total" format
  )

# Plot the observed and predicted probabilities for different racial groups
ggplot(summary_data, aes(x = race, y = probability * 100, fill = measure)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~ time_period, ncol = 1, scales = "free_y") +
  geom_text(aes(label = label), 
            position = position_dodge(width = 0.9), 
            vjust = -0.5, size = 3) + # Adjust size and position as needed
  labs(
    title = "Observed vs Predicted Probability of Kidney Failure by Race",
    x = "Race",
    y = "Probability of Event (%)",
    fill = "Measurement"
  ) +
  scale_fill_manual(values = c("observed" = "darkgray", "predicted" = "lightgray"), 
                    labels = c("Observed", "Predicted")) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(hjust = 0.5)
  )
```

# CKD-EPI 2021 Population

```{r}
# Load necessary libraries
library(dplyr)
library(survival)

# Load dataset
inrange_person_with_summary <- read.csv("person_with_summary_inrange_indexdate_creatnine_albumin.csv")

# Define a plausible range for eGFR and Albumin/Creatinine based on clinical thresholds for eGFR ≤ 60
plausible_eGFR_min <- 0
plausible_eGFR_max <- 60  # Kidney disease defined as eGFR < 60 mL/min/1.73 m²

plausible_acr_min <- 0
plausible_acr_max <- 3000
albuminuria_cutoff <- 10

# Step 1: Select relevant columns and rename Albumin_Creatinine for consistency
person_filtered_KFRE <- inrange_person_with_summary %>%
  select(
    person_id,
    sex_at_birth,
    eGFR_ckd_epi_2021,
    Albumin_Creatinine = `Albumin.Creatinine`,
    age_precise,
    race,
    disease_status,
    time_to_event
  )

# Step 2: Data preprocessing and filtering based on plausible values for eGFR and ACR
person_filtered_KFRE <- person_filtered_KFRE %>%
  mutate(
    event = disease_status
  ) %>%
  # Filter based on eGFR < 60 mL/min/1.73 m² and ACR > 30 mg/g for albuminuria
  filter(
    eGFR_ckd_epi_2021 >= plausible_eGFR_min & eGFR_ckd_epi_2021 <= plausible_eGFR_max,
    Albumin_Creatinine >= albuminuria_cutoff & Albumin_Creatinine <= plausible_acr_max) %>% 
  # Log-transform Albumin/Creatinine ratio (ACR)
  mutate(logACR = log(Albumin_Creatinine)) %>%
  # Ensure complete cases (remove rows with missing data)
  filter(complete.cases(.))


# Filter out rows with non-NA time_to_event values
case_control_time_to_event_counts <- person_filtered_KFRE %>%
  filter(!is.na(time_to_event)) %>%  # Keep only rows with non-NA time_to_event
  group_by(disease_status) %>%  # Assuming "disease_status" column indicates cases and controls
  summarise(count = n())  # Count the number of rows in each group

# Print the result
print("Number of cases and controls with non-NA time_to_event values:")
print(case_control_time_to_event_counts)


# Step 3: Verify summary statistics for eGFR and Albumin/Creatinine
summary(person_filtered_KFRE$eGFR_value)
summary(person_filtered_KFRE$Albumin_Creatinine)

# Check how many records are retained after filtering
cat("Number of records after filtering:", nrow(person_filtered_KFRE), "\n")

# Perform a complete case analysis
initial_row_count <- nrow(person_filtered_KFRE)

complete_cases <- person_filtered_KFRE %>%
  filter(complete.cases(.))

final_row_count <- nrow(complete_cases)

# Check if any rows have been removed
rows_removed <- initial_row_count - final_row_count

if (rows_removed > 0) {
  cat(rows_removed, "rows have been removed during the complete case analysis.\n")
} else {
  cat("No rows have been removed during the complete case analysis.\n")
}

# Report the number of complete cases
num_complete_cases <- nrow(complete_cases)
cat("Number of complete cases:", num_complete_cases, "\n")

# Remove any rows with NA in time_to_event or disease_status
complete_cases <- complete_cases %>%
  filter(!is.na(time_to_event) & !is.na(disease_status))

# Check the event distribution
cat("Event distribution (disease_status):\n")
print(table(complete_cases$disease_status))

# Step 4: Define the formula for calculating βsum (log hazard)
calculate_beta_sum <- function(age, sex, eGFR, logACR) {
  (-0.2201 * (age / 10 - 7.036)) +
  (0.2467 * (ifelse(sex == "Male", 1, 0) - 0.5642)) -
  (0.5567 * (eGFR / 5 - 7.222)) +
  (0.4510 * (logACR - 5.137))
}

# Apply the formula to calculate βsum
complete_cases <- complete_cases %>%
  mutate(beta_sum = calculate_beta_sum(age_precise, sex_at_birth, eGFR_ckd_epi_2021, logACR))

# Remove any rows with infinite or NA beta_sum
complete_cases <- complete_cases %>%
  filter(is.finite(beta_sum) & !is.na(beta_sum))

# Step 5: Calculate the risk for two and five years using the updated formulas
complete_cases <- complete_cases %>%
  mutate(
    risk_2_years = 100 * (1 - 0.975^exp(beta_sum)),  # Two-Year Risk calculation
    risk_5_years = 100 * (1 - 0.9240^exp(beta_sum))  # Five-Year Risk calculation
  )

# Save the data before filtering
before_filtering <- complete_cases

# Step 6: Apply the filtering to create consistent_data
consistent_data <- complete_cases %>%
  filter(!is.na(risk_5_years) & !is.na(risk_2_years))

# Step 7: Define and create the 2-year and 5-year outcomes based on disease_status and time_to_event
consistent_data <- consistent_data %>%
  mutate(
    outcome_2_years = ifelse(disease_status == 1 & time_to_event <= 2 * 365.25, 1, 0),
    outcome_5_years = ifelse(disease_status == 1 & time_to_event <= 5 * 365.25, 1, 0)
  )

# Find the rows that were removed by comparing before and after datasets
removed_rows <- anti_join(before_filtering, consistent_data, by = "person_id")

# Print the removed rows
if (nrow(removed_rows) > 0) {
  cat("Rows that were removed during filtering:\n")
  print(removed_rows)
} else {
  cat("No rows were removed during filtering.\n")
}

# Step 8: Verify the risk values
summary(consistent_data$risk_5_years)
summary(consistent_data$risk_2_years)

# Count the number of person_id entries with event == 1 and event == 0
event_count <- consistent_data %>%
  group_by(event) %>%
  summarise(count = n_distinct(person_id))

# Print the results
cat("Number of person_id with event == 1:", event_count$count[event_count$event == 1], "\n")
cat("Number of person_id with event == 0:", event_count$count[event_count$event == 0], "\n")

# Check the event distribution for the 2-year and 5-year outcomes
cat("\n2-Year Outcome Distribution:\n")
print(table(consistent_data$outcome_2_years))

cat("\n5-Year Outcome Distribution:\n")
print(table(consistent_data$outcome_5_years))


# Load necessary libraries
library(dplyr)
library(pROC)
library(survival)  # For Cox proportional hazards model
library(rms)  # For Harrell's C-statistic
library(kableExtra)
library(ggplot2)

# Function to calculate metrics with bootstrapping and specificity adjustment
calculate_metrics_with_bootstrap <- function(data, risk_col, outcome_col, time_col, time_limit, n_bootstrap = 1000) {
  
  # Record the start time
  start_time <- Sys.time()
  
  # Apply censoring: keep all data but treat events beyond the time limit as censored
  data_filtered <- data %>%
    mutate(
      censored_time = pmin(!!sym(time_col), time_limit * 365),  # Use the minimum to cap the time at the time limit
      event_status = ifelse(!!sym(outcome_col) == 1 & !!sym(time_col) <= time_limit * 365, 1, 0)  # 1 if event happened within time limit, else 0
    )
  
  # Filter out rows with missing or invalid data
  data_filtered <- data_filtered %>%
    filter(!is.na(!!sym(risk_col)) & !is.na(event_status) & !is.na(censored_time))
  
  # Calculate ROC and AUC
  roc_curve <- roc(data_filtered$event_status, data_filtered[[risk_col]], levels = c(0, 1), direction = "<")
  auc_value <- as.numeric(auc(roc_curve))  # AUC
  
  # Set specificity target to 0.9 and determine cutoff
  specificity_target <- 0.9
  optimal_cutoff <- coords(roc_curve, x = "best", ret = "threshold", best.method = "closest.topleft", transpose = TRUE)[1]
  
  # Adjust predictions based on the new cutoff
  pred_class <- ifelse(data_filtered[[risk_col]] >= optimal_cutoff, 1, 0)
  
  # Calculate confusion matrix statistics based on the new cutoff
  confusion_mat <- table(Predicted = pred_class, Actual = data_filtered$event_status)
  
  sensitivity <- confusion_mat[2, 2] / sum(confusion_mat[, 2])
  specificity <- confusion_mat[1, 1] / sum(confusion_mat[, 1])
  ppv <- confusion_mat[2, 2] / sum(confusion_mat[2, ])
  npv <- confusion_mat[1, 1] / sum(confusion_mat[1, ])
  
  # Bootstrapping AUC for confidence intervals
  auc_values <- numeric(n_bootstrap)
  c_stat_values <- numeric(n_bootstrap)  # Store bootstrapped C-statistic values
  
  # Running bootstrap iterations
  for (i in 1:n_bootstrap) {
    bootstrap_sample <- data_filtered[sample(1:nrow(data_filtered), replace = TRUE), ]
    roc_curve <- roc(bootstrap_sample$event_status, bootstrap_sample[[risk_col]])
    auc_values[i] <- as.numeric(auc(roc_curve))
    
    # Calculate Harrell's C-statistic for the bootstrap sample using censored survival
    cox_model_boot <- coxph(Surv(bootstrap_sample$censored_time, bootstrap_sample$event_status) ~ bootstrap_sample[[risk_col]], data = bootstrap_sample)
    c_stat_values[i] <- as.numeric(summary(cox_model_boot)$concordance[1])
  }
  
  # Calculate 95% confidence intervals
  auc_ci <- quantile(auc_values, probs = c(0.025, 0.975))
  c_stat_ci <- quantile(c_stat_values, probs = c(0.025, 0.975))
  
  # Calculate Harrell's C-statistic using the full dataset with censored survival
  cox_model <- coxph(Surv(censored_time, event_status) ~ data_filtered[[risk_col]], data = data_filtered)
  c_stat <- as.numeric(summary(cox_model)$concordance[1]) 
  
  # Record the end time
  end_time <- Sys.time()
  
  # Calculate total runtime
  total_runtime <- end_time - start_time
  
  # Print the number of bootstrap iterations completed and the total runtime
  cat("Number of bootstrap iterations run:", n_bootstrap, "\n")
  cat("Total runtime for the bootstrapping process:", total_runtime, "seconds\n")
  
  # Return the calculated metrics
  return(list(
    AUC = auc_value,
    AUC_Lower = auc_ci[1],
    AUC_Upper = auc_ci[2],
    C_Statistic = c_stat,
    C_Stat_Lower = c_stat_ci[1],  # Lower CI for Harrell's C-statistic
    C_Stat_Upper = c_stat_ci[2],  # Upper CI for Harrell's C-statistic
    Sensitivity = sensitivity,
    Specificity = specificity,
    PPV = ppv,
    NPV = npv,
    Optimal_Cutoff = optimal_cutoff  # Return the cutoff for specificity 0.9
  ))
}

# Function to summarize metrics for each racial group
summarize_risk_metrics <- function(data, race_label, time_limit) {
  # Calculate metrics for the specified time limit
  metrics <- calculate_metrics_with_bootstrap(data, paste0("risk_", time_limit, "_years"), paste0("outcome_", time_limit, "_years"), "time_to_event", time_limit)
  
  # Summarize the metrics
  case_control <- table(data[[paste0("outcome_", time_limit, "_years")]])
  
  return(data.frame(
    Race = race_label,
    `Time Horizon` = paste0(time_limit, " Years"),
    `AUC` = metrics$AUC,
    `AUC Lower CI` = metrics$AUC_Lower,
    `AUC Upper CI` = metrics$AUC_Upper,
    `Harrell's C-Statistic` = metrics$C_Statistic,
    `C-Statistic Lower CI` = metrics$C_Stat_Lower,
    `C-Statistic Upper CI` = metrics$C_Stat_Upper,
    `Sensitivity` = metrics$Sensitivity,
    `Specificity` = metrics$Specificity,
    `PPV` = metrics$PPV,
    `NPV` = metrics$NPV,
    `Cases` = as.numeric(case_control["1"]),
    `Controls` = as.numeric(case_control["0"])
  ))
}

# Evaluate metrics for both 2-year and 5-year outcomes across all racial groups
results_all_2yr <- summarize_risk_metrics(consistent_data, "All Races", 2)
results_all_5yr <- summarize_risk_metrics(consistent_data, "All Races", 5)

# Evaluate metrics for Black or African American group
results_black_2yr <- summarize_risk_metrics(consistent_data %>% filter(race == "Black or African American"), "Black or African American", 2)
results_black_5yr <- summarize_risk_metrics(consistent_data %>% filter(race == "Black or African American"), "Black or African American", 5)

# Evaluate metrics for White group
results_white_2yr <- summarize_risk_metrics(consistent_data %>% filter(race == "White"), "White", 2)
results_white_5yr <- summarize_risk_metrics(consistent_data %>% filter(race == "White"), "White", 5)

# Evaluate metrics for Asian group
results_asian_2yr <- summarize_risk_metrics(consistent_data %>% filter(race == "Asian"), "Asian", 2)
results_asian_5yr <- summarize_risk_metrics(consistent_data %>% filter(race == "Asian"), "Asian", 5)

# Combine all results into one table
final_results <- bind_rows(
  results_all_2yr, results_all_5yr,
  results_black_2yr, results_black_5yr,
  results_white_2yr, results_white_5yr,
  results_asian_2yr, results_asian_5yr
)

# Print the combined results
cat("\nCombined Risk Metrics for 2-Year and 5-Year Predictions, Including AUC, Harrell's C-Statistic, Sensitivity, Specificity, PPV, and NPV:\n")
print(final_results)



```

### Model evaluation:

```{r}
# Load necessary libraries
library(dplyr)
library(ggplot2)

# Ensure that 'consistent_data' has the necessary columns and the correct filtering has been applied
# We'll summarize the observed and predicted probabilities for each racial group

# Summarize observed counts and probabilities for 2-year and 5-year outcomes by race
observed_summary <- consistent_data %>%
  group_by(race) %>%
  summarise(
    observed_count_2_years = sum(outcome_2_years),      # Total observed events in 2 years
    observed_total_2_years = n(),                      # Total number of people observed in 2 years
    observed_2_years = mean(outcome_2_years),          # Average observed probability in 2 years
    observed_count_5_years = sum(outcome_5_years),     # Total observed events in 5 years
    observed_total_5_years = n(),                      # Total number of people observed in 5 years
    observed_5_years = mean(outcome_5_years)           # Average observed probability in 5 years
  )

# Summarize predicted counts and probabilities for 2-year and 5-year risk by race
predicted_summary <- consistent_data %>%
  group_by(race) %>%
  summarise(
    predicted_sum_2_years = sum(risk_2_years / 100),   # Sum of predicted probabilities for 2 years
    predicted_count_2_years = n(),                    # Total number of people predicted in 2 years
    predicted_2_years = mean(risk_2_years / 100),     # Average predicted probability in 2 years
    predicted_sum_5_years = sum(risk_5_years / 100),  # Sum of predicted probabilities for 5 years
    predicted_count_5_years = n(),                    # Total number of people predicted in 5 years
    predicted_5_years = mean(risk_5_years / 100)      # Average predicted probability in 5 years
  )

# Combine observed and predicted data
summary_data <- observed_summary %>%
  left_join(predicted_summary, by = "race") %>%
  pivot_longer(
    cols = c(observed_2_years, predicted_2_years, observed_5_years, predicted_5_years),
    names_to = c("measure", "time_period"),
    names_sep = "_",
    values_to = "probability"
  ) %>%
  mutate(
    count = case_when(
      measure == "observed" & time_period == "2" ~ observed_count_2_years,
      measure == "observed" & time_period == "5" ~ observed_count_5_years,
      measure == "predicted" & time_period == "2" ~ round(predicted_sum_2_years),
      measure == "predicted" & time_period == "5" ~ round(predicted_sum_5_years),
      TRUE ~ NA_real_
    ),
    total = case_when(
      measure == "observed" & time_period == "2" ~ observed_total_2_years,
      measure == "observed" & time_period == "5" ~ observed_total_5_years,
      measure == "predicted" & time_period == "2" ~ predicted_count_2_years,
      measure == "predicted" & time_period == "5" ~ predicted_count_5_years,
      TRUE ~ NA_real_
    ),
    label = paste0(count, "/", total) # Label in "count/total" format
  )

# Plot the observed and predicted probabilities for different racial groups
ggplot(summary_data, aes(x = race, y = probability * 100, fill = measure)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~ time_period, ncol = 1, scales = "free_y") +
  geom_text(aes(label = label), 
            position = position_dodge(width = 0.9), 
            vjust = -0.5, size = 3) + # Adjust size and position as needed
  labs(
    title = "Observed vs Predicted Probability of Kidney Failure by Race",
    x = "Race",
    y = "Probability of Event (%)",
    fill = "Measurement"
  ) +
  scale_fill_manual(values = c("observed" = "darkgray", "predicted" = "lightgray"), 
                    labels = c("Observed", "Predicted")) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(hjust = 0.5)
  )
```

# Model comparison: Observed vs predicted

```{r}
library(knitr)
library(dplyr)
library(tidyr)
library(lubridate)
library(purrr)
library(stringr)

#install.packages("survival")
#install.packages("survminer")

# Load the necessary libraries
library(survival)

# install.packages("RColorBrewer")
library(RColorBrewer)

# Install and load necessary packages
#install.packages("caret")
#install.packages("ggplot2")
library(caret)
library(ggplot2)

# Load necessary libraries
#library(pROC)



# Create the MDRD 60 data frame with updated observed and predicted values
mdrd_greaterthan60 <- data.frame(
  race = c("Asian", "Black or African American", "White"),
  
  # Observed and predicted 2-year event rates from the plot
  observed_2_years = c(28 / 2750, 177 / 14593, 207 / 56992),
  predicted_2_years = c(4 / 2750, 33 / 14593, 62 / 56992),
  
  # Observed and predicted 5-year event rates from the plot
  observed_5_years = c(35 / 2750, 283 / 14593, 321 / 56992),
  predicted_5_years = c(12 / 2750, 88 / 14593, 178 / 56992),
  
  # Counts and totals for observed 2-year data
  observed_count_2_years = c(28, 177, 207),
  observed_total_2_years = c(2750, 14593, 56992),
  
  # Counts and totals for predicted 2-year data
  predicted_count_2_years = c(4, 33, 62),
  predicted_total_2_years = c(2750, 14593, 56992),
  
  # Counts and totals for observed 5-year data
  observed_count_5_years = c(35, 283, 321),
  observed_total_5_years = c(2750, 14593, 56992),
  
  # Counts and totals for predicted 5-year data
  predicted_count_5_years = c(12, 88, 178),
  predicted_total_5_years = c(2750, 14593, 56992)
)



# Save the data frame to a CSV file
#write.csv(mdrd_60, "mdrd_60.csv", row.names = FALSE)


# Create the CKD-EPI-2009-50 data frame with updated observed and predicted values from the image
ckd_epi_2009_greaterthan60 <- data.frame(
  race = c("Asian", "Black or African American", "White"),
  
  # Observed and predicted 2-year event rates from the plot
  observed_2_years = c(28 / 2750, 177 / 14593, 207 / 56992),
  predicted_2_years = c(4 / 2750, 36 / 14593, 64 / 56992),
  
  # Observed and predicted 5-year event rates from the plot
  observed_5_years = c(35 / 2750, 283 / 14593, 321 / 56992),
  predicted_5_years = c(12 / 2750, 98 / 14593, 184 / 56992),
  
  # Counts and totals for observed 2-year data
  observed_count_2_years = c(28, 177, 207),
  observed_total_2_years = c(2750, 14593, 56992),
  
  # Counts and totals for predicted 2-year data
  predicted_count_2_years = c(4, 36, 64),
  predicted_total_2_years = c(2750, 14593, 56992),
  
  # Counts and totals for observed 5-year data
  observed_count_5_years = c(35, 283, 321),
  observed_total_5_years = c(2750, 14593, 56992),
  
  # Counts and totals for predicted 5-year data
  predicted_count_5_years = c(12, 98, 184),
  predicted_total_5_years = c(2750, 14593, 56992)
)


# Save the data frame to a CSV file
#write.csv(ckd_epi_2009_60, "ckd_epi_2009_50.csv", row.names = FALSE)


# Create the CKD-EPI-2021-60 data frame with updated observed and predicted values from the image
ckd_epi_2021_greaterthan60 <- data.frame(
  race = c("Asian", "Black or African American", "White"),
  
  # Observed and predicted 2-year event rates from the plot
  observed_2_years = c(28 / 2750, 177 / 14593, 207 / 56992),
  predicted_2_years = c(4 / 2750, 42 / 14593, 55 / 56992),
  
  # Observed and predicted 5-year event rates from the plot
  observed_5_years = c(35 / 2750, 283 / 14593, 321 / 56992),
  predicted_5_years = c(11 / 2750, 114 / 14593, 155 / 56992),
  
  # Counts and totals for observed 2-year data
  observed_count_2_years = c(28, 177, 207),
  observed_total_2_years = c(2750, 14593, 56992),
  
  # Counts and totals for predicted 2-year data
  predicted_count_2_years = c(4, 42, 55),
  predicted_total_2_years = c(2750, 14593, 56992),
  
  # Counts and totals for observed 5-year data
  observed_count_5_years = c(35, 283, 321),
  observed_total_5_years = c(2750, 14593, 56992),
  
  # Counts and totals for predicted 5-year data
  predicted_count_5_years = c(11, 114, 155),
  predicted_total_5_years = c(2750, 14593, 56992)
)


# Load necessary libraries
library(ggplot2)
library(dplyr)
library(tidyr)

# Combine the datasets into a single data frame with specified model names
combined_data <- bind_rows(
  mdrd_greaterthan60 %>% mutate(model = "MDRD"),
  ckd_epi_2009_greaterthan60 %>% mutate(model = "CKD-EPI 2009"),
  ckd_epi_2021_greaterthan60 %>% mutate(model = "CKD-EPI 2021")
)

# Create a separate column for the observed and predicted counts in the long format
combined_long <- combined_data %>%
  pivot_longer(
    cols = c(observed_2_years, predicted_2_years, observed_5_years, predicted_5_years),
    names_to = "measure_time",
    values_to = "probability"
  ) %>%
  # Split measure_time correctly into measure and time_period
  separate(measure_time, into = c("measure", "time_period"), sep = "_", extra = "drop", fill = "right") %>%
  mutate(
    measure = ifelse(measure == "observed", "Observed", "Predicted"),
    time_period = ifelse(time_period == "2", "2 Years", "5 Years"),
    observed_count = case_when(
      time_period == "2 Years" ~ observed_count_2_years,
      time_period == "5 Years" ~ observed_count_5_years
    ),
    observed_total = case_when(
      time_period == "2 Years" ~ observed_total_2_years,
      time_period == "5 Years" ~ observed_total_5_years
    ),
    predicted_count = case_when(
      time_period == "2 Years" ~ predicted_count_2_years,
      time_period == "5 Years" ~ predicted_count_5_years
    ),
    predicted_total = case_when(
      time_period == "2 Years" ~ predicted_total_2_years,
      time_period == "5 Years" ~ predicted_total_5_years
    ),
    label = ifelse(measure == "Observed", 
                   paste0(observed_count, "/", observed_total), 
                   paste0(predicted_count, "/", predicted_total))
  )

# Ensure that 'model' and 'race' columns are in the correct order
combined_long$model <- factor(combined_long$model, levels = c("MDRD", "CKD-EPI 2009", "CKD-EPI 2021"))
combined_long$race <- factor(combined_long$race, levels = c("White", "Black or African American", "Asian"))

# Create a dataset for the common observed bars for each race and time period
observed_data <- combined_long %>%
  filter(measure == "Observed") %>%
  group_by(race, time_period) %>%
  summarise(probability = mean(probability), observed_count = mean(observed_count), observed_total = mean(observed_total), .groups = 'drop') %>%
  mutate(model = "Observed", measure = "Observed", 
         label = paste0(observed_count, "/", observed_total))  # Add label for observed counts

# Combine observed and predicted data for the plot
combined_for_plot <- bind_rows(combined_long %>% filter(measure == "Predicted"), observed_data)

# Ensure 'model' is ordered correctly with "Observed" appearing first
combined_for_plot$model <- factor(combined_for_plot$model, levels = c("Observed", "MDRD", "CKD-EPI 2009", "CKD-EPI 2021"))

# Define colors for a more professional, grayscale-like palette
colors <- c("Observed" = "black", "Predicted" = "gray70")

# Plot with the common observed bar for each race and time period
final_plot <- ggplot(combined_for_plot, aes(x = model, y = probability * 100, fill = measure)) +
  geom_bar(stat = "identity", position = "dodge", width = 0.6) +  # Reduced bar width for better clarity
  geom_text(aes(label = label), 
            position = position_dodge(width = 0.6), 
            vjust = 1.5, size = 4.5, color = "white", fontface = "bold") +  # Adjust label size, position, and color
  facet_wrap(~ race + time_period, ncol = 2, scales = "free_y", shrink = TRUE) +  # Use 'ncol = 2' to get 2 graphs per row
  labs(
    title = "Observed vs Predicted Probability of Kidney Failure by Race and Model",
    x = "Model Type",
    y = "Probability of Event (%)",
    fill = "Measurement"
  ) +
  scale_fill_manual(values = colors, 
                    labels = c("Observed", "Predicted")) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 12),  # Increase x-axis text size
    axis.text.y = element_text(size = 12),  # Increase y-axis text size
    plot.title = element_text(hjust = 0.5, size = 18, face = "bold"),  # Increase title size and bold it
    legend.position = "bottom",
    legend.text = element_text(size = 12),  # Adjust legend text size
    strip.text = element_text(size = 14, face = "bold"),  # Adjust facet label font size and make it bold
    panel.spacing = unit(1.5, "lines"),  # Increase spacing between facets for clarity
    panel.grid.major = element_blank(),  # Remove major gridlines for a cleaner look
    panel.grid.minor = element_blank(),  # Remove minor gridlines
    axis.line = element_line(color = "black"),  # Add axis lines for better separation
    legend.title = element_text(size = 12, face = "bold")  # Add bold legend title
  ) +
  coord_cartesian(ylim = c(0, NA))  # Let the y-axis automatically adjust per facet

# Display the improved plot
print(final_plot)

# Save the improved plot with increased resolution and adjusted dimensions for publication
ggsave("Lancet_Style_Observed_vs_Predicted_Probability_Figures_Inside_Bars.png", 
       plot = final_plot, dpi = 300, width = 16, height = 12)  # Adjusted width and height for publication

```

# Part 2

# MISSING DATA CHECK

## Remove Features with count less than 10

left with 43 predictors (excluding person_id, disease_status, time_to_event, and only counting 1 eGFR)

```{r}
# Load necessary libraries
library(dplyr)
library(tidyr)

# Load dataset
#filtered_person_with_summary <- read.csv("person_with_summary_inrange_indexdate_creatnine.csv")


# Step 1: Select and rename relevant columns
filtered_person_with_summary <- filtered_person_with_summary %>%
  select(-age_reported,
         -Glomerular.filtration.rate.1.73.sq.M.predicted..Volume.Rate.Area..in.Serum..Plasma.or.Blood.by.Creatinine.based.formula..MDRD.,
         -Tobacco.smoking.status,
         -height,
         -weight,
         -gender) %>%
  rename(smoking = highest_smoking_status_rank)

# Step 2: Filter out non-numeric columns and pivot longer
person_long <- filtered_person_with_summary %>%
  select(person_id, disease_status, where(is.numeric)) %>%
  pivot_longer(
    cols = -c(person_id, disease_status),
    names_to = "variable",
    values_to = "value"
  ) %>%
  filter(!is.na(value) & value != 0)  # Remove rows where the value is NA or zero

# Step 3: Count occurrences of disease_status for each variable
status_counts <- person_long %>%
  group_by(variable, disease_status) %>%
  summarise(count = n_distinct(person_id), .groups = 'drop')

# Step 4: Pivot the summary table to a wide format
summary_wide <- status_counts %>%
  pivot_wider(
    names_from = disease_status,
    values_from = count,
    names_prefix = "disease_status_",
    values_fill = list(count = 0)
  )

# Step 5: Filter out variables that have less than a count of 10 for either disease_status_0 or disease_status_1
filtered_summary <- summary_wide %>%
  filter(disease_status_0 >= 10 & disease_status_1 >= 10)

# Step 6: Extract the variable names that passed the filter
remaining_variables <- filtered_summary$variable

# Step 7: Retain only the person_id, disease_status, race, sex_at_birth, ethnicity, and the remaining variables
filtered_person_with_summary <- filtered_person_with_summary %>%
  select(person_id, disease_status, race, sex_at_birth, ethnicity, all_of(remaining_variables))

# Step 8: Identify the dropped variables
all_variables <- names(select(filtered_person_with_summary, where(is.numeric)))
dropped_variables <- setdiff(all_variables, remaining_variables)

# Print the dropped variables if needed
#print(dropped_variables)

# View the updated column names
colnames(filtered_person_with_summary)

```

# Remove Conditions with perfect separation

48 with 43 predictors

```{r}
# Check for perfect separation: columns with all 0s or all 1s
perfect_separation_cols <- filtered_person_with_summary %>%
  select(where(is.numeric)) %>%  # Select numeric columns (conditions are likely numeric binary)
  summarise_all(~ all(. == 0) | all(. == 1)) %>%  # Check for columns that are all 0s or all 1s
  pivot_longer(cols = everything(), names_to = "condition", values_to = "perfect_separation") %>%
  filter(perfect_separation) %>%  # Filter columns with perfect separation
  pull(condition)  # Get the names of these columns

# List the names of conditions with perfect separation
cat("Conditions removed due to perfect separation:\n")
print(perfect_separation_cols)

# Remove these columns from the dataset
filtered_person_with_summary <- filtered_person_with_summary %>%
  select(-all_of(perfect_separation_cols))

# View the updated dataset without conditions that have perfect separation
print(colnames(filtered_person_with_summary))

```

# Check for multicolinearity

```{r}
# Load necessary libraries
library(dplyr)
library(corrr)
library(knitr)
library(kableExtra)

# Assuming filtered_person_with_summary is the dataset after filtration

# Step 1: Ensure that only numeric columns with variance are checked for correlation
filtered_data_no_constant <- filtered_person_with_summary %>%
  select(-person_id, -disease_status) %>%
  select(where(is.numeric)) %>%  # Ensure we are only dealing with numeric columns
  select_if(~ sd(., na.rm = TRUE) > 0)  # Ensure non-zero standard deviation

# Step 2: Calculate the correlation matrix for the filtered dataset
correlation_matrix <- filtered_data_no_constant %>%
  correlate(method = "pearson", use = "pairwise.complete.obs")

# Step 3: Identify pairs of variables with high correlations (e.g., |correlation| > 0.9)
highly_correlated_pairs <- correlation_matrix %>%
  stretch() %>%  # Convert the correlation matrix into a long format
  filter(abs(r) > 0.9, r != 1) %>%  # Keep only pairs with |correlation| > 0.9 and not self-correlation
  mutate(pair = paste(pmin(x, y), pmax(x, y), sep = "_")) %>%  # Create a unique identifier for pairs
  distinct(pair, .keep_all = TRUE) %>%  # Remove duplicates (e.g., (x,y) and (y,x))
  select(-pair)  # Remove the auxiliary 'pair' column

# Step 4: Prepare the table for the number of observations for each correlated pair
if (nrow(highly_correlated_pairs) > 0) {
  result_table <- data.frame(
    `Variable 1` = character(),
    `Variable 2` = character(),
    `Correlation` = numeric(),
    `Cases (Var 1)` = integer(),
    `Controls (Var 1)` = integer(),
    `Cases (Var 2)` = integer(),
    `Controls (Var 2)` = integer(),
    stringsAsFactors = FALSE
  )
  
  for (i in 1:nrow(highly_correlated_pairs)) {
    var1 <- highly_correlated_pairs$x[i]
    var2 <- highly_correlated_pairs$y[i]
    correlation <- highly_correlated_pairs$r[i]
    
    # Number of non-zero cases (disease_status == 1) and non-zero controls (disease_status == 0) for var1
    cases_var1 <- sum(filtered_person_with_summary$disease_status == 1 & 
                      filtered_person_with_summary[[var1]] != 0 & !is.na(filtered_person_with_summary[[var1]]))
    controls_var1 <- sum(filtered_person_with_summary$disease_status == 0 & 
                         filtered_person_with_summary[[var1]] != 0 & !is.na(filtered_person_with_summary[[var1]]))
    
    # Number of non-zero cases (disease_status == 1) and non-zero controls (disease_status == 0) for var2
    cases_var2 <- sum(filtered_person_with_summary$disease_status == 1 & 
                      filtered_person_with_summary[[var2]] != 0 & !is.na(filtered_person_with_summary[[var2]]))
    controls_var2 <- sum(filtered_person_with_summary$disease_status == 0 & 
                         filtered_person_with_summary[[var2]] != 0 & !is.na(filtered_person_with_summary[[var2]]))
    
    # Add the results to the table
    result_table <- rbind(result_table, data.frame(
      `Variable 1` = var1,
      `Variable 2` = var2,
      `Correlation` = round(correlation, 2),  # Round correlation to 2 decimal places
      `Cases (Var 1)` = cases_var1,
      `Controls (Var 1)` = controls_var1,
      `Cases (Var 2)` = cases_var2,
      `Controls (Var 2)` = controls_var2
    ))
  }
  
  # Print the result table with formatting for publication
  print(result_table)
} else {
  cat("No highly correlated variable pairs found (|correlation| > 0.9).\n")
}

# Print the column names of the original dataset
print(colnames(filtered_person_with_summary))

```

### Remove correlated data

```{r}
filtered_person_with_summary <- filtered_person_with_summary %>%
  select(-Iron.binding.capacity..Mass.volume..in.Serum.or.Plasma, -eGFR_ckd_epi_2009)

```

Remove MDRD eGFR

```{r}
filtered_person_with_summary <- filtered_person_with_summary %>%
  select(-eGFR_MDRD)
```

### Recheck for multicolinearity

```{r}
# Load necessary libraries
library(dplyr)
library(corrr)
library(knitr)
library(kableExtra)

# Assuming filtered_person_with_summary is the dataset after filtration

# Step 1: Ensure that only numeric columns are checked for variance
filtered_data_no_constant <- filtered_person_with_summary %>%
  select(-person_id, -disease_status) %>%
  select(where(is.numeric)) %>%  # Ensure we are only dealing with numeric columns
  select_if(~ !all(is.na(.)) && sd(., na.rm = TRUE) > 0)  # Ensure non-zero standard deviation, no NA values, and proper data type

# Step 2: Calculate the correlation matrix for the filtered dataset
correlation_matrix <- filtered_data_no_constant %>%
  correlate(method = "pearson", use = "pairwise.complete.obs")

# Step 3: Identify pairs of variables with high correlations (e.g., |correlation| > 0.9)
highly_correlated_pairs <- correlation_matrix %>%
  stretch() %>%  # Convert the correlation matrix into a long format
  filter(abs(r) > 0.9, r != 1)  # Keep only pairs with |correlation| > 0.9 and not self-correlation

# Step 4: Prepare the table for the number of observations for each correlated pair
if (nrow(highly_correlated_pairs) > 0) {
  result_table <- data.frame(
    `Variable 1` = character(),
    `Variable 2` = character(),
    `Correlation` = numeric(),
    `Cases (Var 1)` = integer(),
    `Controls (Var 1)` = integer(),
    `Cases (Var 2)` = integer(),
    `Controls (Var 2)` = integer(),
    stringsAsFactors = FALSE
  )
  
  for (i in 1:nrow(highly_correlated_pairs)) {
    var1 <- highly_correlated_pairs$x[i]
    var2 <- highly_correlated_pairs$y[i]
    correlation <- highly_correlated_pairs$r[i]
    
    # Number of non-zero cases (disease_status == 1) and non-zero controls (disease_status == 0) for var1
    cases_var1 <- sum(filtered_person_with_summary$disease_status == 1 & 
                      filtered_person_with_summary[[var1]] != 0 & !is.na(filtered_person_with_summary[[var1]]))
    controls_var1 <- sum(filtered_person_with_summary$disease_status == 0 & 
                         filtered_person_with_summary[[var1]] != 0 & !is.na(filtered_person_with_summary[[var1]]))
    
    # Number of non-zero cases (disease_status == 1) and non-zero controls (disease_status == 0) for var2
    cases_var2 <- sum(filtered_person_with_summary$disease_status == 1 & 
                      filtered_person_with_summary[[var2]] != 0 & !is.na(filtered_person_with_summary[[var2]]))
    controls_var2 <- sum(filtered_person_with_summary$disease_status == 0 & 
                         filtered_person_with_summary[[var2]] != 0 & !is.na(filtered_person_with_summary[[var2]]))
    
    # Add the results to the table
    result_table <- rbind(result_table, data.frame(
      `Variable 1` = var1,
      `Variable 2` = var2,
      `Correlation` = round(correlation, 2),  # Round correlation to 2 decimal places
      `Cases (Var 1)` = cases_var1,
      `Controls (Var 1)` = controls_var1,
      `Cases (Var 2)` = cases_var2,
      `Controls (Var 2)` = controls_var2
    ))
  }
  
  # Print the result table with formatting for publication
  print(result_table)
} else {
  cat("No highly correlated variable pairs found (|correlation| > 0.9).\n")
}

# Print the column names of the original dataset
print(colnames(filtered_person_with_summary))

# Save the data frame to a CSV file
write.csv(filtered_person_with_summary, "filtered_person_with_summary_ml.csv", row.names = FALSE)


```

42 predictors

```{r}
# Load necessary libraries
library(dplyr)
colnames(filtered_person_with_summary)
# Assuming the race column is named 'race' in your dataset 'data_ml'
# Check the distribution of races in the 'race' column
race_distribution <- filtered_person_with_summary %>%
  group_by(race, disease_status) %>%
  summarise(count = n()) %>%
  arrange(desc(count))

# Print the race distribution
print(race_distribution)

# Save the filtered_person_with_summary dataframe as refiltered_person_with_summary.csv
write.csv(filtered_person_with_summary, file = "refiltered_person_with_summary.csv", row.names = FALSE)

```

##Testing Proportional hazards assumptions

Multivariate

```{r}
# Load necessary libraries
library(survival)
library(dplyr)

# Function to test proportional hazards assumption and return a table with results
test_proportional_hazards_multivariate <- function(cox_model, outcome_type) {
  # Perform the proportional hazards assumption test
  ph_test <- cox.zph(cox_model)
  
  # Extract the p-values from the test
  ph_results <- ph_test$table[, "p"]
  
  # Create a data frame to store the covariate names and their PH assumption results
  results_df <- data.frame(
    Covariate = rownames(ph_test$table),
    PH_Assumption = ifelse(ph_results >= 0.05, "Holds", "Violated")
  )
  
  # Print the result table
  cat("\nProportional Hazards Assumption Test Results for", outcome_type, "Outcome:\n")
  print(results_df)
  
  return(results_df)  # Return the table for further use if needed
}

# Fit a multivariate Cox proportional hazards model for 2-year outcome
cox_model_2yr <- coxph(
  Surv(time_to_event_2yr, event_2yr) ~ race + ethnicity + Acidosis + Acute.renal.failure.syndrome +
    Anemia + Anemia.in.chronic.kidney.disease + Atherosclerosis.of.coronary.artery.without.angina.pectoris +
    BMI + Chronic.kidney.disease.stage.3 + Chronic.kidney.disease.stage.4 + Congestive.heart.failure +
    Creatinine..Mass.volume..in.Body.fluid + Diabetes.mellitus + Disorder.of.kidney.and.or.ureter + Disorder.of.muscle +
    Erythrocyte.distribution.width..Ratio..by.Automated.count + Essential.hypertension + Gout + 
    Hemoglobin.A1c.Hemoglobin.total.in.Blood + Hepatitis.B.virus.surface.Ag..Presence..in.Serum.or.Plasma.by.Immunoassay +
    Hyperkalemia + Hypothyroidism + Iron..Mass.volume..in.Serum.or.Plasma + Iron.deficiency.anemia + 
    Iron.saturation..Mass.Fraction..in.Serum.or.Plasma + Parathyrin.intact..Mass.volume..in.Serum.or.Plasma +
    Peripheral.vascular.disease + Polyneuropathy.due.to.diabetes.mellitus + Protein..Mass.volume..in.Urine + Proteinuria +
    Renal.disorder.due.to.type.2.diabetes.mellitus + Systemic.lupus.erythematosus + Systolic.blood.pressure + 
    Transplanted.kidney.present + Triglyceride..Mass.volume..in.Serum.or.Plasma + Type.2.diabetes.mellitus +
    age_precise + NeverSmoker + FormerSmoker + CurrentSmoker + UnknownSmoker  +
    SexMale + SexFemale,
  data = filtered_person_with_summary
)

# Test proportional hazards assumption for the 2-year model
ph_violation_table_2yr <- test_proportional_hazards_multivariate(cox_model_2yr, "2-Year")


# Fit a multivariate Cox proportional hazards model for 5-year outcome
cox_model_5yr <- coxph(
  Surv(time_to_event_5yr, event_5yr) ~ race + ethnicity + Acidosis + Acute.renal.failure.syndrome +
    Anemia + Anemia.in.chronic.kidney.disease + Atherosclerosis.of.coronary.artery.without.angina.pectoris +
    BMI + Chronic.kidney.disease.stage.3 + Chronic.kidney.disease.stage.4 + Congestive.heart.failure +
    Creatinine..Mass.volume..in.Body.fluid + Diabetes.mellitus + Disorder.of.kidney.and.or.ureter + Disorder.of.muscle +
    Erythrocyte.distribution.width..Ratio..by.Automated.count + Essential.hypertension + Gout + 
    Hemoglobin.A1c.Hemoglobin.total.in.Blood + Hepatitis.B.virus.surface.Ag..Presence..in.Serum.or.Plasma.by.Immunoassay +
    Hyperkalemia + Hypothyroidism + Iron..Mass.volume..in.Serum.or.Plasma + Iron.deficiency.anemia + 
    Iron.saturation..Mass.Fraction..in.Serum.or.Plasma + Parathyrin.intact..Mass.volume..in.Serum.or.Plasma +
    Peripheral.vascular.disease + Polyneuropathy.due.to.diabetes.mellitus + Protein..Mass.volume..in.Urine + Proteinuria +
    Renal.disorder.due.to.type.2.diabetes.mellitus + Systemic.lupus.erythematosus + Systolic.blood.pressure + 
    Transplanted.kidney.present + Triglyceride..Mass.volume..in.Serum.or.Plasma + Type.2.diabetes.mellitus +
    age_precise + NeverSmoker + FormerSmoker + CurrentSmoker + UnknownSmoker +
    SexMale + SexFemale,
  data = filtered_person_with_summary
)

# Test proportional hazards assumption for the 5-year model
ph_violation_table_5yr <- test_proportional_hazards_multivariate(cox_model_5yr, "5-Year")

```

### Data cleaning

```{r}
filtered_person_with_summary <- read.csv("refiltered_person_with_summary.csv")

# # Create binary columns for each smoking category
# filtered_person_with_summary$NeverSmoker <- ifelse(filtered_person_with_summary$smoking %in% c(0, 1), 1, 0)
# filtered_person_with_summary$FormerSmoker <- ifelse(filtered_person_with_summary$smoking %in% c(3, 5), 1, 0)
# filtered_person_with_summary$CurrentSmoker <- ifelse(filtered_person_with_summary$smoking %in% c(4, 6, 7, 8, 9, 10, 11), 1, 0)
# filtered_person_with_summary$UnknownSmoker <- ifelse(filtered_person_with_summary$smoking == 2, 1, 0)

# Remove the original 'smoking' column
filtered_person_with_summary <- filtered_person_with_summary[, !colnames(filtered_person_with_summary) %in% "smoking"]


# # Create binary columns for creatinine strata based on sex_at_birth and serum_creatinine
# filtered_person_with_summary$Low_Creatinine <- ifelse(
#   (filtered_person_with_summary$sex_at_birth == "Male" & filtered_person_with_summary$serum_creatinine < 0.7) |
#   (filtered_person_with_summary$sex_at_birth == "Female" & filtered_person_with_summary$serum_creatinine < 0.6), 1, 0)
# 
# filtered_person_with_summary$Normal_Creatinine <- ifelse(
#   (filtered_person_with_summary$sex_at_birth == "Male" & filtered_person_with_summary$serum_creatinine >= 0.7 & filtered_person_with_summary$serum_creatinine <= 1.3) |
#   (filtered_person_with_summary$sex_at_birth == "Female" & filtered_person_with_summary$serum_creatinine >= 0.6 & filtered_person_with_summary$serum_creatinine <= 1.1), 1, 0)
# 
# filtered_person_with_summary$High_Creatinine <- ifelse(
#   (filtered_person_with_summary$sex_at_birth == "Male" & filtered_person_with_summary$serum_creatinine > 1.3) |
#   (filtered_person_with_summary$sex_at_birth == "Female" & filtered_person_with_summary$serum_creatinine > 1.1), 1, 0)
# 
# # Remove the original serum_creatinine column
# filtered_person_with_summary <- filtered_person_with_summary[, !colnames(filtered_person_with_summary) %in% "serum_creatinine"]
# 
# 
# # Create binary variables for CKD stages based on eGFR values
# filtered_person_with_summary$eGFR_Normal_or_Stage1 <- ifelse(filtered_person_with_summary$eGFR_ckd_epi_2021 >= 90, 1, 0)
# filtered_person_with_summary$eGFR_Stage2 <- ifelse(filtered_person_with_summary$eGFR_ckd_epi_2021 >= 60 & filtered_person_with_summary$eGFR_ckd_epi_2021 < 90, 1, 0)
# filtered_person_with_summary$eGFR_Stage3a <- ifelse(filtered_person_with_summary$eGFR_ckd_epi_2021 >= 45 & filtered_person_with_summary$eGFR_ckd_epi_2021 < 60, 1, 0)
# filtered_person_with_summary$eGFR_Stage3b <- ifelse(filtered_person_with_summary$eGFR_ckd_epi_2021 >= 30 & filtered_person_with_summary$eGFR_ckd_epi_2021 < 45, 1, 0)
# filtered_person_with_summary$eGFR_Stage4 <- ifelse(filtered_person_with_summary$eGFR_ckd_epi_2021 >= 15 & filtered_person_with_summary$eGFR_ckd_epi_2021 < 30, 1, 0)
# filtered_person_with_summary$eGFR_Stage5 <- ifelse(filtered_person_with_summary$eGFR_ckd_epi_2021 < 15, 1, 0)
# 
# # Remove the original eGFR_ckd_epi_2021 column
# filtered_person_with_summary <- filtered_person_with_summary[, !colnames(filtered_person_with_summary) %in% "eGFR_ckd_epi_2021"]
# 
# 
# Create 'SexMale' and 'SexFemale' columns based on 'sex_at_birth'
filtered_person_with_summary$SexMale <- ifelse(filtered_person_with_summary$sex_at_birth == "Male", 1, 0)
filtered_person_with_summary$SexFemale <- ifelse(filtered_person_with_summary$sex_at_birth == "Female", 1, 0)

# Remove the original 'sex_at_birth' column
filtered_person_with_summary <- filtered_person_with_summary[, !colnames(filtered_person_with_summary) %in% "sex_at_birth"]

# Remove the predictor Ferritin..Mass.volume..in.Serum.or.Plasma from the dataset
filtered_person_with_summary <- filtered_person_with_summary[, !(colnames(filtered_person_with_summary) == "Ferritin..Mass.volume..in.Serum.or.Plasma")]



# Save the filtered_person_with_summary dataframe as refiltered_person_with_summary.csv
write.csv(filtered_person_with_summary, file = "rerefiltered_person_with_summary.csv", row.names = FALSE)

```

# XG Boost Model

## Data preparation

```{r}
filtered_person_with_summary <- read.csv("rerefiltered_person_with_summary.csv")

# Load necessary libraries
library(dplyr)
library(xgboost)
library(survival)
# Step 1: Replace all NA values in the dataset with 0
filtered_person_with_summary[is.na(filtered_person_with_summary)] <- 0

# Step 2: Ensure there are no more NA values
if (sum(is.na(filtered_person_with_summary)) != 0) {
  stop("There are still NA values in the data.")
}

# Step 3: Define the 5-year and 2-year time limits in days
five_years_in_days <- 5 * 365.25  # 1826.25 days
two_years_in_days <- 2 * 365.25   # 730.5 days

# Step 4: Censor or filter data for 5-year and 2-year events based on `time_to_event`
if (!"time_to_event" %in% colnames(filtered_person_with_summary)) {
  stop("Error: time_to_event column not found in the dataset.")
}

# Create event_5yr and time_to_event_5yr
filtered_person_with_summary$event_5yr <- ifelse(filtered_person_with_summary$time_to_event <= five_years_in_days & 
                                                 filtered_person_with_summary$disease_status == 1, 1, 0)
filtered_person_with_summary$time_to_event_5yr <- pmin(filtered_person_with_summary$time_to_event, five_years_in_days)

# Create event_2yr and time_to_event_2yr
filtered_person_with_summary$event_2yr <- ifelse(filtered_person_with_summary$time_to_event <= two_years_in_days & 
                                                 filtered_person_with_summary$disease_status == 1, 1, 0)
filtered_person_with_summary$time_to_event_2yr <- pmin(filtered_person_with_summary$time_to_event, two_years_in_days)

# Step 5: Remove the `time_to_event` column as it's no longer needed
filtered_person_with_summary <- subset(filtered_person_with_summary, select = -c(time_to_event))

# Step 6: Normalize numeric variables using log(variable + 1)
predictor_cols <- colnames(filtered_person_with_summary)[5:39]  # Adjust index for numeric columns
filtered_person_with_summary[predictor_cols] <- log(filtered_person_with_summary[predictor_cols] + 1)

# Step 7: Create binary race variables based on the race column
filtered_person_with_summary$raceBlack <- ifelse(filtered_person_with_summary$race == "Black or African American", 1, 0)
filtered_person_with_summary$raceWhite <- ifelse(filtered_person_with_summary$race == "White", 1, 0)
filtered_person_with_summary$raceAsian <- ifelse(filtered_person_with_summary$race == "Asian", 0, 0)  # Put 0 for train, Asians will only be in test set

# Step 8: Split Black and White data into 70% training and 30% test
set.seed(123)  # For reproducibility
# Splitting Black data
sample_size_black <- floor(0.7 * nrow(subset(filtered_person_with_summary, raceBlack == 1)))
train_indices_black <- sample(seq_len(nrow(subset(filtered_person_with_summary, raceBlack == 1))), size = sample_size_black)
train_data_black <- subset(filtered_person_with_summary, raceBlack == 1)[train_indices_black, ]
test_data_black <- subset(filtered_person_with_summary, raceBlack == 1)[-train_indices_black, ]

# Splitting White data
sample_size_white <- floor(0.7 * nrow(subset(filtered_person_with_summary, raceWhite == 1)))
train_indices_white <- sample(seq_len(nrow(subset(filtered_person_with_summary, raceWhite == 1))), size = sample_size_white)
train_data_white <- subset(filtered_person_with_summary, raceWhite == 1)[train_indices_white, ]
test_data_white <- subset(filtered_person_with_summary, raceWhite == 1)[-train_indices_white, ]

# Step 9: For Asians, 100% of data goes to the test set
test_data_asian <- subset(filtered_person_with_summary, race == "Asian")
test_data_asian$raceAsian <- 1  # Mark Asians for test

# Step 10: Combine the training data (only Black and White individuals)
train_data <- rbind(train_data_black, train_data_white)

# Step 11: Combine the test data (30% Black, 30% White, and 100% Asian)
test_data <- rbind(test_data_black, test_data_white, test_data_asian)

# Step 12: Filter out non-positive event times from training and test data
# For 2-year training data
valid_indices_train_2yr <- train_data$time_to_event_2yr > 0  # Keep only rows with positive time_to_event
train_data_2yr <- train_data[valid_indices_train_2yr, ]

# For 2-year test data
valid_indices_test_2yr <- test_data$time_to_event_2yr > 0
test_data_2yr <- test_data[valid_indices_test_2yr, ]

# For 5-year training data
valid_indices_train_5yr <- train_data$time_to_event_5yr > 0
train_data_5yr <- train_data[valid_indices_train_5yr, ]

# For 5-year test data
valid_indices_test_5yr <- test_data$time_to_event_5yr > 0
test_data_5yr <- test_data[valid_indices_test_5yr, ]

# Step 13: Create model matrices for 2-year and 5-year predictions

# Exclude the original "race" column to avoid duplicates
non_predictor_cols <- c("person_id", "disease_status", "event_2yr", "event_5yr", "time_to_event_2yr", "time_to_event_5yr", "race")

# 2-year training data (without duplicate race columns)
X_train_2yr <- model.matrix(~ raceBlack + raceWhite + raceAsian + ethnicity + Acidosis + Acute.renal.failure.syndrome + Anemia + BMI + . - 1, 
                            data = train_data_2yr[, !(colnames(train_data_2yr) %in% non_predictor_cols)])
y_train_2yr <- train_data_2yr$event_2yr
time_train_2yr <- train_data_2yr$time_to_event_2yr

# 2-year test data (without duplicate race columns)
X_test_2yr <- model.matrix(~ raceBlack + raceWhite + raceAsian + ethnicity + Acidosis + Acute.renal.failure.syndrome + Anemia + BMI + . - 1, 
                           data = test_data_2yr[, !(colnames(test_data_2yr) %in% non_predictor_cols)])
y_test_2yr <- test_data_2yr$event_2yr
time_test_2yr <- test_data_2yr$time_to_event_2yr

# 5-year training data (without duplicate race columns)
X_train_5yr <- model.matrix(~ raceBlack + raceWhite + raceAsian + ethnicity + Acidosis + Acute.renal.failure.syndrome + Anemia + BMI + . - 1, 
                            data = train_data_5yr[, !(colnames(train_data_5yr) %in% non_predictor_cols)])
y_train_5yr <- train_data_5yr$event_5yr
time_train_5yr <- train_data_5yr$time_to_event_5yr

# 5-year test data (without duplicate race columns)
X_test_5yr <- model.matrix(~ raceBlack + raceWhite + raceAsian + ethnicity + Acidosis + Acute.renal.failure.syndrome + Anemia + BMI + . - 1, 
                           data = test_data_5yr[, !(colnames(test_data_5yr) %in% non_predictor_cols)])
y_test_5yr <- test_data_5yr$event_5yr
time_test_5yr <- test_data_5yr$time_to_event_5yr

# Step 14: Print cases and controls with non-positive time to event for train and test data
# For 2-year training data
non_positive_train_2yr <- train_data[train_data$time_to_event_2yr <= 0, ]
non_positive_cases_train_2yr <- sum(non_positive_train_2yr$event_2yr == 1)
non_positive_controls_train_2yr <- sum(non_positive_train_2yr$event_2yr == 0)

# For 2-year test data
non_positive_test_2yr <- test_data[test_data$time_to_event_2yr <= 0, ]
non_positive_cases_test_2yr <- sum(non_positive_test_2yr$event_2yr == 1)
non_positive_controls_test_2yr <- sum(non_positive_test_2yr$event_2yr == 0)

# For 5-year training data
non_positive_train_5yr <- train_data[train_data$time_to_event_5yr <= 0, ]
non_positive_cases_train_5yr <- sum(non_positive_train_5yr$event_5yr == 1)
non_positive_controls_train_5yr <- sum(non_positive_train_5yr$event_5yr == 0)

# For 5-year test data
non_positive_test_5yr <- test_data[test_data$time_to_event_5yr <= 0, ]
non_positive_cases_test_5yr <- sum(non_positive_test_5yr$event_5yr == 1)
non_positive_controls_test_5yr <- sum(non_positive_test_5yr$event_5yr == 0)

# Step 15: Print the results
cat("2-Year Training Data: \n")
cat("Non-positive cases:", non_positive_cases_train_2yr, "\n")
cat("Non-positive controls:", non_positive_controls_train_2yr, "\n")
cat("Unique non-positive time to events:", unique(non_positive_train_2yr$time_to_event_2yr), "\n\n")

cat("2-Year Test Data: \n")
cat("Non-positive cases:", non_positive_cases_test_2yr, "\n")
cat("Non-positive controls:", non_positive_controls_test_2yr, "\n")
cat("Unique non-positive time to events:", unique(non_positive_test_2yr$time_to_event_2yr), "\n\n")

cat("5-Year Training Data: \n")
cat("Non-positive cases:", non_positive_cases_train_5yr, "\n")
cat("Non-positive controls:", non_positive_controls_train_5yr, "\n")
cat("Unique non-positive time to events:", unique(non_positive_train_5yr$time_to_event_5yr), "\n\n")

cat("5-Year Test Data: \n")
cat("Non-positive cases:", non_positive_cases_test_5yr, "\n")
cat("Non-positive controls:", non_positive_controls_test_5yr, "\n")
cat("Unique non-positive time to events:", unique(non_positive_test_5yr$time_to_event_5yr), "\n\n")

```

#### Counts before cleaning

```{r}
# Load necessary library for better data manipulation
library(dplyr)

# Define the race groups we are interested in
race_groups <- c("White", "Black or African American", "Asian")

# Function to calculate the number of cases and controls for both 2-year and 5-year events
count_cases_controls <- function(data, race_variable, event_2yr_variable, event_5yr_variable) {
  
  # Initialize an empty result data frame to store results
  result <- data.frame(Race = race_groups, 
                       Cases_2yr = integer(length(race_groups)), Controls_2yr = integer(length(race_groups)),
                       Cases_5yr = integer(length(race_groups)), Controls_5yr = integer(length(race_groups)))

  # Loop through each race group
  for (race in race_groups) {
    
    # Filter the data for the current race
    race_data <- data[data[[race_variable]] == race, ]
    
    # Calculate the number of 2-year cases and controls
    num_cases_2yr <- sum(race_data[[event_2yr_variable]] == 1, na.rm = TRUE)
    num_controls_2yr <- sum(race_data[[event_2yr_variable]] == 0, na.rm = TRUE)
    
    # Calculate the number of 5-year cases and controls
    num_cases_5yr <- sum(race_data[[event_5yr_variable]] == 1, na.rm = TRUE)
    num_controls_5yr <- sum(race_data[[event_5yr_variable]] == 0, na.rm = TRUE)
    
    # Store the results in the result data frame for each race group
    result[result$Race == race, "Cases_2yr"] <- num_cases_2yr
    result[result$Race == race, "Controls_2yr"] <- num_controls_2yr
    result[result$Race == race, "Cases_5yr"] <- num_cases_5yr
    result[result$Race == race, "Controls_5yr"] <- num_controls_5yr
  }
  
  # Add a row for the total (across all races)
  total_cases_2yr <- sum(data[[event_2yr_variable]] == 1, na.rm = TRUE)
  total_controls_2yr <- sum(data[[event_2yr_variable]] == 0, na.rm = TRUE)
  total_cases_5yr <- sum(data[[event_5yr_variable]] == 1, na.rm = TRUE)
  total_controls_5yr <- sum(data[[event_5yr_variable]] == 0, na.rm = TRUE)
  
  result <- rbind(result, 
                  data.frame(Race = "Total", 
                             Cases_2yr = total_cases_2yr, Controls_2yr = total_controls_2yr,
                             Cases_5yr = total_cases_5yr, Controls_5yr = total_controls_5yr))
  
  return(result)
}

# Calculate cases and controls for the training data for both 2-year and 5-year events
train_summary <- count_cases_controls(train_data, "race", "event_2yr", "event_5yr")

# Calculate cases and controls for the test data for both 2-year and 5-year events
test_summary <- count_cases_controls(test_data, "race", "event_2yr", "event_5yr")

# Combine the results for both training and test data into one dataset
combined_summary <- list(Train = train_summary, Test = test_summary)

# Print the results
print("Training Data Summary (2-year and 5-year):")
print(combined_summary$Train)

print("Test Data Summary (2-year and 5-year):")
print(combined_summary$Test)

```

#### Counts after cleaning

```{r}
# Load necessary library for better data manipulation
library(dplyr)

# Define the race groups we are interested in
race_groups <- c("White", "Black or African American", "Asian")

# Function to calculate the number of cases and controls for both 2-year and 5-year events, with positive time-to-event
count_cases_controls_positive_time <- function(data, race_variable, event_2yr_variable, event_5yr_variable, time_2yr_variable, time_5yr_variable) {
  
  # Initialize an empty result data frame to store results
  result <- data.frame(Race = race_groups, 
                       Cases_2yr_Positive = integer(length(race_groups)), Controls_2yr_Positive = integer(length(race_groups)),
                       Cases_5yr_Positive = integer(length(race_groups)), Controls_5yr_Positive = integer(length(race_groups)))
  
  # Loop through each race group
  for (race in race_groups) {
    
    # Filter the data for the current race and keep only rows with positive time-to-event
    race_data_2yr <- data[data[[race_variable]] == race & data[[time_2yr_variable]] > 0, ]
    race_data_5yr <- data[data[[race_variable]] == race & data[[time_5yr_variable]] > 0, ]
    
    # Calculate the number of 2-year cases and controls with positive time-to-event
    num_cases_2yr <- sum(race_data_2yr[[event_2yr_variable]] == 1, na.rm = TRUE)
    num_controls_2yr <- sum(race_data_2yr[[event_2yr_variable]] == 0, na.rm = TRUE)
    
    # Calculate the number of 5-year cases and controls with positive time-to-event
    num_cases_5yr <- sum(race_data_5yr[[event_5yr_variable]] == 1, na.rm = TRUE)
    num_controls_5yr <- sum(race_data_5yr[[event_5yr_variable]] == 0, na.rm = TRUE)
    
    # Store the results in the result data frame for each race group
    result[result$Race == race, "Cases_2yr_Positive"] <- num_cases_2yr
    result[result$Race == race, "Controls_2yr_Positive"] <- num_controls_2yr
    result[result$Race == race, "Cases_5yr_Positive"] <- num_cases_5yr
    result[result$Race == race, "Controls_5yr_Positive"] <- num_controls_5yr
  }
  
  # Add a row for the total (across all races) with positive time-to-event
  total_cases_2yr <- sum(data[[event_2yr_variable]] == 1 & data[[time_2yr_variable]] > 0, na.rm = TRUE)
  total_controls_2yr <- sum(data[[event_2yr_variable]] == 0 & data[[time_2yr_variable]] > 0, na.rm = TRUE)
  total_cases_5yr <- sum(data[[event_5yr_variable]] == 1 & data[[time_5yr_variable]] > 0, na.rm = TRUE)
  total_controls_5yr <- sum(data[[event_5yr_variable]] == 0 & data[[time_5yr_variable]] > 0, na.rm = TRUE)
  
  result <- rbind(result, 
                  data.frame(Race = "Total", 
                             Cases_2yr_Positive = total_cases_2yr, Controls_2yr_Positive = total_controls_2yr,
                             Cases_5yr_Positive = total_cases_5yr, Controls_5yr_Positive = total_controls_5yr))
  
  return(result)
}

# Calculate cases and controls with positive time-to-event for the training data
train_summary <- count_cases_controls_positive_time(train_data, "race", "event_2yr", "event_5yr", "time_to_event_2yr", "time_to_event_5yr")

# Calculate cases and controls with positive time-to-event for the test data
test_summary <- count_cases_controls_positive_time(test_data, "race", "event_2yr", "event_5yr", "time_to_event_2yr", "time_to_event_5yr")

# Combine the results for both training and test data into one dataset
combined_summary <- list(Train = train_summary, Test = test_summary)

# Print the results
cat("Training Data Summary (2-year and 5-year with positive time-to-event):\n")
print(combined_summary$Train)

cat("\nTest Data Summary (2-year and 5-year with positive time-to-event):\n")
print(combined_summary$Test)

```

## Race specific model

```{r}
# Load necessary libraries
library(xgboost)
library(pROC)
library(Hmisc)  # For Harrell's C-statistic
library(survival)
library(doParallel)
library(ggplot2)

# Set up parallel backend for faster execution
cores <- detectCores() - 1
registerDoParallel(cores)

# Set random seed for reproducibility
set.seed(123)

# Function to normalize data using log(x + 1)
normalize_data <- function(df) {
  return(log(df + 1))
}

# Optimal Parameters for XGBoost Cox model
optimal_params <- list(
  max_depth = 6,
  min_child_weight = 2,
  gamma = 0.5,
  eta = 0.1,                    # Learning rate
  subsample = 1,              # Full sampling 
  colsample_bytree = 1,       # Full column sampling 
  objective = "survival:cox",   # Cox proportional hazards objective
  eval_metric = "cox-nloglik",  # Evaluation metric for Cox models
  nthread = cores               # Number of threads to use
)

# Function to fit XGBoost model with early stopping and return variable importance
fit_xgboost_and_display <- function(X_data, time_to_event, event_status, title_prefix, params, nrounds, early_stopping_rounds) {
  
  # Convert data to DMatrix for XGBoost
  dtrain <- xgb.DMatrix(data = as.matrix(X_data), label = time_to_event)
  
  # Set base margin for event status (1 = event, 0 = censored)
  setinfo(dtrain, "base_margin", event_status)
  
  # Train the XGBoost model with early stopping
  xgboost_model <- xgboost(
    params = params,
    data = dtrain,
    nrounds = nrounds,
    early_stopping_rounds = early_stopping_rounds,  # Early stopping to prevent overfitting
    verbose = 0  # Suppress output for cleaner logging
  )
  
  # Get variable importance
  importance_matrix <- xgb.importance(model = xgboost_model)
  
  # Plot variable importance
  importance_plot <- ggplot(importance_matrix, aes(x = reorder(Feature, Gain), y = Gain)) +
    geom_bar(stat = "identity") +
    coord_flip() +
    labs(title = paste(title_prefix, "Variable Importance"), x = "Variables", y = "Importance (Gain)") +
    theme_minimal()
  
  print(importance_plot)  # Display the plot
  
  return(list(model = xgboost_model, importance = importance_matrix))
}

# Function to prepare and normalize the data, ensuring clean input
prepare_data <- function(data, time_col, event_col, time_threshold) {
  valid_indices <- data[[time_col]] > 0  # Filter rows with positive time_to_event
  
  # Censor observations beyond the threshold and normalize
  filtered_data <- data[valid_indices, ]
  filtered_data[[event_col]] <- ifelse(filtered_data[[time_col]] > time_threshold, 0, filtered_data[[event_col]])
  
  # Normalize numeric predictor variables
  normalized_data <- filtered_data %>%
    mutate(across(where(is.numeric), ~ normalize_data(.)))
  
  return(normalized_data)
}

# Function to evaluate the XGBoost model with metrics
evaluate_xgboost <- function(xgboost_model, X_test_data, y_test_data, time_test_data, train_column_names) {
  
  # Ensure the test data has the same columns as the training data
  X_test_data <- X_test_data[, train_column_names, drop = FALSE]
  
  # Create a DMatrix for testing
  dtest <- xgb.DMatrix(data = as.matrix(X_test_data))
  
  # Predict risk scores
  risk_scores <- predict(xgboost_model, newdata = dtest)
  
  # Calculate Harrell's C-statistic (concordance index)
  survival_object <- Surv(time_test_data, y_test_data)
  c_stat <- rcorr.cens(risk_scores, survival_object)["C Index"]
  
  # Calculate AUC for the risk scores
  roc_curve <- roc(y_test_data, risk_scores)
  auc_value <- auc(roc_curve)
  
  return(list(c_stat = c_stat, auc = auc_value))
}

# Step 1: Data preparation for 2-year and 5-year predictions
two_years_in_days <- 2 * 365.25
five_years_in_days <- 5 * 365.25

# Exclude unwanted columns like person_id and disease_status for 2-year model matrix
non_predictor_cols <- c("person_id", "disease_status", "event_2yr", "event_5yr", "time_to_event_2yr", "time_to_event_5yr", "race")

# Prepare 2-year training data
train_data_2yr <- prepare_data(train_data, "time_to_event_2yr", "event_2yr", two_years_in_days)
X_train_2yr <- model.matrix(~ . - event_2yr - event_5yr - time_to_event_2yr - time_to_event_5yr - disease_status - person_id - race - 1, data = train_data_2yr)
time_train_2yr <- train_data_2yr$time_to_event_2yr
event_status_2yr <- train_data_2yr$event_2yr

# Prepare 5-year training data
train_data_5yr <- prepare_data(train_data, "time_to_event_5yr", "event_5yr", five_years_in_days)
X_train_5yr <- model.matrix(~ . - event_2yr - event_5yr - time_to_event_2yr - time_to_event_5yr - disease_status - person_id - race - 1, data = train_data_5yr)
time_train_5yr <- train_data_5yr$time_to_event_5yr
event_status_5yr <- train_data_5yr$event_5yr

# Train XGBoost models with early stopping for both 2-year and 5-year models

# Step 2: Train 2-year XGBoost model
result_2yr <- fit_xgboost_and_display(X_train_2yr, time_train_2yr, event_status_2yr, "2-Year", optimal_params, nrounds = 300, early_stopping_rounds = 10)
xgboost_model_2yr <- result_2yr$model

# Step 3: Train 5-year XGBoost model
result_5yr <- fit_xgboost_and_display(X_train_5yr, time_train_5yr, event_status_5yr, "5-Year", optimal_params, nrounds = 300, early_stopping_rounds = 10)
xgboost_model_5yr <- result_5yr$model

# Evaluate models on test data (whole population and subgroups)

# Define a function to evaluate for a specific race group
evaluate_by_race <- function(race_column, X_test, y_test, time_test, xgboost_model, train_column_names) {
  race_indices <- X_test[, race_column] == 1
  X_test_race <- X_test[race_indices, ]
  y_test_race <- y_test[race_indices]
  time_test_race <- time_test[race_indices]
  evaluate_xgboost(xgboost_model, X_test_race, y_test_race, time_test_race, train_column_names)
}

# Full population evaluation
evaluation_2yr_all <- evaluate_xgboost(xgboost_model_2yr, X_test_2yr, y_test_2yr, time_test_2yr, colnames(X_train_2yr))
evaluation_5yr_all <- evaluate_xgboost(xgboost_model_5yr, X_test_5yr, y_test_5yr, time_test_5yr, colnames(X_train_5yr))

# Group analysis by race for 2-year and 5-year predictions
subgroups <- c("raceBlack", "raceWhite", "raceAsian")
results_2yr <- list()
results_5yr <- list()

for (group in subgroups) {
  results_2yr[[group]] <- evaluate_by_race(group, X_test_2yr, y_test_2yr, time_test_2yr, xgboost_model_2yr, colnames(X_train_2yr))
  results_5yr[[group]] <- evaluate_by_race(group, X_test_5yr, y_test_5yr, time_test_5yr, xgboost_model_5yr, colnames(X_train_5yr))
}

# Stop parallel processing
stopImplicitCluster()

# Combine results into a data frame for publication-ready table
results_table <- data.frame(
  Model = c("2-Year All", "2-Year Black", "2-Year White", "2-Year Asian", "5-Year All", "5-Year Black", "5-Year White", "5-Year Asian"),
  AUC = c(evaluation_2yr_all$auc, results_2yr$raceBlack$auc, results_2yr$raceWhite$auc, results_2yr$raceAsian$auc,
          evaluation_5yr_all$auc, results_5yr$raceBlack$auc, results_5yr$raceWhite$auc, results_5yr$raceAsian$auc),
  Harrell_C = c(evaluation_2yr_all$c_stat, results_2yr$raceBlack$c_stat, results_2yr$raceWhite$c_stat, results_2yr$raceAsian$c_stat,
                evaluation_5yr_all$c_stat, results_5yr$raceBlack$c_stat, results_5yr$raceWhite$c_stat, results_5yr$raceAsian$c_stat)
)

# Print results in a table format
print(results_table)

```

## Race_free model:

```{r}
# Load necessary libraries
library(xgboost)
library(pROC)
library(Hmisc)  # For Harrell's C-statistic
library(survival)
library(doParallel)
library(ggplot2)

# Set up parallel backend for faster execution
cores <- detectCores() - 1
registerDoParallel(cores)

# Set random seed for reproducibility
set.seed(123)

# Function to normalize data using log(x + 1)
normalize_data <- function(df) {
  return(log(df + 1))
}

# Optimal Parameters for XGBoost Cox model
optimal_params <- list(
  max_depth = 6,
  min_child_weight = 2,
  gamma = 0.5,
  eta = 0.1,                    # Learning rate
  subsample = 1,                # Full sampling 
  colsample_bytree = 1,         # Full column sampling 
  objective = "survival:cox",   # Cox proportional hazards objective
  eval_metric = "cox-nloglik",  # Evaluation metric for Cox models
  nthread = cores               # Number of threads to use
)

# Function to fit XGBoost model with early stopping and return variable importance
fit_xgboost_and_display <- function(X_data, time_to_event, event_status, title_prefix, params, nrounds, early_stopping_rounds) {
  
  # Convert data to DMatrix for XGBoost
  dtrain <- xgb.DMatrix(data = as.matrix(X_data), label = time_to_event)
  
  # Set base margin for event status (1 = event, 0 = censored)
  setinfo(dtrain, "base_margin", event_status)
  
  # Train the XGBoost model with early stopping
  xgboost_model <- xgboost(
    params = params,
    data = dtrain,
    nrounds = nrounds,
    early_stopping_rounds = early_stopping_rounds,  # Early stopping to prevent overfitting
    verbose = 0  # Suppress output for cleaner logging
  )
  
  # Get variable importance
  importance_matrix <- xgb.importance(model = xgboost_model)
  
  # Plot variable importance
  importance_plot <- ggplot(importance_matrix, aes(x = reorder(Feature, Gain), y = Gain)) +
    geom_bar(stat = "identity") +
    coord_flip() +
    labs(title = paste(title_prefix, "Variable Importance"), x = "Variables", y = "Importance (Gain)") +
    theme_minimal()
  
  print(importance_plot)  # Display the plot
  
  return(list(model = xgboost_model, importance = importance_matrix))
}

# Function to prepare and normalize the data, ensuring clean input
prepare_data <- function(data, time_col, event_col, time_threshold) {
  valid_indices <- data[[time_col]] > 0  # Filter rows with positive time_to_event
  
  # Censor observations beyond the threshold and normalize
  filtered_data <- data[valid_indices, ]
  filtered_data[[event_col]] <- ifelse(filtered_data[[time_col]] > time_threshold, 0, filtered_data[[event_col]])
  
  # Normalize numeric predictor variables
  normalized_data <- filtered_data %>%
    mutate(across(where(is.numeric), ~ normalize_data(.)))
  
  return(normalized_data)
}

# Function to evaluate the XGBoost model with metrics
evaluate_xgboost <- function(xgboost_model, X_test_data, y_test_data, time_test_data, train_column_names) {
  
  # Ensure the test data has the same columns as the training data
  X_test_data <- X_test_data[, train_column_names, drop = FALSE]
  
  # Create a DMatrix for testing
  dtest <- xgb.DMatrix(data = as.matrix(X_test_data))
  
  # Predict risk scores
  risk_scores <- predict(xgboost_model, newdata = dtest)
  
  # Calculate Harrell's C-statistic (concordance index)
  survival_object <- Surv(time_test_data, y_test_data)
  c_stat <- rcorr.cens(risk_scores, survival_object)["C Index"]
  
  # Calculate AUC for the risk scores
  roc_curve <- roc(y_test_data, risk_scores)
  auc_value <- auc(roc_curve)
  
  return(list(c_stat = c_stat, auc = auc_value))
}

# Step 1: Data preparation for 2-year and 5-year predictions
two_years_in_days <- 2 * 365.25
five_years_in_days <- 5 * 365.25

# Prepare 2-year training data (race variables excluded in the model)
train_data_2yr <- prepare_data(train_data, "time_to_event_2yr", "event_2yr", two_years_in_days)
X_train_2yr <- model.matrix(~ . - event_2yr - event_5yr - time_to_event_2yr - time_to_event_5yr - disease_status - person_id - raceBlack - raceWhite - raceAsian - race - 1, 
                            data = train_data_2yr)
time_train_2yr <- train_data_2yr$time_to_event_2yr
event_status_2yr <- train_data_2yr$event_2yr

# Prepare 5-year training data (race variables excluded in the model)
train_data_5yr <- prepare_data(train_data, "time_to_event_5yr", "event_5yr", five_years_in_days)
X_train_5yr <- model.matrix(~ . - event_2yr - event_5yr - time_to_event_2yr - time_to_event_5yr - disease_status - person_id - raceBlack - raceWhite - raceAsian - race - 1, 
                            data = train_data_5yr)
time_train_5yr <- train_data_5yr$time_to_event_5yr
event_status_5yr <- train_data_5yr$event_5yr

# Train XGBoost models with early stopping for both 2-year and 5-year models

# Step 2: Train 2-year XGBoost model
result_2yr <- fit_xgboost_and_display(X_train_2yr, time_train_2yr, event_status_2yr, "2-Year", optimal_params, nrounds = 300, early_stopping_rounds = 10)
xgboost_model_2yr <- result_2yr$model

# Step 3: Train 5-year XGBoost model
result_5yr <- fit_xgboost_and_display(X_train_5yr, time_train_5yr, event_status_5yr, "5-Year", optimal_params, nrounds = 300, early_stopping_rounds = 10)
xgboost_model_5yr <- result_5yr$model

# Full population evaluation (all races combined)
evaluation_2yr_all <- evaluate_xgboost(xgboost_model_2yr, X_test_2yr, y_test_2yr, time_test_2yr, colnames(X_train_2yr))
evaluation_5yr_all <- evaluate_xgboost(xgboost_model_5yr, X_test_5yr, y_test_5yr, time_test_5yr, colnames(X_train_5yr))

# Define a function to evaluate for a specific race group
evaluate_by_race <- function(race_column, X_test, y_test, time_test, xgboost_model, train_column_names) {
  race_indices <- X_test[, race_column] == 1
  X_test_race <- X_test[race_indices, ]
  y_test_race <- y_test[race_indices]
  time_test_race <- time_test[race_indices]
  evaluate_xgboost(xgboost_model, X_test_race, y_test_race, time_test_race, train_column_names)
}

# Group analysis by race for 2-year and 5-year predictions
subgroups <- c("raceBlack", "raceWhite", "raceAsian")
results_2yr <- list()
results_5yr <- list()

for (group in subgroups) {
  results_2yr[[group]] <- evaluate_by_race(group, X_test_2yr, y_test_2yr, time_test_2yr, xgboost_model_2yr, colnames(X_train_2yr))
  results_5yr[[group]] <- evaluate_by_race(group, X_test_5yr, y_test_5yr, time_test_5yr, xgboost_model_5yr, colnames(X_train_5yr))
}

# Stop parallel processing
stopImplicitCluster()

# Combine results into a data frame for publication-ready table
results_table <- data.frame(
  Model = c("2-Year All", "2-Year Black", "2-Year White", "2-Year Asian", "5-Year All", "5-Year Black", "5-Year White", "5-Year Asian"),
  AUC = c(evaluation_2yr_all$auc, results_2yr$raceBlack$auc, results_2yr$raceWhite$auc, results_2yr$raceAsian$auc,
          evaluation_5yr_all$auc, results_5yr$raceBlack$auc, results_5yr$raceWhite$auc, results_5yr$raceAsian$auc),
  Harrell_C = c(evaluation_2yr_all$c_stat, results_2yr$raceBlack$c_stat, results_2yr$raceWhite$c_stat, results_2yr$raceAsian$c_stat,
                evaluation_5yr_all$c_stat, results_5yr$raceBlack$c_stat, results_5yr$raceWhite$c_stat, results_5yr$raceAsian$c_stat)
)

# Print results in a table format
print(results_table)

```

## Lab_free model- with race:

```{r}
# Load necessary libraries
library(xgboost)
library(pROC)
library(Hmisc)  # For Harrell's C-statistic
library(survival)
library(doParallel)
library(ggplot2)

# Set up parallel backend for faster execution
cores <- detectCores() - 1
registerDoParallel(cores)

# Set random seed for reproducibility
set.seed(123)

# Normalization function (log(x + 1)) to transform skewed data
normalize_data <- function(df) {
  return(log(df + 1))
}

# Define the list of lab columns to remove
labs_columns <- c("Creatinine..Mass.volume..in.Body.fluid", 
                  "Erythrocyte.distribution.width..Ratio..by.Automated.count", 
                  "Ferritin..Mass.volume..in.Serum.or.Plasma", 
                  "Hemoglobin.A1c.Hemoglobin.total.in.Blood", 
                  "Hepatitis.B.virus.surface.Ag..Presence..in.Serum.or.Plasma.by.Immunoassay", 
                  "Iron..Mass.volume..in.Serum.or.Plasma", 
                  "Iron.saturation..Mass.Fraction..in.Serum.or.Plasma", 
                  "Parathyrin.intact..Mass.volume..in.Serum.or.Plasma", 
                  "Protein..Mass.volume..in.Urine", 
                  "Triglyceride..Mass.volume..in.Serum.or.Plasma", 
                  "eGFR_ckd_epi_2021", 
                  "serum_creatinine")

# Optimal Parameters for XGBoost Cox model
optimal_params <- list(
  max_depth = 6,
  min_child_weight = 2,
  gamma = 0.5,
  eta = 0.1,                    # Learning rate
  subsample = 1,                # Full sampling
  colsample_bytree = 1,         # Full column sampling
  objective = "survival:cox",   # Cox proportional hazards objective
  eval_metric = "cox-nloglik",  # Evaluation metric for Cox models
  nthread = cores               # Number of threads to use
)

# Function to fit XGBoost model and display variable importance
fit_xgboost_and_display <- function(X_data, time_to_event, event_status, title_prefix, params, nrounds) {
  
  # Convert the data to DMatrix, which is optimized for XGBoost
  dtrain <- xgb.DMatrix(data = as.matrix(X_data), label = time_to_event)
  
  # Set base_margin to indicate censoring (1 = event occurred, 0 = censored)
  setinfo(dtrain, "base_margin", event_status)
  
  # Train the XGBoost model with the provided parameters and number of rounds
  xgboost_model <- xgboost(
    params = params, 
    data = dtrain, 
    nrounds = nrounds,  # Use nrounds to control the number of boosting rounds
    verbose = 0  # Suppress output
  )
  
  # Get the feature importance
  importance_matrix <- xgb.importance(model = xgboost_model)
  
  # Step 1: Display the selected variables and their importance
  print(paste("Selected Variables by XGBoost for", title_prefix, "Prediction:"))
  print(importance_matrix)
  
  # Step 2: Create a variable importance plot
  p <- ggplot(importance_matrix, aes(x = reorder(Feature, Gain), y = Gain)) +
    geom_bar(stat = "identity") +
    coord_flip() +  # Flip the plot to make it horizontal
    labs(title = paste(title_prefix, "Variable Importance Plot (XGBoost)"), 
         x = "Variables", 
         y = "Importance (Gain)") +
    theme_minimal()
  
  # Display the plot
  print(p)
  
  # Return the final model and feature importance
  return(list(model = xgboost_model, importance = importance_matrix))
}

# Function to prepare and normalize the data
prepare_data <- function(data, time_col, event_col, time_threshold, labs_columns) {
  # Filter out rows with non-positive time_to_event
  valid_indices <- data[[time_col]] > 0
  
  # Filter only rows with positive time to event
  filtered_data <- data[valid_indices, ]
  
  # Censor observations that exceed the time threshold (2 years or 5 years)
  filtered_data[[event_col]] <- ifelse(filtered_data[[time_col]] > time_threshold, 0, filtered_data[[event_col]])
  
  # Normalize the predictor variables with log(x + 1)
  normalized_data <- filtered_data %>%
    mutate(across(where(is.numeric), ~ normalize_data(.)))
  
  # Remove lab columns from the dataset
  normalized_data <- normalized_data[, !colnames(normalized_data) %in% labs_columns]
  
  return(normalized_data)
}

# Function to ensure test data includes all columns present in the training data
clean_test_data <- function(X_test, time_test, event_status, train_colnames) {
  
  # Ensure that the test data includes only observations with positive time-to-event
  valid_indices <- time_test > 0
  X_test_cleaned <- X_test[valid_indices, , drop = FALSE]
  time_test_cleaned <- time_test[valid_indices]
  event_status_cleaned <- event_status[valid_indices]
  
  # Remove columns not present in the training set (e.g., lab-related columns)
  X_test_cleaned <- X_test_cleaned[, colnames(X_test_cleaned) %in% train_colnames, drop = FALSE]
  
  # Add missing columns (if any) from the training set, initializing with 0s
  missing_cols <- setdiff(train_colnames, colnames(X_test_cleaned))
  if (length(missing_cols) > 0) {
    for (col in missing_cols) {
      X_test_cleaned[[col]] <- 0  # Add missing columns with 0s
    }
  }
  
  # Ensure the columns are in the same order as the training data
  X_test_cleaned <- X_test_cleaned[, train_colnames, drop = FALSE]
  
  return(list(X_test = X_test_cleaned, time_test = time_test_cleaned, event_status = event_status_cleaned))
}

# Function to evaluate the XGBoost model on the test data
evaluate_xgboost <- function(xgboost_model, X_test_data, y_test_data, time_test_data, train_column_names) {
  
  # Step 1: Ensure the test data has the same columns as the training data
  X_test_data <- X_test_data[, train_column_names, drop = FALSE]
  
  # Set feature names for the test data
  dtest <- xgb.DMatrix(data = as.matrix(X_test_data))
  colnames(dtest) <- train_column_names  # Explicitly set column names
  
  # Step 2: Predict risk scores using the XGBoost model
  risk_scores <- predict(xgboost_model, newdata = dtest)
  
  # Step 3: Calculate Harrell's C-statistic using time-to-event data
  survival_object <- Surv(time_test_data, y_test_data)
  
  # Calculate Harrell's C-statistic (concordance index)
  c_stat <- rcorr.cens(risk_scores, survival_object)["C Index"]
  
  # Step 4: Calculate AUC using the ROC curve
  roc_curve <- roc(y_test_data, risk_scores)
  auc_value <- auc(roc_curve)
  
  return(list(c_stat = c_stat, auc = auc_value))
}

# Step 1: Data preparation for 2-year and 5-year predictions
two_years_in_days <- 2 * 365.25
five_years_in_days <- 5 * 365.25

# Prepare the training data for 2-year prediction, excluding lab columns
train_data_2yr <- prepare_data(train_data, "time_to_event_2yr", "event_2yr", two_years_in_days, labs_columns)
X_train_2yr <- model.matrix(~ . - event_2yr - event_5yr - time_to_event_2yr - time_to_event_5yr - disease_status - person_id - race - 1, 
                            data = train_data_2yr)
time_train_2yr <- train_data_2yr$time_to_event_2yr
event_status_2yr <- train_data_2yr$event_2yr

# Prepare the training data for 5-year prediction, excluding lab columns
train_data_5yr <- prepare_data(train_data, "time_to_event_5yr", "event_5yr", five_years_in_days, labs_columns)
X_train_5yr <- model.matrix(~ . - event_2yr - event_5yr - time_to_event_2yr - time_to_event_5yr - disease_status - person_id - race - 1, 
                            data = train_data_5yr)
time_train_5yr <- train_data_5yr$time_to_event_5yr
event_status_5yr <- train_data_5yr$event_5yr

# Step 2: Train XGBoost models for 2-year and 5-year predictions using the **optimal parameters**

# Train and display results for the 2-year XGBoost model
result_2yr <- fit_xgboost_and_display(X_train_2yr, time_train_2yr, event_status_2yr, "2-Year", optimal_params, nrounds = 400)
xgboost_model_2yr <- result_2yr$model
importance_2yr <- result_2yr$importance  # Variable importance for 2-year model

# Train and display results for the 5-year XGBoost model
result_5yr <- fit_xgboost_and_display(X_train_5yr, time_train_5yr, event_status_5yr, "5-Year", optimal_params, nrounds = 400)
xgboost_model_5yr <- result_5yr$model
importance_5yr <- result_5yr$importance  # Variable importance for 5-year model

# Step 3: Display the final parameters
print("Final Hyperparameters for the model:")
print(optimal_params)

# Step 4: Stop parallel processing
stopImplicitCluster()

# Step 5: Clean and evaluate test data

# Clean the 2-year test data
test_data_cleaned_2yr <- clean_test_data(X_test_2yr, time_test_2yr, y_test_2yr, colnames(X_train_2yr))

# Evaluate the 2-year XGBoost model on the 2-year test data
evaluation_2yr <- evaluate_xgboost(
  xgboost_model_2yr, 
  test_data_cleaned_2yr$X_test, 
  test_data_cleaned_2yr$event_status, 
  test_data_cleaned_2yr$time_test, 
  colnames(X_train_2yr)
)

# Print the results for the 2-year model
print(paste("AUC for 2-Year Prediction:", evaluation_2yr$auc))

# Clean the 5-year test data
test_data_cleaned_5yr <- clean_test_data(X_test_5yr, time_test_5yr, y_test_5yr, colnames(X_train_5yr))

# Evaluate the 5-year XGBoost model on the 5-year test data
evaluation_5yr <- evaluate_xgboost(
  xgboost_model_5yr, 
  test_data_cleaned_5yr$X_test, 
  test_data_cleaned_5yr$event_status, 
  test_data_cleaned_5yr$time_test, 
  colnames(X_train_5yr)
)

# Print the results for the 5-year model
print(paste("AUC for 5-Year Prediction:", evaluation_5yr$auc))

# Step 6: Subgroup analysis for race groups

# Function to evaluate for a specific race group
evaluate_by_race <- function(race_column, X_test, y_test, time_test, xgboost_model, train_column_names) {
  race_indices <- X_test[, race_column] == 1
  X_test_race <- X_test[race_indices, ]
  y_test_race <- y_test[race_indices]
  time_test_race <- time_test[race_indices]
  evaluate_xgboost(xgboost_model, X_test_race, y_test_race, time_test_race, train_column_names)
}

# Group analysis by race for 2-year and 5-year predictions
subgroups <- c("raceBlack", "raceWhite", "raceAsian")
results_2yr <- list()
results_5yr <- list()

for (group in subgroups) {
  results_2yr[[group]] <- evaluate_by_race(group, X_test_2yr, y_test_2yr, time_test_2yr, xgboost_model_2yr, colnames(X_train_2yr))
  results_5yr[[group]] <- evaluate_by_race(group, X_test_5yr, y_test_5yr, time_test_5yr, xgboost_model_5yr, colnames(X_train_5yr))
}

# Combine results into a data frame for race subgroup analysis
results_table <- data.frame(
  Model = c("2-Year All", "2-Year Black", "2-Year White", "2-Year Asian", "5-Year All", "5-Year Black", "5-Year White", "5-Year Asian"),
  AUC = c(evaluation_2yr$auc, results_2yr$raceBlack$auc, results_2yr$raceWhite$auc, results_2yr$raceAsian$auc,
          evaluation_5yr$auc, results_5yr$raceBlack$auc, results_5yr$raceWhite$auc, results_5yr$raceAsian$auc),
  Harrell_C = c(evaluation_2yr$c_stat, results_2yr$raceBlack$c_stat, results_2yr$raceWhite$c_stat, results_2yr$raceAsian$c_stat,
                evaluation_5yr$c_stat, results_5yr$raceBlack$c_stat, results_5yr$raceWhite$c_stat, results_5yr$raceAsian$c_stat)
)

# Print the results table
print(results_table)

```

## Lab_free model- without race:

```{r}
# Load necessary libraries
library(xgboost)
library(pROC)
library(Hmisc)  # For Harrell's C-statistic
library(survival)
library(doParallel)
library(ggplot2)

# Set up parallel backend for faster execution
cores <- detectCores() - 1
registerDoParallel(cores)

# Set random seed for reproducibility
set.seed(123)

# Normalization function (log(x + 1)) to transform skewed data
normalize_data <- function(df) {
  return(log(df + 1))
}

# Define the list of lab columns to remove
labs_columns <- c("Creatinine..Mass.volume..in.Body.fluid", 
                  "Erythrocyte.distribution.width..Ratio..by.Automated.count", 
                  "Ferritin..Mass.volume..in.Serum.or.Plasma", 
                  "Hemoglobin.A1c.Hemoglobin.total.in.Blood", 
                  "Hepatitis.B.virus.surface.Ag..Presence..in.Serum.or.Plasma.by.Immunoassay", 
                  "Iron..Mass.volume..in.Serum.or.Plasma", 
                  "Iron.saturation..Mass.Fraction..in.Serum.or.Plasma", 
                  "Parathyrin.intact..Mass.volume..in.Serum.or.Plasma", 
                  "Protein..Mass.volume..in.Urine", 
                  "Triglyceride..Mass.volume..in.Serum.or.Plasma", 
                  "eGFR_ckd_epi_2021", 
                  "serum_creatinine",
                  "raceBlack", "raceWhite", "raceAsian")

# Optimal Parameters for XGBoost Cox model
optimal_params <- list(
  max_depth = 6,
  min_child_weight = 2,
  gamma = 0.5,
  eta = 0.1,                    # Learning rate
  subsample = 1,                # Full sampling
  colsample_bytree = 1,         # Full column sampling
  objective = "survival:cox",   # Cox proportional hazards objective
  eval_metric = "cox-nloglik",  # Evaluation metric for Cox models
  nthread = cores               # Number of threads to use
)

# Function to fit XGBoost model and display variable importance
fit_xgboost_and_display <- function(X_data, time_to_event, event_status, title_prefix, params, nrounds) {
  
  # Convert the data to DMatrix, which is optimized for XGBoost
  dtrain <- xgb.DMatrix(data = as.matrix(X_data), label = time_to_event)
  
  # Set base_margin to indicate censoring (1 = event occurred, 0 = censored)
  setinfo(dtrain, "base_margin", event_status)
  
  # Train the XGBoost model with the provided parameters and number of rounds
  xgboost_model <- xgboost(
    params = params, 
    data = dtrain, 
    nrounds = nrounds,  # Use nrounds to control the number of boosting rounds
    verbose = 0  # Suppress output
  )
  
  # Get the feature importance
  importance_matrix <- xgb.importance(model = xgboost_model)
  
  # Step 1: Display the selected variables and their importance
  print(paste("Selected Variables by XGBoost for", title_prefix, "Prediction:"))
  print(importance_matrix)
  
  # Step 2: Create a variable importance plot
  p <- ggplot(importance_matrix, aes(x = reorder(Feature, Gain), y = Gain)) +
    geom_bar(stat = "identity") +
    coord_flip() +  # Flip the plot to make it horizontal
    labs(title = paste(title_prefix, "Variable Importance Plot (XGBoost)"), 
         x = "Variables", 
         y = "Importance (Gain)") +
    theme_minimal()
  
  # Display the plot
  print(p)
  
  # Return the final model and feature importance
  return(list(model = xgboost_model, importance = importance_matrix))
}

# Function to prepare and normalize the data
prepare_data <- function(data, time_col, event_col, time_threshold, labs_columns) {
  # Filter out rows with non-positive time_to_event
  valid_indices <- data[[time_col]] > 0
  
  # Filter only rows with positive time to event
  filtered_data <- data[valid_indices, ]
  
  # Censor observations that exceed the time threshold (2 years or 5 years)
  filtered_data[[event_col]] <- ifelse(filtered_data[[time_col]] > time_threshold, 0, filtered_data[[event_col]])
  
  # Normalize the predictor variables with log(x + 1)
  normalized_data <- filtered_data %>%
    mutate(across(where(is.numeric), ~ normalize_data(.)))
  
  # Remove lab columns from the dataset
  normalized_data <- normalized_data[, !colnames(normalized_data) %in% labs_columns]
  
  return(normalized_data)
}

# Function to ensure test data includes all columns present in the training data
clean_test_data <- function(X_test, time_test, event_status, train_colnames) {
  
  # Ensure that the test data includes only observations with positive time-to-event
  valid_indices <- time_test > 0
  X_test_cleaned <- X_test[valid_indices, , drop = FALSE]
  time_test_cleaned <- time_test[valid_indices]
  event_status_cleaned <- event_status[valid_indices]
  
  # Remove columns not present in the training set (e.g., lab-related columns)
  X_test_cleaned <- X_test_cleaned[, colnames(X_test_cleaned) %in% train_colnames, drop = FALSE]
  
  # Add missing columns (if any) from the training set, initializing with 0s
  missing_cols <- setdiff(train_colnames, colnames(X_test_cleaned))
  if (length(missing_cols) > 0) {
    for (col in missing_cols) {
      X_test_cleaned[[col]] <- 0  # Add missing columns with 0s
    }
  }
  
  # Ensure the columns are in the same order as the training data
  X_test_cleaned <- X_test_cleaned[, train_colnames, drop = FALSE]
  
  return(list(X_test = X_test_cleaned, time_test = time_test_cleaned, event_status = event_status_cleaned))
}

# Function to evaluate the XGBoost model on the test data
evaluate_xgboost <- function(xgboost_model, X_test_data, y_test_data, time_test_data, train_column_names) {
  
  # Step 1: Ensure the test data has the same columns as the training data
  X_test_data <- X_test_data[, train_column_names, drop = FALSE]
  
  # Set feature names for the test data
  dtest <- xgb.DMatrix(data = as.matrix(X_test_data))
  colnames(dtest) <- train_column_names  # Explicitly set column names
  
  # Step 2: Predict risk scores using the XGBoost model
  risk_scores <- predict(xgboost_model, newdata = dtest)
  
  # Step 3: Calculate Harrell's C-statistic using time-to-event data
  survival_object <- Surv(time_test_data, y_test_data)
  
  # Calculate Harrell's C-statistic (concordance index)
  c_stat <- rcorr.cens(risk_scores, survival_object)["C Index"]
  
  # Step 4: Calculate AUC using the ROC curve
  roc_curve <- roc(y_test_data, risk_scores)
  auc_value <- auc(roc_curve)
  
  return(list(c_stat = c_stat, auc = auc_value))
}

# Step 1: Data preparation for 2-year and 5-year predictions
two_years_in_days <- 2 * 365.25
five_years_in_days <- 5 * 365.25

# Prepare the training data for 2-year prediction, excluding lab columns
train_data_2yr <- prepare_data(train_data, "time_to_event_2yr", "event_2yr", two_years_in_days, labs_columns)
X_train_2yr <- model.matrix(~ . - event_2yr - event_5yr - time_to_event_2yr - time_to_event_5yr - disease_status - person_id - race - 1, 
                            data = train_data_2yr)
time_train_2yr <- train_data_2yr$time_to_event_2yr
event_status_2yr <- train_data_2yr$event_2yr

# Prepare the training data for 5-year prediction, excluding lab columns
train_data_5yr <- prepare_data(train_data, "time_to_event_5yr", "event_5yr", five_years_in_days, labs_columns)
X_train_5yr <- model.matrix(~ . - event_2yr - event_5yr - time_to_event_2yr - time_to_event_5yr - disease_status - person_id - race - 1, 
                            data = train_data_5yr)
time_train_5yr <- train_data_5yr$time_to_event_5yr
event_status_5yr <- train_data_5yr$event_5yr

# Step 2: Train XGBoost models for 2-year and 5-year predictions using the **optimal parameters**

# Train and display results for the 2-year XGBoost model
result_2yr <- fit_xgboost_and_display(X_train_2yr, time_train_2yr, event_status_2yr, "2-Year", optimal_params, nrounds = 400)
xgboost_model_2yr <- result_2yr$model
importance_2yr <- result_2yr$importance  # Variable importance for 2-year model

# Train and display results for the 5-year XGBoost model
result_5yr <- fit_xgboost_and_display(X_train_5yr, time_train_5yr, event_status_5yr, "5-Year", optimal_params, nrounds = 400)
xgboost_model_5yr <- result_5yr$model
importance_5yr <- result_5yr$importance  # Variable importance for 5-year model

# Step 3: Display the final parameters
print("Final Hyperparameters for the model:")
print(optimal_params)

# Step 4: Stop parallel processing
stopImplicitCluster()

# Step 5: Clean and evaluate test data

# Clean the 2-year test data
test_data_cleaned_2yr <- clean_test_data(X_test_2yr, time_test_2yr, y_test_2yr, colnames(X_train_2yr))

# Evaluate the 2-year XGBoost model on the 2-year test data
evaluation_2yr <- evaluate_xgboost(
  xgboost_model_2yr, 
  test_data_cleaned_2yr$X_test, 
  test_data_cleaned_2yr$event_status, 
  test_data_cleaned_2yr$time_test, 
  colnames(X_train_2yr)
)

# Print the results for the 2-year model
print(paste("AUC for 2-Year Prediction:", evaluation_2yr$auc))

# Clean the 5-year test data
test_data_cleaned_5yr <- clean_test_data(X_test_5yr, time_test_5yr, y_test_5yr, colnames(X_train_5yr))

# Evaluate the 5-year XGBoost model on the 5-year test data
evaluation_5yr <- evaluate_xgboost(
  xgboost_model_5yr, 
  test_data_cleaned_5yr$X_test, 
  test_data_cleaned_5yr$event_status, 
  test_data_cleaned_5yr$time_test, 
  colnames(X_train_5yr)
)

# Print the results for the 5-year model
print(paste("AUC for 5-Year Prediction:", evaluation_5yr$auc))

# Step 6: Subgroup analysis for race groups

# Function to evaluate for a specific race group
evaluate_by_race <- function(race_column, X_test, y_test, time_test, xgboost_model, train_column_names) {
  race_indices <- X_test[, race_column] == 1
  X_test_race <- X_test[race_indices, ]
  y_test_race <- y_test[race_indices]
  time_test_race <- time_test[race_indices]
  evaluate_xgboost(xgboost_model, X_test_race, y_test_race, time_test_race, train_column_names)
}

# Group analysis by race for 2-year and 5-year predictions
subgroups <- c("raceBlack", "raceWhite", "raceAsian")
results_2yr <- list()
results_5yr <- list()

for (group in subgroups) {
  results_2yr[[group]] <- evaluate_by_race(group, X_test_2yr, y_test_2yr, time_test_2yr, xgboost_model_2yr, colnames(X_train_2yr))
  results_5yr[[group]] <- evaluate_by_race(group, X_test_5yr, y_test_5yr, time_test_5yr, xgboost_model_5yr, colnames(X_train_5yr))
}

# Combine results into a data frame for race subgroup analysis
results_table <- data.frame(
  Model = c("2-Year All", "2-Year Black", "2-Year White", "2-Year Asian", "5-Year All", "5-Year Black", "5-Year White", "5-Year Asian"),
  AUC = c(evaluation_2yr$auc, results_2yr$raceBlack$auc, results_2yr$raceWhite$auc, results_2yr$raceAsian$auc,
          evaluation_5yr$auc, results_5yr$raceBlack$auc, results_5yr$raceWhite$auc, results_5yr$raceAsian$auc),
  Harrell_C = c(evaluation_2yr$c_stat, results_2yr$raceBlack$c_stat, results_2yr$raceWhite$c_stat, results_2yr$raceAsian$c_stat,
                evaluation_5yr$c_stat, results_5yr$raceBlack$c_stat, results_5yr$raceWhite$c_stat, results_5yr$raceAsian$c_stat)
)

# Print the results table
print(results_table)

```

# Random Survival Forest

## Data preparation

```{r}
# Step 1: Load the dataset
filtered_person_with_summary <- read.csv("rerefiltered_person_with_summary.csv")

# Step 2: Replace all NA values in the dataset with 0
filtered_person_with_summary[is.na(filtered_person_with_summary)] <- 0

# Step 3: Ensure no NA values
if (sum(is.na(filtered_person_with_summary)) != 0) {
  stop("There are still NA values in the data.")
}

# Step 4: Define time limits for 5-year and 2-year in days
five_years_in_days <- 5 * 365.25  # 1826.25 days
two_years_in_days <- 2 * 365.25   # 730.5 days

# Step 5: Create event_5yr and time_to_event_5yr
filtered_person_with_summary$event_5yr <- ifelse(filtered_person_with_summary$time_to_event <= five_years_in_days & 
                                                 filtered_person_with_summary$disease_status == 1, 1, 0)
filtered_person_with_summary$time_to_event_5yr <- pmin(filtered_person_with_summary$time_to_event, five_years_in_days)

# Create event_2yr and time_to_event_2yr
filtered_person_with_summary$event_2yr <- ifelse(filtered_person_with_summary$time_to_event <= two_years_in_days & 
                                                 filtered_person_with_summary$disease_status == 1, 1, 0)
filtered_person_with_summary$time_to_event_2yr <- pmin(filtered_person_with_summary$time_to_event, two_years_in_days)

# Step 6: Normalize numeric variables using log(variable + 1)
numeric_cols <- colnames(filtered_person_with_summary)[5:39]  # Adjust this range for your numeric columns
filtered_person_with_summary[numeric_cols] <- log(filtered_person_with_summary[numeric_cols] + 1)

# Step 7: Create binary race variables
filtered_person_with_summary$raceBlack <- ifelse(filtered_person_with_summary$race == "Black or African American", 1, 0)
filtered_person_with_summary$raceWhite <- ifelse(filtered_person_with_summary$race == "White", 1, 0)
filtered_person_with_summary$raceAsian <- ifelse(filtered_person_with_summary$race == "Asian", 1, 0)

# Step 8: Split Black and White data into 70% training and 30% test
set.seed(123)  # For reproducibility
train_test_split <- function(data, race_col, split_ratio = 0.7) {
  sample_size <- floor(split_ratio * nrow(data))
  train_indices <- sample(seq_len(nrow(data)), size = sample_size)
  train_data <- data[train_indices, ]
  test_data <- data[-train_indices, ]
  return(list(train_data = train_data, test_data = test_data))
}

# Splitting Black data
black_data <- subset(filtered_person_with_summary, raceBlack == 1)
split_black <- train_test_split(black_data)
train_data_black <- split_black$train_data
test_data_black <- split_black$test_data

# Splitting White data
white_data <- subset(filtered_person_with_summary, raceWhite == 1)
split_white <- train_test_split(white_data)
train_data_white <- split_white$train_data
test_data_white <- split_white$test_data

# 100% of Asian data goes to test
test_data_asian <- subset(filtered_person_with_summary, raceAsian == 1)

# Step 9: Combine the training data (Black and White) and test data (Black, White, Asian)
train_data <- rbind(train_data_black, train_data_white)
test_data <- rbind(test_data_black, test_data_white, test_data_asian)

# Step 10: Filter out non-positive event times for training and test data
filter_positive_times <- function(data, time_col) {
  return(data[data[[time_col]] > 0, ])
}

# For 2-year data
train_data_2yr <- filter_positive_times(train_data, "time_to_event_2yr")
test_data_2yr <- filter_positive_times(test_data, "time_to_event_2yr")

# For 5-year data
train_data_5yr <- filter_positive_times(train_data, "time_to_event_5yr")
test_data_5yr <- filter_positive_times(test_data, "time_to_event_5yr")

# Step 11: Prepare data for random survival forest (no need to create model matrices)
# Select relevant columns for RSF (drop non-predictor columns)
non_predictor_cols <- c("person_id", "disease_status", "race", "event_2yr", "event_5yr", "time_to_event_2yr", "time_to_event_5yr")
predictor_cols_2yr <- setdiff(colnames(train_data_2yr), non_predictor_cols)
predictor_cols_5yr <- setdiff(colnames(train_data_5yr), non_predictor_cols)

```

#### Counts before cleaning

```{r}
# Load necessary library for better data manipulation
library(dplyr)

# Define the race groups we are interested in
race_groups <- c("White", "Black or African American", "Asian")

# Function to calculate the number of cases and controls for both 2-year and 5-year events
count_cases_controls <- function(data, race_variable, event_2yr_variable, event_5yr_variable) {
  
  # Initialize an empty result data frame to store results
  result <- data.frame(Race = race_groups, 
                       Cases_2yr = integer(length(race_groups)), Controls_2yr = integer(length(race_groups)),
                       Cases_5yr = integer(length(race_groups)), Controls_5yr = integer(length(race_groups)))

  # Loop through each race group
  for (race in race_groups) {
    
    # Filter the data for the current race
    race_data <- data[data[[race_variable]] == race, ]
    
    # Calculate the number of 2-year cases and controls
    num_cases_2yr <- sum(race_data[[event_2yr_variable]] == 1, na.rm = TRUE)
    num_controls_2yr <- sum(race_data[[event_2yr_variable]] == 0, na.rm = TRUE)
    
    # Calculate the number of 5-year cases and controls
    num_cases_5yr <- sum(race_data[[event_5yr_variable]] == 1, na.rm = TRUE)
    num_controls_5yr <- sum(race_data[[event_5yr_variable]] == 0, na.rm = TRUE)
    
    # Store the results in the result data frame for each race group
    result[result$Race == race, "Cases_2yr"] <- num_cases_2yr
    result[result$Race == race, "Controls_2yr"] <- num_controls_2yr
    result[result$Race == race, "Cases_5yr"] <- num_cases_5yr
    result[result$Race == race, "Controls_5yr"] <- num_controls_5yr
  }
  
  # Add a row for the total (across all races)
  total_cases_2yr <- sum(data[[event_2yr_variable]] == 1, na.rm = TRUE)
  total_controls_2yr <- sum(data[[event_2yr_variable]] == 0, na.rm = TRUE)
  total_cases_5yr <- sum(data[[event_5yr_variable]] == 1, na.rm = TRUE)
  total_controls_5yr <- sum(data[[event_5yr_variable]] == 0, na.rm = TRUE)
  
  result <- rbind(result, 
                  data.frame(Race = "Total", 
                             Cases_2yr = total_cases_2yr, Controls_2yr = total_controls_2yr,
                             Cases_5yr = total_cases_5yr, Controls_5yr = total_controls_5yr))
  
  return(result)
}

# Calculate cases and controls for the training data for both 2-year and 5-year events
train_summary <- count_cases_controls(train_data, "race", "event_2yr", "event_5yr")

# Calculate cases and controls for the test data for both 2-year and 5-year events
test_summary <- count_cases_controls(test_data, "race", "event_2yr", "event_5yr")

# Combine the results for both training and test data into one dataset
combined_summary <- list(Train = train_summary, Test = test_summary)

# Print the results
print("Training Data Summary (2-year and 5-year):")
print(combined_summary$Train)

print("Test Data Summary (2-year and 5-year):")
print(combined_summary$Test)

```

#### Counts after cleaning

```{r}
# Load necessary library for better data manipulation
library(dplyr)

# Define the race groups we are interested in
race_groups <- c("White", "Black or African American", "Asian")

# Function to calculate the number of cases and controls for both 2-year and 5-year events, with positive time-to-event
count_cases_controls_positive_time <- function(data, race_variable, event_2yr_variable, event_5yr_variable, time_2yr_variable, time_5yr_variable) {
  
  # Initialize an empty result data frame to store results
  result <- data.frame(Race = race_groups, 
                       Cases_2yr_Positive = integer(length(race_groups)), Controls_2yr_Positive = integer(length(race_groups)),
                       Cases_5yr_Positive = integer(length(race_groups)), Controls_5yr_Positive = integer(length(race_groups)))
  
  # Loop through each race group
  for (race in race_groups) {
    
    # Filter the data for the current race and keep only rows with positive time-to-event
    race_data_2yr <- data[data[[race_variable]] == race & data[[time_2yr_variable]] > 0, ]
    race_data_5yr <- data[data[[race_variable]] == race & data[[time_5yr_variable]] > 0, ]
    
    # Calculate the number of 2-year cases and controls with positive time-to-event
    num_cases_2yr <- sum(race_data_2yr[[event_2yr_variable]] == 1, na.rm = TRUE)
    num_controls_2yr <- sum(race_data_2yr[[event_2yr_variable]] == 0, na.rm = TRUE)
    
    # Calculate the number of 5-year cases and controls with positive time-to-event
    num_cases_5yr <- sum(race_data_5yr[[event_5yr_variable]] == 1, na.rm = TRUE)
    num_controls_5yr <- sum(race_data_5yr[[event_5yr_variable]] == 0, na.rm = TRUE)
    
    # Store the results in the result data frame for each race group
    result[result$Race == race, "Cases_2yr_Positive"] <- num_cases_2yr
    result[result$Race == race, "Controls_2yr_Positive"] <- num_controls_2yr
    result[result$Race == race, "Cases_5yr_Positive"] <- num_cases_5yr
    result[result$Race == race, "Controls_5yr_Positive"] <- num_controls_5yr
  }
  
  # Add a row for the total (across all races) with positive time-to-event
  total_cases_2yr <- sum(data[[event_2yr_variable]] == 1 & data[[time_2yr_variable]] > 0, na.rm = TRUE)
  total_controls_2yr <- sum(data[[event_2yr_variable]] == 0 & data[[time_2yr_variable]] > 0, na.rm = TRUE)
  total_cases_5yr <- sum(data[[event_5yr_variable]] == 1 & data[[time_5yr_variable]] > 0, na.rm = TRUE)
  total_controls_5yr <- sum(data[[event_5yr_variable]] == 0 & data[[time_5yr_variable]] > 0, na.rm = TRUE)
  
  result <- rbind(result, 
                  data.frame(Race = "Total", 
                             Cases_2yr_Positive = total_cases_2yr, Controls_2yr_Positive = total_controls_2yr,
                             Cases_5yr_Positive = total_cases_5yr, Controls_5yr_Positive = total_controls_5yr))
  
  return(result)
}

# Calculate cases and controls with positive time-to-event for the training data
train_summary <- count_cases_controls_positive_time(train_data, "race", "event_2yr", "event_5yr", "time_to_event_2yr", "time_to_event_5yr")

# Calculate cases and controls with positive time-to-event for the test data
test_summary <- count_cases_controls_positive_time(test_data, "race", "event_2yr", "event_5yr", "time_to_event_2yr", "time_to_event_5yr")

# Combine the results for both training and test data into one dataset
combined_summary <- list(Train = train_summary, Test = test_summary)

# Print the results
cat("Training Data Summary (2-year and 5-year with positive time-to-event):\n")
print(combined_summary$Train)

cat("\nTest Data Summary (2-year and 5-year with positive time-to-event):\n")
print(combined_summary$Test)

```

## 
