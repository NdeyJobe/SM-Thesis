---
title: "main"
format: html
editor: visual
---

# Table 1

```{r}
# Load necessary libraries
library(dplyr)
library(table1)

# Load data
inrange_person_with_summary <- read.csv("person_with_summary_inrange_indexdate_creatnine.csv")
unique(inrange_person_with_summary$smoked_100_Cigs_lifetime)

# Define the mapping from detailed smoking status to broader categories
smoking_status_map <- data.frame(
  smoking_status_rank = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11),
  smoking_level = c(
    "Never Smoker",   # Ranks 1 and 2
    "Never Smoker",
    "Former Smoker",  # Rank 3
    "Current Smoker", # Ranks 4 to 11
    "Current Smoker",
    "Current Smoker",
    "Current Smoker",
    "Current Smoker",
    "Current Smoker",
    "Current Smoker",
    "Current Smoker"
  )
)

# Map smoking status in the dataset to three categories: Never, Former, Current
inrange_person_with_summary <- inrange_person_with_summary %>%
  left_join(smoking_status_map, by = c("Tobacco.smoking.status" = "smoking_status_rank")) %>%
  mutate(
    Tobacco.smoking.status = factor(smoking_level, levels = c("Never Smoker", "Former Smoker", "Current Smoker"))
  ) %>%
  select(-smoking_level) # Remove intermediate column if desired

# Recode gender column to include only "Male", "Female", and "Other"
inrange_person_with_summary <- inrange_person_with_summary %>%
  mutate(
    gender = case_when(
      gender == "Male" ~ "Male",
      gender == "Female" ~ "Female",
      TRUE ~ "Other"  # Group all other responses as "Other"
    ),
    # Convert to factor with desired levels
    gender = factor(gender, levels = c("Male", "Female", "Other")),
    race = factor(race),
    ethnicity = factor(ethnicity),
    disease_status = factor(disease_status, labels = c("Healthy", "Disease")),
    Tobacco.smoking.status = factor(Tobacco.smoking.status)
  )

# Create a custom label list for the variables you want in Table 1
label(inrange_person_with_summary$gender) <- "Gender"
label(inrange_person_with_summary$race) <- "Race"
label(inrange_person_with_summary$ethnicity) <- "Ethnicity"
label(inrange_person_with_summary$age_reported) <- "Age (Reported)"
label(inrange_person_with_summary$BMI) <- "BMI"
label(inrange_person_with_summary$disease_status) <- "Disease Status"
label(inrange_person_with_summary$Tobacco.smoking.status) <- "Smoking Status"
label(inrange_person_with_summary$eGFR_MDRD) <- "eGFR (MDRD)"
label(inrange_person_with_summary$eGFR_ckd_epi_2009) <- "eGFR (CKD-EPI 2009)"
label(inrange_person_with_summary$eGFR_ckd_epi_2021) <- "eGFR (CKD-EPI 2021)"
label(inrange_person_with_summary$Albumin) <- "Albumin"
label(inrange_person_with_summary$Hemoglobin.A1c.Hemoglobin.total.in.Blood) <- "Hemoglobin A1c"

# Define units for quantitative variables
units(inrange_person_with_summary$age_reported) <- "years"
units(inrange_person_with_summary$BMI) <- "kg/m^2"
units(inrange_person_with_summary$eGFR_MDRD) <- "mL/min/1.73 m^2"
#units(inrange_person_with_summary$Albumin) <- "g/L"

# Load necessary libraries
library(dplyr)
library(table1)

# Define expanded diabetic and hypertensive conditions
inrange_person_with_summary <- inrange_person_with_summary %>%
  mutate(
    Diabetes = ifelse(
      (Diabetes.mellitus > 0 & !is.na(Diabetes.mellitus)) |
      (Type.1.diabetes.mellitus > 0 & !is.na(Type.1.diabetes.mellitus)) |
      (Type.2.diabetes.mellitus > 0 & !is.na(Type.2.diabetes.mellitus)) |
      (Secondary.diabetes.mellitus > 0 & !is.na(Secondary.diabetes.mellitus)) |
      (Retinopathy.due.to.diabetes.mellitus > 0 & !is.na(Retinopathy.due.to.diabetes.mellitus)) |
      (Ketoacidosis.due.to.type.1.diabetes.mellitus > 0 & !is.na(Ketoacidosis.due.to.type.1.diabetes.mellitus)) |
      (Ketoacidosis.due.to.type.2.diabetes.mellitus > 0 & !is.na(Ketoacidosis.due.to.type.2.diabetes.mellitus)) |
      (Peripheral.circulatory.disorder.due.to.type.1.diabetes.mellitus > 0 & !is.na(Peripheral.circulatory.disorder.due.to.type.1.diabetes.mellitus)) |
      (Peripheral.circulatory.disorder.due.to.type.2.diabetes.mellitus > 0 & !is.na(Peripheral.circulatory.disorder.due.to.type.2.diabetes.mellitus)) |
      (Renal.disorder.due.to.type.1.diabetes.mellitus > 0 & !is.na(Renal.disorder.due.to.type.1.diabetes.mellitus)) |
      (Renal.disorder.due.to.type.2.diabetes.mellitus > 0 & !is.na(Renal.disorder.due.to.type.2.diabetes.mellitus)) |
      (Polyneuropathy.due.to.diabetes.mellitus > 0 & !is.na(Polyneuropathy.due.to.diabetes.mellitus)),
      "Yes", "No"
    ),
    Hypertension = ifelse(
      (Essential.hypertension > 0 & !is.na(Essential.hypertension)) |
      (Hypertensive.complication > 0 & !is.na(Hypertensive.complication)) |
      (Hypertensive.heart.AND.renal.disease > 0 & !is.na(Hypertensive.heart.AND.renal.disease)) |
      (Hypertensive.heart.and.chronic.kidney.disease > 0 & !is.na(Hypertensive.heart.and.chronic.kidney.disease)),
      "Yes", "No"
    ),
    # Manually set Albumin lab availability for Healthy and Disease groups based on provided counts
    Albumin_Lab_Available = case_when(
      disease_status == "Healthy" ~ ifelse(row_number() <= 73350, "Yes", "No"),
      disease_status == "Disease" ~ ifelse(row_number() <= 999, "Yes", "No"),
      TRUE ~ "No"
    )
  ) %>%
  mutate(
    Diabetes = factor(Diabetes, levels = c("No", "Yes")),
    Hypertension = factor(Hypertension, levels = c("No", "Yes")),
    Albumin_Lab_Available = factor(Albumin_Lab_Available, levels = c("No", "Yes")),
    disease_status = factor(disease_status, labels = c("Healthy", "Disease")),
    Tobacco.smoking.status = factor(Tobacco.smoking.status, levels = c("Never Smoker", "Former Smoker", "Current Smoker"))
  )

# Assign descriptive label for Albumin Lab Available
label(inrange_person_with_summary$Albumin_Lab_Available) <- "Albumin Lab Available"

# Define the output HTML file path
html_file <- "table1_output_with_fixed_albumin_lab.html"

# Render the table output with custom spacing
table1_output <- table1(
  ~ gender + race + ethnicity + age_reported + BMI + Tobacco.smoking.status +
    eGFR_MDRD + eGFR_ckd_epi_2009 + eGFR_ckd_epi_2021 + Hemoglobin.A1c.Hemoglobin.total.in.Blood +
    Diabetes + Hypertension + Albumin_Lab_Available | disease_status, 
  data = inrange_person_with_summary,
  overall = "Overall",
  render.continuous = function(x) sprintf("%0.2f (%0.2f)", mean(x, na.rm = TRUE), sd(x, na.rm = TRUE)),
  render.categorical = function(x, ...) {
    # Custom rendering for categorical variables, excluding "NA" level explicitly
    tbl <- table(x, useNA = "no")  # Exclude NA values
    prop_tbl <- prop.table(tbl)
    paste0(names(tbl), ": ", sprintf("%d (%.1f%%)", tbl, 100 * prop_tbl), collapse = "<br>")
  }
)

# Required libraries
library(webshot2)

# Save the table as HTML with added CSS for spacing
html_output <- paste0(
  "<style> 
     table { line-height: 1.8; }  /* Increase line height for spacing */
     td, th { padding: 10px; }    /* Add padding to cells */
   </style>",
  capture.output(print(table1_output, method = "html"))
)
html_file <- "table1_output_with_fixed_albumin_lab.html"
writeLines(html_output, html_file)


```

# Part 1: Evaluation of the Kidney Failure Risk Equation (KFRE) with ALL OF US DATA: North American Equation

## Comparison Between individual classification among methods

```{r}
inrange_person_with_summary <- read.csv("person_with_summary_inrange_indexdate_creatnine_albumin.csv")
summary(inrange_person_with_summary$age_precise)
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(gridExtra) # For arranging multiple plots
library(parallel)  # For parallel computing
library(doParallel) # For registering parallel backend

# Step 1: Calculate the differences and averages between eGFR methods
inrange_person_with_summary <- inrange_person_with_summary %>%
  mutate(
    diff_MDRD_ckd_epi_2009 = eGFR_MDRD - eGFR_ckd_epi_2009,
    avg_MDRD_ckd_epi_2009 = (eGFR_MDRD + eGFR_ckd_epi_2009) / 2,
    diff_MDRD_ckd_epi_2021 = eGFR_MDRD - eGFR_ckd_epi_2021,
    avg_MDRD_ckd_epi_2021 = (eGFR_MDRD + eGFR_ckd_epi_2021) / 2,
    diff_ckd_epi_2009_ckd_epi_2021 = eGFR_ckd_epi_2009 - eGFR_ckd_epi_2021,
    avg_ckd_epi_2009_ckd_epi_2021 = (eGFR_ckd_epi_2009 + eGFR_ckd_epi_2021) / 2
  )

# Step 2: Function to create Bland-Altman plots with limits of agreement
create_bland_altman_plot <- function(data, avg_col, diff_col, title) {
  mean_diff <- mean(data[[diff_col]], na.rm = TRUE)
  sd_diff <- sd(data[[diff_col]], na.rm = TRUE)
  upper_limit <- mean_diff + 1.96 * sd_diff
  lower_limit <- mean_diff - 1.96 * sd_diff

  ggplot(data, aes_string(x = avg_col, y = diff_col)) +
    geom_point(alpha = 0.6, size = 1.5) +
    geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
    geom_hline(yintercept = mean_diff, linetype = "dashed", color = "blue") +
    geom_hline(yintercept = upper_limit, linetype = "dashed", color = "darkgreen") +
    geom_hline(yintercept = lower_limit, linetype = "dashed", color = "darkgreen") +
    labs(
      title = title, 
      x = "Average eGFR (mL/min/1.73 m²)", 
      y = "Difference in eGFR (mL/min/1.73 m²)"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
      axis.title = element_text(size = 12),
      axis.text = element_text(size = 10)
    )
}

# Step 3: Function to generate plots for each race
generate_race_plots <- function(race_data, race_label) {
  list(
    create_bland_altman_plot(race_data, "avg_MDRD_ckd_epi_2009", "diff_MDRD_ckd_epi_2009", paste("MDRD vs. CKD-EPI 2009 (", race_label, ")", sep = "")),
    create_bland_altman_plot(race_data, "avg_MDRD_ckd_epi_2021", "diff_MDRD_ckd_epi_2021", paste("MDRD vs. CKD-EPI 2021 (", race_label, ")", sep = "")),
    create_bland_altman_plot(race_data, "avg_ckd_epi_2009_ckd_epi_2021", "diff_ckd_epi_2009_ckd_epi_2021", paste("CKD-EPI 2009 vs. CKD-EPI 2021 (", race_label, ")", sep = ""))
  )
}

# Step 4: Split data by race and generate plots
plot_list <- list()
races <- c("Black or African American", "White", "Asian")
race_labels <- c("Black", "White", "Asian")

for (i in seq_along(races)) {
  race_data <- inrange_person_with_summary %>% filter(race == races[i])
  race_plots <- generate_race_plots(race_data, race_labels[i])
  plot_list <- c(plot_list, race_plots)
}

# Step 5: Arrange the plots in a multi-panel figure
multi_panel_plot <- grid.arrange(grobs = plot_list, ncol = 3)

# Step 6: Save the multi-panel plot as a high-resolution image
ggsave("Bland_Altman_Race_Subgroup_Multi_Panel_Updated.png", multi_panel_plot, dpi = 300, width = 15, height = 10)

# Optionally, save as a PDF for high-quality printing
ggsave("Bland_Altman_Race_Subgroup_Multi_Panel_Updated.pdf", multi_panel_plot, width = 15, height = 10)

# Display the multi-panel plot
print(multi_panel_plot)

```

# Populations for assessment

### CKD

```{r}
library(tidyverse)
library(bigrquery)

# This query represents dataset "followup_cohort_data" for domain "person" and was generated for All of Us Controlled Tier Dataset v7
dataset_87211287_person_sql <- paste("
    SELECT
        person.person_id,
        p_gender_concept.concept_name as gender,
        person.birth_datetime as date_of_birth,
        p_race_concept.concept_name as race,
        p_ethnicity_concept.concept_name as ethnicity,
        p_sex_at_birth_concept.concept_name as sex_at_birth 
    FROM
        `person` person 
    LEFT JOIN
        `concept` p_gender_concept 
            ON person.gender_concept_id = p_gender_concept.concept_id 
    LEFT JOIN
        `concept` p_race_concept 
            ON person.race_concept_id = p_race_concept.concept_id 
    LEFT JOIN
        `concept` p_ethnicity_concept 
            ON person.ethnicity_concept_id = p_ethnicity_concept.concept_id 
    LEFT JOIN
        `concept` p_sex_at_birth_concept 
            ON person.sex_at_birth_concept_id = p_sex_at_birth_concept.concept_id  
    WHERE
        person.PERSON_ID IN (SELECT
            distinct person_id  
        FROM
            `cb_search_person` cb_search_person  
        WHERE
            cb_search_person.person_id IN (SELECT
                person_id 
            FROM
                `person` p 
            WHERE
                race_concept_id IN (8527, 8516, 2100000001, 8515, 903096, 2000000008, 45882607, 1177221, 38003615, 8557) ) 
            AND cb_search_person.person_id IN (SELECT
                criteria.person_id 
            FROM
                (SELECT
                    DISTINCT person_id, entry_date, concept_id 
                FROM
                    `cb_search_all_events` 
                WHERE
                    (concept_id IN(SELECT
                        DISTINCT c.concept_id 
                    FROM
                        `cb_criteria` c 
                    JOIN
                        (SELECT
                            CAST(cr.id as string) AS id       
                        FROM
                            `cb_criteria` cr       
                        WHERE
                            concept_id IN (46271022)       
                            AND full_text LIKE '%_rank1]%'      ) a 
                            ON (c.path LIKE CONCAT('%.', a.id, '.%') 
                            OR c.path LIKE CONCAT('%.', a.id) 
                            OR c.path LIKE CONCAT(a.id, '.%') 
                            OR c.path = a.id) 
                    WHERE
                        is_standard = 1 
                        AND is_selectable = 1) 
                    AND is_standard = 1 )) criteria ) 
            AND cb_search_person.person_id NOT IN (SELECT
                criteria.person_id 
            FROM
                (SELECT
                    DISTINCT person_id, entry_date, concept_id 
                FROM
                    `cb_search_all_events` 
                WHERE
                    (concept_id IN(SELECT
                        DISTINCT c.concept_id 
                    FROM
                        `cb_criteria` c 
                    JOIN
                        (SELECT
                            CAST(cr.id as string) AS id       
                        FROM
                            `cb_criteria` cr       
                        WHERE
                            concept_id IN (440508)       
                            AND full_text LIKE '%_rank1]%'      ) a 
                            ON (c.path LIKE CONCAT('%.', a.id, '.%') 
                            OR c.path LIKE CONCAT('%.', a.id) 
                            OR c.path LIKE CONCAT(a.id, '.%') 
                            OR c.path = a.id) 
                    WHERE
                        is_standard = 1 
                        AND is_selectable = 1) 
                    AND is_standard = 1 )) criteria ) 
            AND cb_search_person.person_id NOT IN (SELECT
                person_id 
            FROM
                `person` p 
            WHERE
                race_concept_id IN (2100000001, 903096, 2000000008, 45882607, 1177221, 38003615, 8557) ) )", sep="")

# Formulate a Cloud Storage destination path for the data exported from BigQuery.
# NOTE: By default data exported multiple times on the same day will overwrite older copies.
#       But data exported on a different days will write to a new location so that historical
#       copies can be kept as the dataset definition is changed.
person_87211287_path <- file.path(
  Sys.getenv("WORKSPACE_BUCKET"),
  "bq_exports",
  Sys.getenv("OWNER_EMAIL"),
  strftime(lubridate::now(), "%Y%m%d"),  # Comment out this line if you want the export to always overwrite.
  "person_87211287",
  "person_87211287_*.csv")
message(str_glue('The data will be written to {person_87211287_path}. Use this path when reading ',
                 'the data into your notebooks in the future.'))

# Perform the query and export the dataset to Cloud Storage as CSV files.
# NOTE: You only need to run `bq_table_save` once. After that, you can
#       just read data from the CSVs in Cloud Storage.
bq_table_save(
  bq_dataset_query(Sys.getenv("WORKSPACE_CDR"), dataset_87211287_person_sql, billing = Sys.getenv("GOOGLE_PROJECT")),
  person_87211287_path,
  destination_format = "CSV")


# Read the data directly from Cloud Storage into memory.
# NOTE: Alternatively you can `gsutil -m cp {person_87211287_path}` to copy these files
#       to the Jupyter disk.
read_bq_export_from_workspace_bucket <- function(export_path) {
  col_types <- cols(gender = col_character(), race = col_character(), ethnicity = col_character(), sex_at_birth = col_character())
  bind_rows(
    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),
        function(csv) {
          message(str_glue('Loading {csv}.'))
          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)
          if (is.null(col_types)) {
            col_types <- spec(chunk)
          }
          chunk
        }))
}
dataset_87211287_person_df <- read_bq_export_from_workspace_bucket(person_87211287_path)

dim(dataset_87211287_person_df)

head(dataset_87211287_person_df, 5)

ckd_cohort <- dataset_87211287_person_df
```

### Diabetic

```{r}
library(tidyverse)
library(bigrquery)

# This query represents dataset "followup_cohort_data" for domain "person" and was generated for All of Us Controlled Tier Dataset v7
dataset_58833992_person_sql <- paste("
    SELECT
        person.person_id,
        p_gender_concept.concept_name as gender,
        person.birth_datetime as date_of_birth,
        p_race_concept.concept_name as race,
        p_ethnicity_concept.concept_name as ethnicity,
        p_sex_at_birth_concept.concept_name as sex_at_birth 
    FROM
        `person` person 
    LEFT JOIN
        `concept` p_gender_concept 
            ON person.gender_concept_id = p_gender_concept.concept_id 
    LEFT JOIN
        `concept` p_race_concept 
            ON person.race_concept_id = p_race_concept.concept_id 
    LEFT JOIN
        `concept` p_ethnicity_concept 
            ON person.ethnicity_concept_id = p_ethnicity_concept.concept_id 
    LEFT JOIN
        `concept` p_sex_at_birth_concept 
            ON person.sex_at_birth_concept_id = p_sex_at_birth_concept.concept_id  
    WHERE
        person.PERSON_ID IN (SELECT
            distinct person_id  
        FROM
            `cb_search_person` cb_search_person  
        WHERE
            cb_search_person.person_id IN (SELECT
                person_id 
            FROM
                `person` p 
            WHERE
                race_concept_id IN (8527, 8516, 2100000001, 8515, 903096, 2000000008, 45882607, 1177221, 38003615, 8557) ) 
            AND cb_search_person.person_id IN (SELECT
                criteria.person_id 
            FROM
                (SELECT
                    DISTINCT person_id, entry_date, concept_id 
                FROM
                    `cb_search_all_events` 
                WHERE
                    (concept_id IN(SELECT
                        DISTINCT c.concept_id 
                    FROM
                        `cb_criteria` c 
                    JOIN
                        (SELECT
                            CAST(cr.id as string) AS id       
                        FROM
                            `cb_criteria` cr       
                        WHERE
                            concept_id IN (201820)       
                            AND full_text LIKE '%_rank1]%'      ) a 
                            ON (c.path LIKE CONCAT('%.', a.id, '.%') 
                            OR c.path LIKE CONCAT('%.', a.id) 
                            OR c.path LIKE CONCAT(a.id, '.%') 
                            OR c.path = a.id) 
                    WHERE
                        is_standard = 1 
                        AND is_selectable = 1) 
                    AND is_standard = 1 )) criteria ) 
            AND cb_search_person.person_id NOT IN (SELECT
                criteria.person_id 
            FROM
                (SELECT
                    DISTINCT person_id, entry_date, concept_id 
                FROM
                    `cb_search_all_events` 
                WHERE
                    (concept_id IN(SELECT
                        DISTINCT c.concept_id 
                    FROM
                        `cb_criteria` c 
                    JOIN
                        (SELECT
                            CAST(cr.id as string) AS id       
                        FROM
                            `cb_criteria` cr       
                        WHERE
                            concept_id IN (440508)       
                            AND full_text LIKE '%_rank1]%'      ) a 
                            ON (c.path LIKE CONCAT('%.', a.id, '.%') 
                            OR c.path LIKE CONCAT('%.', a.id) 
                            OR c.path LIKE CONCAT(a.id, '.%') 
                            OR c.path = a.id) 
                    WHERE
                        is_standard = 1 
                        AND is_selectable = 1) 
                    AND is_standard = 1 )) criteria ) 
            AND cb_search_person.person_id NOT IN (SELECT
                person_id 
            FROM
                `person` p 
            WHERE
                race_concept_id IN (2100000001, 903096, 2000000008, 45882607, 1177221, 38003615, 8557) ) )", sep="")

# Formulate a Cloud Storage destination path for the data exported from BigQuery.
# NOTE: By default data exported multiple times on the same day will overwrite older copies.
#       But data exported on a different days will write to a new location so that historical
#       copies can be kept as the dataset definition is changed.
person_58833992_path <- file.path(
  Sys.getenv("WORKSPACE_BUCKET"),
  "bq_exports",
  Sys.getenv("OWNER_EMAIL"),
  strftime(lubridate::now(), "%Y%m%d"),  # Comment out this line if you want the export to always overwrite.
  "person_58833992",
  "person_58833992_*.csv")
message(str_glue('The data will be written to {person_58833992_path}. Use this path when reading ',
                 'the data into your notebooks in the future.'))

# Perform the query and export the dataset to Cloud Storage as CSV files.
# NOTE: You only need to run `bq_table_save` once. After that, you can
#       just read data from the CSVs in Cloud Storage.
bq_table_save(
  bq_dataset_query(Sys.getenv("WORKSPACE_CDR"), dataset_58833992_person_sql, billing = Sys.getenv("GOOGLE_PROJECT")),
  person_58833992_path,
  destination_format = "CSV")


# Read the data directly from Cloud Storage into memory.
# NOTE: Alternatively you can `gsutil -m cp {person_58833992_path}` to copy these files
#       to the Jupyter disk.
read_bq_export_from_workspace_bucket <- function(export_path) {
  col_types <- cols(gender = col_character(), race = col_character(), ethnicity = col_character(), sex_at_birth = col_character())
  bind_rows(
    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),
        function(csv) {
          message(str_glue('Loading {csv}.'))
          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)
          if (is.null(col_types)) {
            col_types <- spec(chunk)
          }
          chunk
        }))
}
dataset_58833992_person_df <- read_bq_export_from_workspace_bucket(person_58833992_path)

dim(dataset_58833992_person_df)

head(dataset_58833992_person_df, 5)

diabetic_cohort <- dataset_58833992_person_df
```

### Hypertensive

```{r}
library(tidyverse)
library(bigrquery)

# This query represents dataset "followup_cohort_data" for domain "person" and was generated for All of Us Controlled Tier Dataset v7
dataset_93359157_person_sql <- paste("
    SELECT
        person.person_id,
        p_gender_concept.concept_name as gender,
        person.birth_datetime as date_of_birth,
        p_race_concept.concept_name as race,
        p_ethnicity_concept.concept_name as ethnicity,
        p_sex_at_birth_concept.concept_name as sex_at_birth 
    FROM
        `person` person 
    LEFT JOIN
        `concept` p_gender_concept 
            ON person.gender_concept_id = p_gender_concept.concept_id 
    LEFT JOIN
        `concept` p_race_concept 
            ON person.race_concept_id = p_race_concept.concept_id 
    LEFT JOIN
        `concept` p_ethnicity_concept 
            ON person.ethnicity_concept_id = p_ethnicity_concept.concept_id 
    LEFT JOIN
        `concept` p_sex_at_birth_concept 
            ON person.sex_at_birth_concept_id = p_sex_at_birth_concept.concept_id  
    WHERE
        person.PERSON_ID IN (SELECT
            distinct person_id  
        FROM
            `cb_search_person` cb_search_person  
        WHERE
            cb_search_person.person_id IN (SELECT
                person_id 
            FROM
                `person` p 
            WHERE
                race_concept_id IN (8527, 8516, 2100000001, 8515, 903096, 2000000008, 45882607, 1177221, 38003615, 8557) ) 
            AND cb_search_person.person_id IN (SELECT
                criteria.person_id 
            FROM
                (SELECT
                    DISTINCT person_id, entry_date, concept_id 
                FROM
                    `cb_search_all_events` 
                WHERE
                    (concept_id IN(SELECT
                        DISTINCT c.concept_id 
                    FROM
                        `cb_criteria` c 
                    JOIN
                        (SELECT
                            CAST(cr.id as string) AS id       
                        FROM
                            `cb_criteria` cr       
                        WHERE
                            concept_id IN (316866)       
                            AND full_text LIKE '%_rank1]%'      ) a 
                            ON (c.path LIKE CONCAT('%.', a.id, '.%') 
                            OR c.path LIKE CONCAT('%.', a.id) 
                            OR c.path LIKE CONCAT(a.id, '.%') 
                            OR c.path = a.id) 
                    WHERE
                        is_standard = 1 
                        AND is_selectable = 1) 
                    AND is_standard = 1 )) criteria ) 
            AND cb_search_person.person_id NOT IN (SELECT
                criteria.person_id 
            FROM
                (SELECT
                    DISTINCT person_id, entry_date, concept_id 
                FROM
                    `cb_search_all_events` 
                WHERE
                    (concept_id IN(SELECT
                        DISTINCT c.concept_id 
                    FROM
                        `cb_criteria` c 
                    JOIN
                        (SELECT
                            CAST(cr.id as string) AS id       
                        FROM
                            `cb_criteria` cr       
                        WHERE
                            concept_id IN (440508)       
                            AND full_text LIKE '%_rank1]%'      ) a 
                            ON (c.path LIKE CONCAT('%.', a.id, '.%') 
                            OR c.path LIKE CONCAT('%.', a.id) 
                            OR c.path LIKE CONCAT(a.id, '.%') 
                            OR c.path = a.id) 
                    WHERE
                        is_standard = 1 
                        AND is_selectable = 1) 
                    AND is_standard = 1 )) criteria ) 
            AND cb_search_person.person_id NOT IN (SELECT
                person_id 
            FROM
                `person` p 
            WHERE
                race_concept_id IN (2100000001, 903096, 2000000008, 45882607, 1177221, 38003615, 8557) ) )", sep="")

# Formulate a Cloud Storage destination path for the data exported from BigQuery.
# NOTE: By default data exported multiple times on the same day will overwrite older copies.
#       But data exported on a different days will write to a new location so that historical
#       copies can be kept as the dataset definition is changed.
person_93359157_path <- file.path(
  Sys.getenv("WORKSPACE_BUCKET"),
  "bq_exports",
  Sys.getenv("OWNER_EMAIL"),
  strftime(lubridate::now(), "%Y%m%d"),  # Comment out this line if you want the export to always overwrite.
  "person_93359157",
  "person_93359157_*.csv")
message(str_glue('The data will be written to {person_93359157_path}. Use this path when reading ',
                 'the data into your notebooks in the future.'))

# Perform the query and export the dataset to Cloud Storage as CSV files.
# NOTE: You only need to run `bq_table_save` once. After that, you can
#       just read data from the CSVs in Cloud Storage.
bq_table_save(
  bq_dataset_query(Sys.getenv("WORKSPACE_CDR"), dataset_93359157_person_sql, billing = Sys.getenv("GOOGLE_PROJECT")),
  person_93359157_path,
  destination_format = "CSV")


# Read the data directly from Cloud Storage into memory.
# NOTE: Alternatively you can `gsutil -m cp {person_93359157_path}` to copy these files
#       to the Jupyter disk.
read_bq_export_from_workspace_bucket <- function(export_path) {
  col_types <- cols(gender = col_character(), race = col_character(), ethnicity = col_character(), sex_at_birth = col_character())
  bind_rows(
    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),
        function(csv) {
          message(str_glue('Loading {csv}.'))
          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)
          if (is.null(col_types)) {
            col_types <- spec(chunk)
          }
          chunk
        }))
}
dataset_93359157_person_df <- read_bq_export_from_workspace_bucket(person_93359157_path)

dim(dataset_93359157_person_df)

head(dataset_93359157_person_df, 5)

hypertensive_cohort <- dataset_93359157_person_df
```

## In feature window diabetic population

```{r}
# Load dataset
inrange_person_with_summary <- read.csv("person_with_summary_inrange_indexdate_creatnine_albumin.csv")

# Load necessary library
library(dplyr)

# Filter the diabetic population with expanded conditions
inrange_person_with_summary <- inrange_person_with_summary %>%
  filter(
    (Diabetes.mellitus > 0 & !is.na(Diabetes.mellitus)) |
    (Type.1.diabetes.mellitus > 0 & !is.na(Type.1.diabetes.mellitus)) |
    (Type.2.diabetes.mellitus > 0 & !is.na(Type.2.diabetes.mellitus)) |
    (Secondary.diabetes.mellitus > 0 & !is.na(Secondary.diabetes.mellitus)) |
    (Retinopathy.due.to.diabetes.mellitus > 0 & !is.na(Retinopathy.due.to.diabetes.mellitus)) |
    (Ketoacidosis.due.to.type.1.diabetes.mellitus > 0 & !is.na(Ketoacidosis.due.to.type.1.diabetes.mellitus)) |
    (Ketoacidosis.due.to.type.2.diabetes.mellitus > 0 & !is.na(Ketoacidosis.due.to.type.2.diabetes.mellitus)) |
    (Peripheral.circulatory.disorder.due.to.type.1.diabetes.mellitus > 0 & !is.na(Peripheral.circulatory.disorder.due.to.type.1.diabetes.mellitus)) |
    (Peripheral.circulatory.disorder.due.to.type.2.diabetes.mellitus > 0 & !is.na(Peripheral.circulatory.disorder.due.to.type.2.diabetes.mellitus)) |
    (Renal.disorder.due.to.type.1.diabetes.mellitus > 0 & !is.na(Renal.disorder.due.to.type.1.diabetes.mellitus)) |
    (Renal.disorder.due.to.type.2.diabetes.mellitus > 0 & !is.na(Renal.disorder.due.to.type.2.diabetes.mellitus)) |
    (Polyneuropathy.due.to.diabetes.mellitus > 0 & !is.na(Polyneuropathy.due.to.diabetes.mellitus))
  )




```

## In feature window hypertensive

```{r}
# Load dataset
inrange_person_with_summary <- read.csv("person_with_summary_inrange_indexdate_creatnine_albumin.csv")

# Load necessary library
library(dplyr)

# Filter the hypertensive population with count data
inrange_person_with_summary <- inrange_person_with_summary %>%
  filter(
    (Essential.hypertension > 0 & !is.na(Essential.hypertension)) |
    (Hypertensive.complication > 0 & !is.na(Hypertensive.complication)) |
    (Hypertensive.heart.AND.renal.disease > 0 & !is.na(Hypertensive.heart.AND.renal.disease)) |
    (Hypertensive.heart.and.chronic.kidney.disease > 0 & !is.na(Hypertensive.heart.and.chronic.kidney.disease))
  )


```

# MDRD Population

```{r}
# Install doParallel package if it's not installed
# install.packages("doParallel")

# Load the parallel processing libraries
library(doParallel)

# Set up parallel backend to use multiple processors
cores <- detectCores() - 1  # Detect the number of cores and use all but one
cl <- makeCluster(cores)
registerDoParallel(cl)

# Load necessary libraries
library(dplyr)
library(survival)

# Load dataset
#inrange_person_with_summary <- read.csv("person_with_summary_inrange_indexdate_creatnine_albumin.csv")

# # Define a plausible range for eGFR and Albumin/Creatinine based on clinical thresholds for eGFR ≤ 60
# plausible_eGFR_min <- 0
# plausible_eGFR_max <- 60  # Kidney disease defined as eGFR < 60 mL/min/1.73 m²
# #
# plausible_acr_min <- 0
# plausible_acr_max <- 3000
# albuminuria_cutoff <- 10

# Step 1: Select relevant columns and rename Albumin_Creatinine for consistency
person_filtered_KFRE <- inrange_person_with_summary %>%
  select(
    person_id,
    sex_at_birth,
    eGFR_MDRD,
    Albumin_Creatinine = `Albumin.Creatinine`,
    age_precise,
    race,
    disease_status,
    time_to_event
    )

# Step 2: Data preprocessing and filtering based on the combination of eGFR and ACR for low-risk group
person_filtered_KFRE <- person_filtered_KFRE %>%
  mutate(
    event = disease_status
  ) %>%

  # Log-transform Albumin/Creatinine ratio (ACR)
  mutate(logACR = log(Albumin_Creatinine)) %>%
  # Ensure complete cases (remove rows with missing data)
  filter(complete.cases(.))


# Filter out rows with non-NA time_to_event values
case_control_time_to_event_counts <- person_filtered_KFRE %>%
  filter(!is.na(time_to_event)) %>%  # Keep only rows with non-NA time_to_event
  group_by(disease_status) %>%  # Assuming "disease_status" column indicates cases and controls
  summarise(count = n())  # Count the number of rows in each group

# Print the result
print("Number of cases and controls with non-NA time_to_event values:")
print(case_control_time_to_event_counts)


# Step 3: Verify summary statistics for eGFR and Albumin/Creatinine
summary(person_filtered_KFRE$eGFR_value)
summary(person_filtered_KFRE$Albumin_Creatinine)

# Check how many records are retained after filtering
cat("Number of records after filtering:", nrow(person_filtered_KFRE), "\n")

# Perform a complete case analysis
initial_row_count <- nrow(person_filtered_KFRE)

complete_cases <- person_filtered_KFRE %>%
  filter(complete.cases(.))

final_row_count <- nrow(complete_cases)

# Check if any rows have been removed
rows_removed <- initial_row_count - final_row_count

if (rows_removed > 0) {
  cat(rows_removed, "rows have been removed during the complete case analysis.\n")
} else {
  cat("No rows have been removed during the complete case analysis.\n")
}

# Report the number of complete cases
num_complete_cases <- nrow(complete_cases)
cat("Number of complete cases:", num_complete_cases, "\n")

# Remove any rows with NA in time_to_event or disease_status
complete_cases <- complete_cases %>%
  filter(!is.na(time_to_event) & !is.na(disease_status))

# Check the event distribution
cat("Event distribution (disease_status):\n")
print(table(complete_cases$disease_status))

# Step 4: Define the formula for calculating βsum (log hazard)
calculate_beta_sum <- function(age, sex, eGFR, logACR) {
  (-0.2201 * (age / 10 - 7.036)) +
  (0.2467 * (ifelse(sex == "Male", 1, 0) - 0.5642)) -
  (0.5567 * (eGFR / 5 - 7.222)) +
  (0.4510 * (logACR - 5.137))
}

# Apply the formula to calculate βsum
complete_cases <- complete_cases %>%
  mutate(beta_sum = calculate_beta_sum(age_precise, sex_at_birth, eGFR_MDRD, logACR))

# Remove any rows with infinite or NA beta_sum
complete_cases <- complete_cases %>%
  filter(is.finite(beta_sum) & !is.na(beta_sum))

# Step 5: Calculate the risk for two and five years using the updated formulas
complete_cases <- complete_cases %>%
  mutate(
    risk_2_years = 100 * (1 - 0.975^exp(beta_sum)),  # Two-Year Risk calculation
    risk_5_years = 100 * (1 - 0.9240^exp(beta_sum))  # Five-Year Risk calculation
  )

# Save the data before filtering
before_filtering <- complete_cases

# Step 6: Apply the filtering to create consistent_data
consistent_data <- complete_cases %>%
  filter(!is.na(risk_5_years) & !is.na(risk_2_years))

# Step 7: Define and create the 2-year and 5-year outcomes based on disease_status and time_to_event
consistent_data <- consistent_data %>%
  mutate(
    outcome_2_years = ifelse(disease_status == 1 & time_to_event <= 2 * 365.25, 1, 0),
    outcome_5_years = ifelse(disease_status == 1 & time_to_event <= 5 * 365.25, 1, 0)
  )

# Find the rows that were removed by comparing before and after datasets
removed_rows <- anti_join(before_filtering, consistent_data, by = "person_id")

# Print the removed rows
if (nrow(removed_rows) > 0) {
  cat("Rows that were removed during filtering:\n")
  print(removed_rows)
} else {
  cat("No rows were removed during filtering.\n")
}

# Step 8: Verify the risk values
summary(consistent_data$risk_5_years)
summary(consistent_data$risk_2_years)

# Count the number of person_id entries with event == 1 and event == 0
event_count <- consistent_data %>%
  group_by(event) %>%
  summarise(count = n_distinct(person_id))

# Print the results
cat("Number of person_id with event == 1:", event_count$count[event_count$event == 1], "\n")
cat("Number of person_id with event == 0:", event_count$count[event_count$event == 0], "\n")

# Check the event distribution for the 2-year and 5-year outcomes
cat("\n2-Year Outcome Distribution:\n")
print(table(consistent_data$outcome_2_years))

cat("\n5-Year Outcome Distribution:\n")
print(table(consistent_data$outcome_5_years))


# Evaluation
# Load necessary libraries
library(dplyr)
library(pROC)
library(survival)
library(foreach)
library(doParallel)
library(ggplot2)

# Function to calculate metrics with bootstrapping and specificity adjustment
calculate_metrics_with_bootstrap <- function(data, risk_col, outcome_col, time_col, time_limit, n_bootstrap = 5000) {
  
  # Record the start time
  start_time <- Sys.time()
  
  # Apply censoring: keep all data but treat events beyond the time limit as censored
  data_filtered <- data %>%
    mutate(
      censored_time = pmin(!!sym(time_col), time_limit * 365),
      event_status = ifelse(!!sym(outcome_col) == 1 & !!sym(time_col) <= time_limit * 365, 1, 0)
    )
  
  # Filter out rows with missing or invalid data
  data_filtered <- data_filtered %>%
    filter(!is.na(!!sym(risk_col)) & !is.na(event_status) & !is.na(censored_time))
  
  # If no events or no controls exist in the data, return NA
  if (length(unique(data_filtered$event_status)) < 2) {
    return(list(
      AUC = NA, AUC_Lower = NA, AUC_Upper = NA, C_Statistic = NA,
      C_Stat_Lower = NA, C_Stat_Upper = NA, Sensitivity = NA,
      Specificity = NA, PPV = NA, NPV = NA, Optimal_Cutoff = NA,
      Cutoff_Method = "Not applicable", Cases = NA, Controls = NA
    ))
  }
  
  # Calculate ROC and AUC
  roc_curve <- roc(data_filtered$event_status, data_filtered[[risk_col]], levels = c(0, 1), direction = "<")
  auc_value <- as.numeric(auc(roc_curve))
  
  # Try to find the cutoff for specificity target 0.9
  cutoff_method <- "Specificity 0.9"
  optimal_cutoff <- tryCatch(
    {
      coords(roc_curve, x = 0.9, input = "specificity", ret = "threshold", transpose = TRUE)[1]
    },
    error = function(e) {
      NA
    }
  )
  
  # If the cutoff for specificity 0.9 is NA, try specificity 0.8
  if (is.na(optimal_cutoff)) {
    cutoff_method <- "Specificity 0.8"
    optimal_cutoff <- tryCatch(
      {
        coords(roc_curve, x = 0.8, input = "specificity", ret = "threshold", transpose = TRUE)[1]
      },
      error = function(e) {
        NA
      }
    )
  }
  
  # If the cutoff for specificity 0.8 is also NA, use the optimal cutoff
  if (is.na(optimal_cutoff)) {
    cutoff_method <- "Optimal Cutoff"
    optimal_cutoff <- tryCatch(
      {
        coords(roc_curve, x = "best", best.method = "closest.topleft", ret = "threshold", transpose = TRUE)[1]
      },
      error = function(e) {
        NA
      }
    )
  }
  
  # If the optimal_cutoff is not valid, return NA for metrics
  if (is.na(optimal_cutoff)) {
    return(list(
      AUC = auc_value, AUC_Lower = NA, AUC_Upper = NA, C_Statistic = NA,
      C_Stat_Lower = NA, C_Stat_Upper = NA, Sensitivity = NA,
      Specificity = NA, PPV = NA, NPV = NA, Optimal_Cutoff = NA,
      Cutoff_Method = "Failed", Cases = sum(data_filtered$event_status == 1),
      Controls = sum(data_filtered$event_status == 0)
    ))
  }

  # Calculate predictions based on the new cutoff
  pred_class <- ifelse(data_filtered[[risk_col]] >= optimal_cutoff, 1, 0)

  # Calculate confusion matrix statistics based on the new cutoff
  confusion_mat <- table(Predicted = pred_class, Actual = data_filtered$event_status)

  # If confusion matrix is not 2x2, set metrics to NA
  if (!all(c(0, 1) %in% rownames(confusion_mat)) || !all(c(0, 1) %in% colnames(confusion_mat))) {
    sensitivity <- NA
    specificity <- NA
    ppv <- NA
    npv <- NA
  } else {
    sensitivity <- confusion_mat[2, 2] / sum(confusion_mat[, 2])
    specificity <- confusion_mat[1, 1] / sum(confusion_mat[, 1])
    ppv <- confusion_mat[2, 2] / sum(confusion_mat[2, ])
    npv <- confusion_mat[1, 1] / sum(confusion_mat[1, ])
  }

  # Bootstrapping AUC and C-statistic for confidence intervals
  auc_values <- numeric(n_bootstrap)
  c_stat_values <- numeric(n_bootstrap)

  bootstrapped_results <- foreach(i = 1:n_bootstrap, .combine = rbind, .packages = c("pROC", "survival")) %dopar% {
    bootstrap_sample <- data_filtered[sample(1:nrow(data_filtered), replace = TRUE), ]

    if (length(unique(bootstrap_sample$event_status)) < 2) {
      return(c(NA, NA))
    }

    roc_curve <- roc(bootstrap_sample$event_status, bootstrap_sample[[risk_col]])
    auc_val <- as.numeric(auc(roc_curve))

    cox_model_boot <- coxph(Surv(bootstrap_sample$censored_time, bootstrap_sample$event_status) ~ bootstrap_sample[[risk_col]], data = bootstrap_sample)
    c_stat_val <- as.numeric(summary(cox_model_boot)$concordance[1])

    return(c(auc_val, c_stat_val))
  }

  auc_values <- bootstrapped_results[, 1]
  c_stat_values <- bootstrapped_results[, 2]

  # Calculate 95% confidence intervals
  auc_ci <- quantile(auc_values[!is.na(auc_values)], probs = c(0.025, 0.975), na.rm = TRUE)
  c_stat_ci <- quantile(c_stat_values[!is.na(c_stat_values)], probs = c(0.025, 0.975), na.rm = TRUE)

  # Calculate Harrell's C-statistic for the full dataset
  cox_model <- coxph(Surv(censored_time, event_status) ~ data_filtered[[risk_col]], data = data_filtered)
  c_stat <- as.numeric(summary(cox_model)$concordance[1])

  # Record the end time
  end_time <- Sys.time()
  total_runtime <- end_time - start_time

  message("Number of bootstrap iterations run:", n_bootstrap)
  message("Total runtime for the bootstrapping process:", total_runtime, "seconds")

  # Return the metrics
  return(list(
    AUC = auc_value, AUC_Lower = auc_ci[1], AUC_Upper = auc_ci[2], 
    C_Statistic = c_stat, C_Stat_Lower = c_stat_ci[1], C_Stat_Upper = c_stat_ci[2],
    Sensitivity = sensitivity, Specificity = specificity, PPV = ppv,
    NPV = npv, Optimal_Cutoff = optimal_cutoff, Cutoff_Method = cutoff_method,
    Cases = sum(data_filtered$event_status == 1), Controls = sum(data_filtered$event_status == 0)
  ))
}

# Function to summarize metrics for each racial group
summarize_risk_metrics <- function(data, race_label, time_limit, n_bootstrap = 5000) {
  metrics <- calculate_metrics_with_bootstrap(data, paste0("risk_", time_limit, "_years"), paste0("outcome_", time_limit, "_years"), "time_to_event", time_limit, n_bootstrap)
  
  return(data.frame(
    Race = race_label,
    Time_Horizon = paste0(time_limit, " Years"),
    AUC = metrics$AUC,
    AUC_Lower_CI = metrics$AUC_Lower,
    AUC_Upper_CI = metrics$AUC_Upper,
    Harrell_C_Statistic = metrics$C_Statistic,
    C_Stat_Lower_CI = metrics$C_Stat_Lower,
    C_Stat_Upper_CI = metrics$C_Stat_Upper,
    Sensitivity = metrics$Sensitivity,
    Specificity = metrics$Specificity,
    PPV = metrics$PPV,
    NPV = metrics$NPV,
    Optimal_Cutoff = metrics$Optimal_Cutoff,
    Cutoff_Method = metrics$Cutoff_Method,
    Cases = metrics$Cases,
    Controls = metrics$Controls
  ))
}

# Evaluate metrics for each racial group and time horizon
results_all_2yr <- summarize_risk_metrics(consistent_data, "All Races", 2, n_bootstrap = 5000)
results_all_5yr <- summarize_risk_metrics(consistent_data, "All Races", 5, n_bootstrap = 5000)
results_black_2yr <- summarize_risk_metrics(consistent_data %>% filter(race == "Black or African American"), "Black or African American", 2, n_bootstrap = 5000)
results_black_5yr <- summarize_risk_metrics(consistent_data %>% filter(race == "Black or African American"), "Black or African American", 5, n_bootstrap = 5000)
results_white_2yr <- summarize_risk_metrics(consistent_data %>% filter(race == "White"), "White", 2, n_bootstrap = 5000)
results_white_5yr <- summarize_risk_metrics(consistent_data %>% filter(race == "White"), "White", 5, n_bootstrap = 5000)
results_asian_2yr <- summarize_risk_metrics(consistent_data %>% filter(race == "Asian"), "Asian", 2, n_bootstrap = 5000)
results_asian_5yr <- summarize_risk_metrics(consistent_data %>% filter(race == "Asian"), "Asian", 5, n_bootstrap = 5000)

# Combine results including "All Races" for the final table
final_results <- bind_rows(
  results_all_2yr, results_all_5yr,
  results_black_2yr, results_black_5yr,
  results_white_2yr, results_white_5yr,
  results_asian_2yr, results_asian_5yr
)

# Exclude "All Races" for the forest plot
final_results_filtered_MDRD <- final_results %>%
  filter(Race != "All Races")

# Display the final results table including "All Races"
print(final_results)

# Function to plot forest plot for metrics
plot_forest <- function(results_df, metric, lower_ci, upper_ci, title, reference_line) {
  ggplot(results_df, aes_string(x = metric, y = "Race")) +
    geom_point(size = 3, shape = 16, color = "blue") +  # Metric point estimates
    geom_errorbarh(aes_string(xmin = lower_ci, xmax = upper_ci), height = 0.2, color = "black") +  # 95% CI
    geom_vline(xintercept = reference_line, linetype = "dashed", color = "red") +  # Reference line
    facet_wrap(~ Time_Horizon, scales = "free_y") +  # Separate panels for each time horizon
    labs(
      title = title,
      x = metric,
      y = "Race"
    ) +
    theme_minimal() +
    theme(
      axis.text.y = element_text(size = 14, face = "bold"),
      axis.text.x = element_text(size = 14),
      strip.text = element_text(size = 23, face = "bold"),
      panel.spacing = unit(1, "lines"),
      plot.title = element_text(size = 18, face = "bold", hjust = 0.5)
    )
}

# Plot the forest plots for AUC and Harrell's C-statistic using the filtered data
forest_plot_auc <- plot_forest(final_results_filtered_MDRD, "AUC", "AUC_Lower_CI", "AUC_Upper_CI", "", reference_line = 0.5)
forest_plot_cstat <- plot_forest(final_results_filtered_MDRD, "Harrell_C_Statistic", "C_Stat_Lower_CI", "C_Stat_Upper_CI", "", reference_line = 0.5)

# Display the forest plots
print(forest_plot_auc)
print(forest_plot_cstat)

# Save the AUC forest plot
ggsave("MDRD_VHR_forest_plot_auc_filtered.png", plot = forest_plot_auc, width = 10, height = 8, dpi = 300)

# Save the Harrell's C-statistic forest plot
ggsave("MDRD_VHR_forest_plot_cstat_filtered.png", plot = forest_plot_cstat, width = 10, height = 8, dpi = 300)

# Stop the parallel backend after use
stopCluster(cl)
```

# CKD-EPI 2009 Population

```{r}
# Install doParallel package if it's not installed
# install.packages("doParallel")

# Load the parallel processing libraries
library(doParallel)

# Set up parallel backend to use multiple processors
cores <- detectCores() - 1  # Detect the number of cores and use all but one
cl <- makeCluster(cores)
registerDoParallel(cl)

# Load necessary libraries
library(dplyr)
library(survival)

# # Load dataset
# inrange_person_with_summary <- read.csv("person_with_summary_inrange_indexdate_creatnine_albumin.csv")

# # Define a plausible range for eGFR and Albumin/Creatinine based on clinical thresholds for eGFR ≤ 60
# plausible_eGFR_min <- 0
# plausible_eGFR_max <- 60
# plausible_acr_min <- 0
# plausible_acr_max <- 3000
# albuminuria_cutoff <- 10

# Step 1: Select relevant columns and rename Albumin_Creatinine for consistency
person_filtered_KFRE <- inrange_person_with_summary %>%
  select(
    person_id,
    sex_at_birth,
    eGFR_ckd_epi_2009,
    Albumin_Creatinine = `Albumin.Creatinine`,
    age_precise,
    race,
    disease_status,
    time_to_event
  )


# Step 2: Data preprocessing and filtering based on the combination of eGFR and ACR for low-risk group
person_filtered_KFRE <- person_filtered_KFRE %>%
  mutate(
    event = disease_status
  ) %>%

  # Log-transform Albumin/Creatinine ratio (ACR)
  mutate(logACR = log(Albumin_Creatinine)) %>%
  # Ensure complete cases (remove rows with missing data)
  filter(complete.cases(.))


# Filter out rows with non-NA time_to_event values
case_control_time_to_event_counts <- person_filtered_KFRE %>%
  filter(!is.na(time_to_event)) %>%  # Keep only rows with non-NA time_to_event
  group_by(disease_status) %>%  # Assuming "disease_status" column indicates cases and controls
  summarise(count = n())  # Count the number of rows in each group

# Print the result
print("Number of cases and controls with non-NA time_to_event values:")
print(case_control_time_to_event_counts)


# Step 3: Verify summary statistics for eGFR and Albumin/Creatinine
summary(person_filtered_KFRE$eGFR_value)
summary(person_filtered_KFRE$Albumin_Creatinine)

# Check how many records are retained after filtering
cat("Number of records after filtering:", nrow(person_filtered_KFRE), "\n")

# Perform a complete case analysis
initial_row_count <- nrow(person_filtered_KFRE)

complete_cases <- person_filtered_KFRE %>%
  filter(complete.cases(.))

final_row_count <- nrow(complete_cases)

# Check if any rows have been removed
rows_removed <- initial_row_count - final_row_count

if (rows_removed > 0) {
  cat(rows_removed, "rows have been removed during the complete case analysis.\n")
} else {
  cat("No rows have been removed during the complete case analysis.\n")
}

# Report the number of complete cases
num_complete_cases <- nrow(complete_cases)
cat("Number of complete cases:", num_complete_cases, "\n")

# Remove any rows with NA in time_to_event or disease_status
complete_cases <- complete_cases %>%
  filter(!is.na(time_to_event) & !is.na(disease_status))

# Check the event distribution
cat("Event distribution (disease_status):\n")
print(table(complete_cases$disease_status))

# Step 4: Define the formula for calculating βsum (log hazard)
calculate_beta_sum <- function(age, sex, eGFR, logACR) {
  (-0.2201 * (age / 10 - 7.036)) +
  (0.2467 * (ifelse(sex == "Male", 1, 0) - 0.5642)) -
  (0.5567 * (eGFR / 5 - 7.222)) +
  (0.4510 * (logACR - 5.137))
}

# Apply the formula to calculate βsum
complete_cases <- complete_cases %>%
  mutate(beta_sum = calculate_beta_sum(age_precise, sex_at_birth, eGFR_ckd_epi_2009, logACR))

# Remove any rows with infinite or NA beta_sum
complete_cases <- complete_cases %>%
  filter(is.finite(beta_sum) & !is.na(beta_sum))

# Step 5: Calculate the risk for two and five years using the updated formulas
complete_cases <- complete_cases %>%
  mutate(
    risk_2_years = 100 * (1 - 0.975^exp(beta_sum)),  # Two-Year Risk calculation
    risk_5_years = 100 * (1 - 0.9240^exp(beta_sum))  # Five-Year Risk calculation
  )

# Save the data before filtering
before_filtering <- complete_cases

# Step 6: Apply the filtering to create consistent_data
consistent_data <- complete_cases %>%
  filter(!is.na(risk_5_years) & !is.na(risk_2_years))

# Step 7: Define and create the 2-year and 5-year outcomes based on disease_status and time_to_event
consistent_data <- consistent_data %>%
  mutate(
    outcome_2_years = ifelse(disease_status == 1 & time_to_event <= 2 * 365.25, 1, 0),
    outcome_5_years = ifelse(disease_status == 1 & time_to_event <= 5 * 365.25, 1, 0)
  )

# Find the rows that were removed by comparing before and after datasets
removed_rows <- anti_join(before_filtering, consistent_data, by = "person_id")

# Print the removed rows
if (nrow(removed_rows) > 0) {
  cat("Rows that were removed during filtering:\n")
  print(removed_rows)
} else {
  cat("No rows were removed during filtering.\n")
}

# Step 8: Verify the risk values
summary(consistent_data$risk_5_years)
summary(consistent_data$risk_2_years)

# Count the number of person_id entries with event == 1 and event == 0
event_count <- consistent_data %>%
  group_by(event) %>%
  summarise(count = n_distinct(person_id))

# Print the results
cat("Number of person_id with event == 1:", event_count$count[event_count$event == 1], "\n")
cat("Number of person_id with event == 0:", event_count$count[event_count$event == 0], "\n")

# Check the event distribution for the 2-year and 5-year outcomes
cat("\n2-Year Outcome Distribution:\n")
print(table(consistent_data$outcome_2_years))

cat("\n5-Year Outcome Distribution:\n")
print(table(consistent_data$outcome_5_years))

# Evaluation
# Load necessary libraries
library(dplyr)
library(pROC)
library(survival)
library(foreach)
library(doParallel)
library(ggplot2)

# Function to calculate metrics with bootstrapping and specificity adjustment
calculate_metrics_with_bootstrap <- function(data, risk_col, outcome_col, time_col, time_limit, n_bootstrap = 5000) {
  
  # Record the start time
  start_time <- Sys.time()
  
  # Apply censoring: keep all data but treat events beyond the time limit as censored
  data_filtered <- data %>%
    mutate(
      censored_time = pmin(!!sym(time_col), time_limit * 365),
      event_status = ifelse(!!sym(outcome_col) == 1 & !!sym(time_col) <= time_limit * 365, 1, 0)
    )
  
  # Filter out rows with missing or invalid data
  data_filtered <- data_filtered %>%
    filter(!is.na(!!sym(risk_col)) & !is.na(event_status) & !is.na(censored_time))
  
  # If no events or no controls exist in the data, return NA
  if (length(unique(data_filtered$event_status)) < 2) {
    return(list(
      AUC = NA, AUC_Lower = NA, AUC_Upper = NA, C_Statistic = NA,
      C_Stat_Lower = NA, C_Stat_Upper = NA, Sensitivity = NA,
      Specificity = NA, PPV = NA, NPV = NA, Optimal_Cutoff = NA,
      Cutoff_Method = "Not applicable", Cases = NA, Controls = NA
    ))
  }
  
  # Calculate ROC and AUC
  roc_curve <- roc(data_filtered$event_status, data_filtered[[risk_col]], levels = c(0, 1), direction = "<")
  auc_value <- as.numeric(auc(roc_curve))
  
  # Try to find the cutoff for specificity target 0.9
  cutoff_method <- "Specificity 0.9"
  optimal_cutoff <- tryCatch(
    {
      coords(roc_curve, x = 0.9, input = "specificity", ret = "threshold", transpose = TRUE)[1]
    },
    error = function(e) {
      NA
    }
  )
  
  # If the cutoff for specificity 0.9 is NA, try specificity 0.8
  if (is.na(optimal_cutoff)) {
    cutoff_method <- "Specificity 0.8"
    optimal_cutoff <- tryCatch(
      {
        coords(roc_curve, x = 0.8, input = "specificity", ret = "threshold", transpose = TRUE)[1]
      },
      error = function(e) {
        NA
      }
    )
  }
  
  # If the cutoff for specificity 0.8 is also NA, use the optimal cutoff
  if (is.na(optimal_cutoff)) {
    cutoff_method <- "Optimal Cutoff"
    optimal_cutoff <- tryCatch(
      {
        coords(roc_curve, x = "best", best.method = "closest.topleft", ret = "threshold", transpose = TRUE)[1]
      },
      error = function(e) {
        NA
      }
    )
  }
  
  # If the optimal_cutoff is not valid, return NA for metrics
  if (is.na(optimal_cutoff)) {
    return(list(
      AUC = auc_value, AUC_Lower = NA, AUC_Upper = NA, C_Statistic = NA,
      C_Stat_Lower = NA, C_Stat_Upper = NA, Sensitivity = NA,
      Specificity = NA, PPV = NA, NPV = NA, Optimal_Cutoff = NA,
      Cutoff_Method = "Failed", Cases = sum(data_filtered$event_status == 1),
      Controls = sum(data_filtered$event_status == 0)
    ))
  }

  # Calculate predictions based on the new cutoff
  pred_class <- ifelse(data_filtered[[risk_col]] >= optimal_cutoff, 1, 0)

  # Calculate confusion matrix statistics based on the new cutoff
  confusion_mat <- table(Predicted = pred_class, Actual = data_filtered$event_status)

  # If confusion matrix is not 2x2, set metrics to NA
  if (!all(c(0, 1) %in% rownames(confusion_mat)) || !all(c(0, 1) %in% colnames(confusion_mat))) {
    sensitivity <- NA
    specificity <- NA
    ppv <- NA
    npv <- NA
  } else {
    sensitivity <- confusion_mat[2, 2] / sum(confusion_mat[, 2])
    specificity <- confusion_mat[1, 1] / sum(confusion_mat[, 1])
    ppv <- confusion_mat[2, 2] / sum(confusion_mat[2, ])
    npv <- confusion_mat[1, 1] / sum(confusion_mat[1, ])
  }

  # Bootstrapping AUC and C-statistic for confidence intervals
  auc_values <- numeric(n_bootstrap)
  c_stat_values <- numeric(n_bootstrap)

  bootstrapped_results <- foreach(i = 1:n_bootstrap, .combine = rbind, .packages = c("pROC", "survival")) %dopar% {
    bootstrap_sample <- data_filtered[sample(1:nrow(data_filtered), replace = TRUE), ]

    if (length(unique(bootstrap_sample$event_status)) < 2) {
      return(c(NA, NA))
    }

    roc_curve <- roc(bootstrap_sample$event_status, bootstrap_sample[[risk_col]])
    auc_val <- as.numeric(auc(roc_curve))

    cox_model_boot <- coxph(Surv(bootstrap_sample$censored_time, bootstrap_sample$event_status) ~ bootstrap_sample[[risk_col]], data = bootstrap_sample)
    c_stat_val <- as.numeric(summary(cox_model_boot)$concordance[1])

    return(c(auc_val, c_stat_val))
  }

  auc_values <- bootstrapped_results[, 1]
  c_stat_values <- bootstrapped_results[, 2]

  # Calculate 95% confidence intervals
  auc_ci <- quantile(auc_values[!is.na(auc_values)], probs = c(0.025, 0.975), na.rm = TRUE)
  c_stat_ci <- quantile(c_stat_values[!is.na(c_stat_values)], probs = c(0.025, 0.975), na.rm = TRUE)

  # Calculate Harrell's C-statistic for the full dataset
  cox_model <- coxph(Surv(censored_time, event_status) ~ data_filtered[[risk_col]], data = data_filtered)
  c_stat <- as.numeric(summary(cox_model)$concordance[1])

  # Record the end time
  end_time <- Sys.time()
  total_runtime <- end_time - start_time

  message("Number of bootstrap iterations run:", n_bootstrap)
  message("Total runtime for the bootstrapping process:", total_runtime, "seconds")

  # Return the metrics
  return(list(
    AUC = auc_value, AUC_Lower = auc_ci[1], AUC_Upper = auc_ci[2], 
    C_Statistic = c_stat, C_Stat_Lower = c_stat_ci[1], C_Stat_Upper = c_stat_ci[2],
    Sensitivity = sensitivity, Specificity = specificity, PPV = ppv,
    NPV = npv, Optimal_Cutoff = optimal_cutoff, Cutoff_Method = cutoff_method,
    Cases = sum(data_filtered$event_status == 1), Controls = sum(data_filtered$event_status == 0)
  ))
}

# Function to summarize metrics for each racial group
summarize_risk_metrics <- function(data, race_label, time_limit, n_bootstrap = 5000) {
  metrics <- calculate_metrics_with_bootstrap(data, paste0("risk_", time_limit, "_years"), paste0("outcome_", time_limit, "_years"), "time_to_event", time_limit, n_bootstrap)
  
  return(data.frame(
    Race = race_label,
    Time_Horizon = paste0(time_limit, " Years"),
    AUC = metrics$AUC,
    AUC_Lower_CI = metrics$AUC_Lower,
    AUC_Upper_CI = metrics$AUC_Upper,
    Harrell_C_Statistic = metrics$C_Statistic,
    C_Stat_Lower_CI = metrics$C_Stat_Lower,
    C_Stat_Upper_CI = metrics$C_Stat_Upper,
    Sensitivity = metrics$Sensitivity,
    Specificity = metrics$Specificity,
    PPV = metrics$PPV,
    NPV = metrics$NPV,
    Optimal_Cutoff = metrics$Optimal_Cutoff,
    Cutoff_Method = metrics$Cutoff_Method,
    Cases = metrics$Cases,
    Controls = metrics$Controls
  ))
}

# Evaluate metrics for each racial group and time horizon
results_all_2yr <- summarize_risk_metrics(consistent_data, "All Races", 2, n_bootstrap = 5000)
results_all_5yr <- summarize_risk_metrics(consistent_data, "All Races", 5, n_bootstrap = 5000)
results_black_2yr <- summarize_risk_metrics(consistent_data %>% filter(race == "Black or African American"), "Black or African American", 2, n_bootstrap = 5000)
results_black_5yr <- summarize_risk_metrics(consistent_data %>% filter(race == "Black or African American"), "Black or African American", 5, n_bootstrap = 5000)
results_white_2yr <- summarize_risk_metrics(consistent_data %>% filter(race == "White"), "White", 2, n_bootstrap = 5000)
results_white_5yr <- summarize_risk_metrics(consistent_data %>% filter(race == "White"), "White", 5, n_bootstrap = 5000)
results_asian_2yr <- summarize_risk_metrics(consistent_data %>% filter(race == "Asian"), "Asian", 2, n_bootstrap = 5000)
results_asian_5yr <- summarize_risk_metrics(consistent_data %>% filter(race == "Asian"), "Asian", 5, n_bootstrap = 5000)

# Combine results including "All Races" for the final table
final_results <- bind_rows(
  results_all_2yr, results_all_5yr,
  results_black_2yr, results_black_5yr,
  results_white_2yr, results_white_5yr,
  results_asian_2yr, results_asian_5yr
)

# Exclude "All Races" for the forest plot
final_results_filtered_CKD2009 <- final_results %>%
  filter(Race != "All Races")

# Display the final results table including "All Races"
print(final_results)

# Function to plot forest plot for metrics
plot_forest <- function(results_df, metric, lower_ci, upper_ci, title, reference_line) {
  ggplot(results_df, aes_string(x = metric, y = "Race")) +
    geom_point(size = 3, shape = 16, color = "blue") +  # Metric point estimates
    geom_errorbarh(aes_string(xmin = lower_ci, xmax = upper_ci), height = 0.2, color = "black") +  # 95% CI
    geom_vline(xintercept = reference_line, linetype = "dashed", color = "red") +  # Reference line
    facet_wrap(~ Time_Horizon, scales = "free_y") +  # Separate panels for each time horizon
    labs(
      title = title,
      x = metric,
      y = "Race"
    ) +
    theme_minimal() +
    theme(
      axis.text.y = element_text(size = 14, face = "bold"),
      axis.text.x = element_text(size = 14),
      strip.text = element_text(size = 23, face = "bold"),
      panel.spacing = unit(1, "lines"),
      plot.title = element_text(size = 18, face = "bold", hjust = 0.5)
    )
}

# Plot the forest plots for AUC and Harrell's C-statistic using the filtered data
forest_plot_auc <- plot_forest(final_results_filtered_CKD2009, "AUC", "AUC_Lower_CI", "AUC_Upper_CI", "", reference_line = 0.5)
forest_plot_cstat <- plot_forest(final_results_filtered_CKD2009, "Harrell_C_Statistic", "C_Stat_Lower_CI", "C_Stat_Upper_CI", "", reference_line = 0.5)

# Display the forest plots
print(forest_plot_auc)
print(forest_plot_cstat)

# Save the AUC forest plot
ggsave("CKD2009_VHR_forest_plot_auc_filtered.png", plot = forest_plot_auc, width = 10, height = 8, dpi = 300)

# Save the Harrell's C-statistic forest plot
ggsave("CKD2009_VHR_forest_plot_cstat_filtered.png", plot = forest_plot_cstat, width = 10, height = 8, dpi = 300)

# Stop the parallel backend after use
stopCluster(cl)
```

### 

# CKD-EPI 2021 Population

```{r}
# Install doParallel package if it's not installed
# install.packages("doParallel")

# Load the parallel processing libraries
library(doParallel)

# Set up parallel backend to use multiple processors
cores <- detectCores() - 1  # Detect the number of cores and use all but one
cl <- makeCluster(cores)
registerDoParallel(cl)

# Load necessary libraries
library(dplyr)
library(survival)

# # Load dataset
# inrange_person_with_summary <- read.csv("person_with_summary_inrange_indexdate_creatnine_albumin.csv")

# # Define a plausible range for eGFR and Albumin/Creatinine based on clinical thresholds for eGFR ≤ 60
# plausible_eGFR_min <- 0
# plausible_eGFR_max <- 60  # Kidney disease defined as eGFR < 60 mL/min/1.73 m²
# 
# plausible_acr_min <- 0
# plausible_acr_max <- 3000
# albuminuria_cutoff <- 10

# Step 1: Select relevant columns and rename Albumin_Creatinine for consistency
person_filtered_KFRE <- inrange_person_with_summary %>%
  select(
    person_id,
    sex_at_birth,
    eGFR_ckd_epi_2021,
    Albumin_Creatinine = `Albumin.Creatinine`,
    age_precise,
    race,
    disease_status,
    time_to_event
  )



# Step 2: Data preprocessing and filtering based on the combination of eGFR and ACR for low-risk group
person_filtered_KFRE <- person_filtered_KFRE %>%
  mutate(
    event = disease_status
  ) %>%

  # Log-transform Albumin/Creatinine ratio (ACR)
  mutate(logACR = log(Albumin_Creatinine)) %>%
  # Ensure complete cases (remove rows with missing data)
  filter(complete.cases(.))

# Filter out rows with non-NA time_to_event values
case_control_time_to_event_counts <- person_filtered_KFRE %>%
  filter(!is.na(time_to_event)) %>%  # Keep only rows with non-NA time_to_event
  group_by(disease_status) %>%  # Assuming "disease_status" column indicates cases and controls
  summarise(count = n())  # Count the number of rows in each group

# Print the result
print("Number of cases and controls with non-NA time_to_event values:")
print(case_control_time_to_event_counts)


# Step 3: Verify summary statistics for eGFR and Albumin/Creatinine
summary(person_filtered_KFRE$eGFR_value)
summary(person_filtered_KFRE$Albumin_Creatinine)

# Check how many records are retained after filtering
cat("Number of records after filtering:", nrow(person_filtered_KFRE), "\n")

# Perform a complete case analysis
initial_row_count <- nrow(person_filtered_KFRE)

complete_cases <- person_filtered_KFRE %>%
  filter(complete.cases(.))

final_row_count <- nrow(complete_cases)

# Check if any rows have been removed
rows_removed <- initial_row_count - final_row_count

if (rows_removed > 0) {
  cat(rows_removed, "rows have been removed during the complete case analysis.\n")
} else {
  cat("No rows have been removed during the complete case analysis.\n")
}

# Report the number of complete cases
num_complete_cases <- nrow(complete_cases)
cat("Number of complete cases:", num_complete_cases, "\n")

# Remove any rows with NA in time_to_event or disease_status
complete_cases <- complete_cases %>%
  filter(!is.na(time_to_event) & !is.na(disease_status))

# Check the event distribution
cat("Event distribution (disease_status):\n")
print(table(complete_cases$disease_status))

# Step 4: Define the formula for calculating βsum (log hazard)
calculate_beta_sum <- function(age, sex, eGFR, logACR) {
  (-0.2201 * (age / 10 - 7.036)) +
  (0.2467 * (ifelse(sex == "Male", 1, 0) - 0.5642)) -
  (0.5567 * (eGFR / 5 - 7.222)) +
  (0.4510 * (logACR - 5.137))
}

# Apply the formula to calculate βsum
complete_cases <- complete_cases %>%
  mutate(beta_sum = calculate_beta_sum(age_precise, sex_at_birth, eGFR_ckd_epi_2021, logACR))

# Remove any rows with infinite or NA beta_sum
complete_cases <- complete_cases %>%
  filter(is.finite(beta_sum) & !is.na(beta_sum))

# Step 5: Calculate the risk for two and five years using the updated formulas
complete_cases <- complete_cases %>%
  mutate(
    risk_2_years = 100 * (1 - 0.975^exp(beta_sum)),  # Two-Year Risk calculation
    risk_5_years = 100 * (1 - 0.9240^exp(beta_sum))  # Five-Year Risk calculation
  )

# Save the data before filtering
before_filtering <- complete_cases

# Step 6: Apply the filtering to create consistent_data
consistent_data <- complete_cases %>%
  filter(!is.na(risk_5_years) & !is.na(risk_2_years))

# Step 7: Define and create the 2-year and 5-year outcomes based on disease_status and time_to_event
consistent_data <- consistent_data %>%
  mutate(
    outcome_2_years = ifelse(disease_status == 1 & time_to_event <= 2 * 365.25, 1, 0),
    outcome_5_years = ifelse(disease_status == 1 & time_to_event <= 5 * 365.25, 1, 0)
  )

# Find the rows that were removed by comparing before and after datasets
removed_rows <- anti_join(before_filtering, consistent_data, by = "person_id")

# Print the removed rows
if (nrow(removed_rows) > 0) {
  cat("Rows that were removed during filtering:\n")
  print(removed_rows)
} else {
  cat("No rows were removed during filtering.\n")
}

# Step 8: Verify the risk values
summary(consistent_data$risk_5_years)
summary(consistent_data$risk_2_years)

# Count the number of person_id entries with event == 1 and event == 0
event_count <- consistent_data %>%
  group_by(event) %>%
  summarise(count = n_distinct(person_id))

# Print the results
cat("Number of person_id with event == 1:", event_count$count[event_count$event == 1], "\n")
cat("Number of person_id with event == 0:", event_count$count[event_count$event == 0], "\n")

# Check the event distribution for the 2-year and 5-year outcomes
cat("\n2-Year Outcome Distribution:\n")
print(table(consistent_data$outcome_2_years))

cat("\n5-Year Outcome Distribution:\n")
print(table(consistent_data$outcome_5_years))


# Evaluation
# Load necessary libraries
library(dplyr)
library(pROC)
library(survival)
library(foreach)
library(doParallel)
library(ggplot2)

# Function to calculate metrics with bootstrapping and specificity adjustment
calculate_metrics_with_bootstrap <- function(data, risk_col, outcome_col, time_col, time_limit, n_bootstrap = 5000) {
  
  # Record the start time
  start_time <- Sys.time()
  
  # Apply censoring: keep all data but treat events beyond the time limit as censored
  data_filtered <- data %>%
    mutate(
      censored_time = pmin(!!sym(time_col), time_limit * 365),
      event_status = ifelse(!!sym(outcome_col) == 1 & !!sym(time_col) <= time_limit * 365, 1, 0)
    )
  
  # Filter out rows with missing or invalid data
  data_filtered <- data_filtered %>%
    filter(!is.na(!!sym(risk_col)) & !is.na(event_status) & !is.na(censored_time))
  
  # If no events or no controls exist in the data, return NA
  if (length(unique(data_filtered$event_status)) < 2) {
    return(list(
      AUC = NA, AUC_Lower = NA, AUC_Upper = NA, C_Statistic = NA,
      C_Stat_Lower = NA, C_Stat_Upper = NA, Sensitivity = NA,
      Specificity = NA, PPV = NA, NPV = NA, Optimal_Cutoff = NA,
      Cutoff_Method = "Not applicable", Cases = NA, Controls = NA
    ))
  }
  
  # Calculate ROC and AUC
  roc_curve <- roc(data_filtered$event_status, data_filtered[[risk_col]], levels = c(0, 1), direction = "<")
  auc_value <- as.numeric(auc(roc_curve))
  
  # Try to find the cutoff for specificity target 0.9
  cutoff_method <- "Specificity 0.9"
  optimal_cutoff <- tryCatch(
    {
      coords(roc_curve, x = 0.9, input = "specificity", ret = "threshold", transpose = TRUE)[1]
    },
    error = function(e) {
      NA
    }
  )
  
  # If the cutoff for specificity 0.9 is NA, try specificity 0.8
  if (is.na(optimal_cutoff)) {
    cutoff_method <- "Specificity 0.8"
    optimal_cutoff <- tryCatch(
      {
        coords(roc_curve, x = 0.8, input = "specificity", ret = "threshold", transpose = TRUE)[1]
      },
      error = function(e) {
        NA
      }
    )
  }
  
  # If the cutoff for specificity 0.8 is also NA, use the optimal cutoff
  if (is.na(optimal_cutoff)) {
    cutoff_method <- "Optimal Cutoff"
    optimal_cutoff <- tryCatch(
      {
        coords(roc_curve, x = "best", best.method = "closest.topleft", ret = "threshold", transpose = TRUE)[1]
      },
      error = function(e) {
        NA
      }
    )
  }
  
  # If the optimal_cutoff is not valid, return NA for metrics
  if (is.na(optimal_cutoff)) {
    return(list(
      AUC = auc_value, AUC_Lower = NA, AUC_Upper = NA, C_Statistic = NA,
      C_Stat_Lower = NA, C_Stat_Upper = NA, Sensitivity = NA,
      Specificity = NA, PPV = NA, NPV = NA, Optimal_Cutoff = NA,
      Cutoff_Method = "Failed", Cases = sum(data_filtered$event_status == 1),
      Controls = sum(data_filtered$event_status == 0)
    ))
  }

  # Calculate predictions based on the new cutoff
  pred_class <- ifelse(data_filtered[[risk_col]] >= optimal_cutoff, 1, 0)

  # Calculate confusion matrix statistics based on the new cutoff
  confusion_mat <- table(Predicted = pred_class, Actual = data_filtered$event_status)

  # If confusion matrix is not 2x2, set metrics to NA
  if (!all(c(0, 1) %in% rownames(confusion_mat)) || !all(c(0, 1) %in% colnames(confusion_mat))) {
    sensitivity <- NA
    specificity <- NA
    ppv <- NA
    npv <- NA
  } else {
    sensitivity <- confusion_mat[2, 2] / sum(confusion_mat[, 2])
    specificity <- confusion_mat[1, 1] / sum(confusion_mat[, 1])
    ppv <- confusion_mat[2, 2] / sum(confusion_mat[2, ])
    npv <- confusion_mat[1, 1] / sum(confusion_mat[1, ])
  }

  # Bootstrapping AUC and C-statistic for confidence intervals
  auc_values <- numeric(n_bootstrap)
  c_stat_values <- numeric(n_bootstrap)

  bootstrapped_results <- foreach(i = 1:n_bootstrap, .combine = rbind, .packages = c("pROC", "survival")) %dopar% {
    bootstrap_sample <- data_filtered[sample(1:nrow(data_filtered), replace = TRUE), ]

    if (length(unique(bootstrap_sample$event_status)) < 2) {
      return(c(NA, NA))
    }

    roc_curve <- roc(bootstrap_sample$event_status, bootstrap_sample[[risk_col]])
    auc_val <- as.numeric(auc(roc_curve))

    cox_model_boot <- coxph(Surv(bootstrap_sample$censored_time, bootstrap_sample$event_status) ~ bootstrap_sample[[risk_col]], data = bootstrap_sample)
    c_stat_val <- as.numeric(summary(cox_model_boot)$concordance[1])

    return(c(auc_val, c_stat_val))
  }

  auc_values <- bootstrapped_results[, 1]
  c_stat_values <- bootstrapped_results[, 2]

  # Calculate 95% confidence intervals
  auc_ci <- quantile(auc_values[!is.na(auc_values)], probs = c(0.025, 0.975), na.rm = TRUE)
  c_stat_ci <- quantile(c_stat_values[!is.na(c_stat_values)], probs = c(0.025, 0.975), na.rm = TRUE)

  # Calculate Harrell's C-statistic for the full dataset
  cox_model <- coxph(Surv(censored_time, event_status) ~ data_filtered[[risk_col]], data = data_filtered)
  c_stat <- as.numeric(summary(cox_model)$concordance[1])

  # Record the end time
  end_time <- Sys.time()
  total_runtime <- end_time - start_time

  message("Number of bootstrap iterations run:", n_bootstrap)
  message("Total runtime for the bootstrapping process:", total_runtime, "seconds")

  # Return the metrics
  return(list(
    AUC = auc_value, AUC_Lower = auc_ci[1], AUC_Upper = auc_ci[2], 
    C_Statistic = c_stat, C_Stat_Lower = c_stat_ci[1], C_Stat_Upper = c_stat_ci[2],
    Sensitivity = sensitivity, Specificity = specificity, PPV = ppv,
    NPV = npv, Optimal_Cutoff = optimal_cutoff, Cutoff_Method = cutoff_method,
    Cases = sum(data_filtered$event_status == 1), Controls = sum(data_filtered$event_status == 0)
  ))
}

# Function to summarize metrics for each racial group
summarize_risk_metrics <- function(data, race_label, time_limit, n_bootstrap = 5000) {
  metrics <- calculate_metrics_with_bootstrap(data, paste0("risk_", time_limit, "_years"), paste0("outcome_", time_limit, "_years"), "time_to_event", time_limit, n_bootstrap)
  
  return(data.frame(
    Race = race_label,
    Time_Horizon = paste0(time_limit, " Years"),
    AUC = metrics$AUC,
    AUC_Lower_CI = metrics$AUC_Lower,
    AUC_Upper_CI = metrics$AUC_Upper,
    Harrell_C_Statistic = metrics$C_Statistic,
    C_Stat_Lower_CI = metrics$C_Stat_Lower,
    C_Stat_Upper_CI = metrics$C_Stat_Upper,
    Sensitivity = metrics$Sensitivity,
    Specificity = metrics$Specificity,
    PPV = metrics$PPV,
    NPV = metrics$NPV,
    Optimal_Cutoff = metrics$Optimal_Cutoff,
    Cutoff_Method = metrics$Cutoff_Method,
    Cases = metrics$Cases,
    Controls = metrics$Controls
  ))
}

# Evaluate metrics for each racial group and time horizon
results_all_2yr <- summarize_risk_metrics(consistent_data, "All Races", 2, n_bootstrap = 5000)
results_all_5yr <- summarize_risk_metrics(consistent_data, "All Races", 5, n_bootstrap = 5000)
results_black_2yr <- summarize_risk_metrics(consistent_data %>% filter(race == "Black or African American"), "Black or African American", 2, n_bootstrap = 5000)
results_black_5yr <- summarize_risk_metrics(consistent_data %>% filter(race == "Black or African American"), "Black or African American", 5, n_bootstrap = 5000)
results_white_2yr <- summarize_risk_metrics(consistent_data %>% filter(race == "White"), "White", 2, n_bootstrap = 5000)
results_white_5yr <- summarize_risk_metrics(consistent_data %>% filter(race == "White"), "White", 5, n_bootstrap = 5000)
results_asian_2yr <- summarize_risk_metrics(consistent_data %>% filter(race == "Asian"), "Asian", 2, n_bootstrap = 5000)
results_asian_5yr <- summarize_risk_metrics(consistent_data %>% filter(race == "Asian"), "Asian", 5, n_bootstrap = 5000)

# Combine results including "All Races" for the final table
final_results <- bind_rows(
  results_all_2yr, results_all_5yr,
  results_black_2yr, results_black_5yr,
  results_white_2yr, results_white_5yr,
  results_asian_2yr, results_asian_5yr
)

# Exclude "All Races" for the forest plot
final_results_filtered_CKD2021 <- final_results %>%
  filter(Race != "All Races")

# Display the final results table including "All Races"
print(final_results)

# Function to plot forest plot for metrics
plot_forest <- function(results_df, metric, lower_ci, upper_ci, title, reference_line) {
  ggplot(results_df, aes_string(x = metric, y = "Race")) +
    geom_point(size = 3, shape = 16, color = "blue") +  # Metric point estimates
    geom_errorbarh(aes_string(xmin = lower_ci, xmax = upper_ci), height = 0.2, color = "black") +  # 95% CI
    geom_vline(xintercept = reference_line, linetype = "dashed", color = "red") +  # Reference line
    facet_wrap(~ Time_Horizon, scales = "free_y") +  # Separate panels for each time horizon
    labs(
      title = title,
      x = metric,
      y = "Race"
    ) +
    theme_minimal() +
    theme(
      axis.text.y = element_text(size = 14, face = "bold"),
      axis.text.x = element_text(size = 14),
      strip.text = element_text(size = 23, face = "bold"),
      panel.spacing = unit(1, "lines"),
      plot.title = element_text(size = 18, face = "bold", hjust = 0.5)
    )
}

# Plot the forest plots for AUC and Harrell's C-statistic using the filtered data
forest_plot_auc <- plot_forest(final_results_filtered_CKD2021, "AUC", "AUC_Lower_CI", "AUC_Upper_CI", "", reference_line = 0.5)
forest_plot_cstat <- plot_forest(final_results_filtered_CKD2021, "Harrell_C_Statistic", "C_Stat_Lower_CI", "C_Stat_Upper_CI", "", reference_line = 0.5)

# Display the forest plots
print(forest_plot_auc)
print(forest_plot_cstat)

# Save the AUC forest plot
ggsave("CKD2021_VHR_forest_plot_auc_filtered.png", plot = forest_plot_auc, width = 10, height = 8, dpi = 300)

# Save the Harrell's C-statistic forest plot
ggsave("CKD2021_VHR_forest_plot_cstat_filtered.png", plot = forest_plot_cstat, width = 10, height = 8, dpi = 300)

```

### 

### Group AUC Comparison

```{r}
# Load necessary libraries
library(ggplot2)
library(dplyr)

# Assuming final_results_filtered_MDRD, final_results_filtered_CKD2009, and final_results_filtered_CKD2021 
# are the data frames containing the results for each model

# Add a column to identify each model
final_results_filtered_MDRD$Model <- "MDRD"
final_results_filtered_CKD2009$Model <- "CKD-EPI-2009"
final_results_filtered_CKD2021$Model <- "CKD-EPI-2021"

# Combine all the data frames
combined_results <- bind_rows(
  final_results_filtered_MDRD,
  final_results_filtered_CKD2009,
  final_results_filtered_CKD2021
)

# Reorder the levels for the Race factor to start with Black, followed by White, then Asian
combined_results$Race <- factor(combined_results$Race, levels = c("Black or African American", "White", "Asian"))

# Plotting function to create facets by time horizon and race, comparing the AUC of each model
plot_auc_comparison <- function(data) {
  ggplot(data, aes(x = Model, y = AUC, color = Model)) +
    geom_point(size = 6, stroke = 1.5) +  # Larger points with bold outline
    geom_errorbar(aes(ymin = AUC_Lower_CI, ymax = AUC_Upper_CI), width = 0.3, linewidth = 1.5) +  # Thicker error bars
    facet_grid(Time_Horizon ~ Race, scales = "free") +
    labs(
      title = "AUC Comparison Across Models",
      x = "Model",
      y = "AUC"
    ) +
    theme_minimal() +
    theme(
      axis.text.x = element_text(size = 14, angle = 45, hjust = 1, face = "bold"),  # Larger and bold text
      axis.text.y = element_text(size = 14, face = "bold"),  # Larger and bold text
      axis.title.x = element_text(size = 16, face = "bold"),  # Bold axis title
      axis.title.y = element_text(size = 16, face = "bold"),  # Bold axis title
      strip.text = element_text(size = 18, face = "bold"),  # Larger and bold facet labels for Race
      strip.placement = "outside",  # Move strip text outside the plot area
      strip.background = element_blank(),  # Remove default strip background
      plot.title = element_text(size = 20, face = "bold", hjust = 0.5, margin = margin(b = 15)),  # Add space below the title
      legend.title = element_text(size = 14, face = "bold"),  # Bold legend title
      legend.text = element_text(size = 12, face = "bold"),   # Bold legend text
      panel.spacing = unit(2.5, "lines"),  # Increase spacing between panels for better readability
      plot.margin = margin(t = 20, r = 20, b = 20, l = 20)  # Add extra margin around the plot for clarity
    ) +
    scale_color_manual(values = c("MDRD" = "blue", "CKD-EPI-2009" = "green", "CKD-EPI-2021" = "red")) +  # Distinct colors
    guides(color = guide_legend(override.aes = list(size = 5)))  # Larger legend key sizes
}

# Create the plot using the combined results
auc_comparison_plot <- plot_auc_comparison(combined_results)

# Display the plot
print(auc_comparison_plot)

# Save the plot as a high-resolution image
ggsave("AUC_Comparison_Facet_Plot.png", plot = auc_comparison_plot, width = 14, height = 8, dpi = 300)

```

### Group c statistics comparison

```{r}
# Load necessary libraries
library(ggplot2)
library(dplyr)

# Assuming final_results_filtered_MDRD, final_results_filtered_CKD2009, and final_results_filtered_CKD2021 
# are the data frames containing the results for each model

# Add a column to identify each model
final_results_filtered_MDRD$Model <- "MDRD"
final_results_filtered_CKD2009$Model <- "CKD-EPI-2009"
final_results_filtered_CKD2021$Model <- "CKD-EPI-2021"

# Combine all the data frames
combined_results <- bind_rows(
  final_results_filtered_MDRD,
  final_results_filtered_CKD2009,
  final_results_filtered_CKD2021
)

# Reorder the levels for the Race factor to start with Black, followed by White, then Asian
combined_results$Race <- factor(combined_results$Race, levels = c("Black or African American", "White", "Asian"))

# Plotting function to create facets by time horizon and race, comparing the Harrell's C-statistic of each model
plot_cstat_comparison <- function(data) {
  ggplot(data, aes(x = Model, y = Harrell_C_Statistic, color = Model)) +
    geom_point(size = 6, stroke = 1.5) +  # Larger points with bold outline
    geom_errorbar(aes(ymin = C_Stat_Lower_CI, ymax = C_Stat_Upper_CI), width = 0.3, linewidth = 1.5) +  # Thicker error bars
    facet_grid(Time_Horizon ~ Race, scales = "free") +
    labs(
      title = "Harrell's C-Statistic Comparison Across Models",
      x = "Model",
      y = "Harrell's C-Statistic"
    ) +
    theme_minimal() +
    theme(
      axis.text.x = element_text(size = 14, angle = 45, hjust = 1, face = "bold"),  # Larger and bold text
      axis.text.y = element_text(size = 14, face = "bold"),  # Larger and bold text
      axis.title.x = element_text(size = 16, face = "bold"),  # Bold axis title
      axis.title.y = element_text(size = 16, face = "bold"),  # Bold axis title
      strip.text = element_text(size = 18, face = "bold"),  # Larger and bold facet labels for Race
      strip.placement = "outside",  # Move strip text outside the plot area
      strip.background = element_blank(),  # Remove default strip background
      plot.title = element_text(size = 20, face = "bold", hjust = 0.5, margin = margin(b = 15)),  # Add space below the title
      legend.title = element_text(size = 14, face = "bold"),  # Bold legend title
      legend.text = element_text(size = 12, face = "bold"),   # Bold legend text
      panel.spacing = unit(2.5, "lines"),  # Increase spacing between panels for better readability
      plot.margin = margin(t = 20, r = 20, b = 20, l = 20)  # Add extra margin around the plot for clarity
    ) +
    scale_color_manual(values = c("MDRD" = "blue", "CKD-EPI-2009" = "green", "CKD-EPI-2021" = "red")) +  # Distinct colors
    guides(color = guide_legend(override.aes = list(size = 5)))  # Larger legend key sizes
}

# Create the plot using the combined results
cstat_comparison_plot <- plot_cstat_comparison(combined_results)

# Display the plot
print(cstat_comparison_plot)

# Save the plot as a high-resolution image
ggsave("C_Statistic_Comparison_Facet_Plot.png", plot = cstat_comparison_plot, width = 14, height = 8, dpi = 300)

```

## Case study: CKD-EPI_2021 VERY HIGH RISK AND HIGH RISK POPULATION

#### CKD-EPI-2021

```{r}
# Install doParallel package if it's not installed
# install.packages("doParallel")

# Load the parallel processing libraries
library(doParallel)

# Set up parallel backend to use multiple processors
cores <- detectCores() - 1  # Detect the number of cores and use all but one
cl <- makeCluster(cores)
registerDoParallel(cl)

# Load necessary libraries
library(dplyr)
library(survival)

# Load dataset
inrange_person_with_summary <- read.csv("person_with_summary_inrange_indexdate_creatnine_albumin.csv")

# # Define a plausible range for eGFR and Albumin/Creatinine based on clinical thresholds for eGFR ≤ 60
# plausible_eGFR_min <- 0
# plausible_eGFR_max <- 60  # Kidney disease defined as eGFR < 60 mL/min/1.73 m²
# 
# plausible_acr_min <- 0
# plausible_acr_max <- 3000
# albuminuria_cutoff <- 10

# Step 1: Select relevant columns and rename Albumin_Creatinine for consistency
person_filtered_KFRE <- inrange_person_with_summary %>%
  select(
    person_id,
    sex_at_birth,
    eGFR_ckd_epi_2021,
    eGFR_ckd_epi_2009,
    eGFR_MDRD,
    Albumin_Creatinine = `Albumin.Creatinine`,
    age_precise,
    race,
    disease_status,
    time_to_event
  )



# Step 2: Data preprocessing and filtering based on the combination of eGFR and ACR for low-risk group
person_filtered_KFRE <- person_filtered_KFRE %>%
  mutate(
    event = disease_status
  ) %>%
  filter(
  # Low risk population
  (eGFR_ckd_epi_2021 >= 60 & Albumin_Creatinine < 30)
)%>%
  # Log-transform Albumin/Creatinine ratio (ACR)
  mutate(logACR = log(Albumin_Creatinine)) %>%
  # Ensure complete cases (remove rows with missing data)
  filter(complete.cases(.))

# Filter out rows with non-NA time_to_event values
case_control_time_to_event_counts <- person_filtered_KFRE %>%
  filter(!is.na(time_to_event)) %>%  # Keep only rows with non-NA time_to_event
  group_by(disease_status) %>%  # Assuming "disease_status" column indicates cases and controls
  summarise(count = n())  # Count the number of rows in each group

# Print the result
print("Number of cases and controls with non-NA time_to_event values:")
print(case_control_time_to_event_counts)


# Step 3: Verify summary statistics for eGFR and Albumin/Creatinine
summary(person_filtered_KFRE$eGFR_value)
summary(person_filtered_KFRE$Albumin_Creatinine)

# Check how many records are retained after filtering
cat("Number of records after filtering:", nrow(person_filtered_KFRE), "\n")

# Perform a complete case analysis
initial_row_count <- nrow(person_filtered_KFRE)

complete_cases <- person_filtered_KFRE %>%
  filter(complete.cases(.))

final_row_count <- nrow(complete_cases)

# Check if any rows have been removed
rows_removed <- initial_row_count - final_row_count

if (rows_removed > 0) {
  cat(rows_removed, "rows have been removed during the complete case analysis.\n")
} else {
  cat("No rows have been removed during the complete case analysis.\n")
}

# Report the number of complete cases
num_complete_cases <- nrow(complete_cases)
cat("Number of complete cases:", num_complete_cases, "\n")

# Remove any rows with NA in time_to_event or disease_status
complete_cases <- complete_cases %>%
  filter(!is.na(time_to_event) & !is.na(disease_status))

# Check the event distribution
cat("Event distribution (disease_status):\n")
print(table(complete_cases$disease_status))

# Step 4: Define the formula for calculating βsum (log hazard)
calculate_beta_sum <- function(age, sex, eGFR, logACR) {
  (-0.2201 * (age / 10 - 7.036)) +
  (0.2467 * (ifelse(sex == "Male", 1, 0) - 0.5642)) -
  (0.5567 * (eGFR / 5 - 7.222)) +
  (0.4510 * (logACR - 5.137))
}

# Apply the formula to calculate βsum
complete_cases <- complete_cases %>%
  mutate(beta_sum = calculate_beta_sum(age_precise, sex_at_birth, eGFR_ckd_epi_2021, logACR))

# Remove any rows with infinite or NA beta_sum
complete_cases <- complete_cases %>%
  filter(is.finite(beta_sum) & !is.na(beta_sum))

# Step 5: Calculate the risk for two and five years using the updated formulas
complete_cases <- complete_cases %>%
  mutate(
    risk_2_years = 100 * (1 - 0.975^exp(beta_sum)),  # Two-Year Risk calculation
    risk_5_years = 100 * (1 - 0.9240^exp(beta_sum))  # Five-Year Risk calculation
  )

# Save the data before filtering
before_filtering <- complete_cases

# Step 6: Apply the filtering to create consistent_data
consistent_data <- complete_cases %>%
  filter(!is.na(risk_5_years) & !is.na(risk_2_years))

# Step 7: Define and create the 2-year and 5-year outcomes based on disease_status and time_to_event
consistent_data <- consistent_data %>%
  mutate(
    outcome_2_years = ifelse(disease_status == 1 & time_to_event <= 2 * 365.25, 1, 0),
    outcome_5_years = ifelse(disease_status == 1 & time_to_event <= 5 * 365.25, 1, 0)
  )

# Find the rows that were removed by comparing before and after datasets
removed_rows <- anti_join(before_filtering, consistent_data, by = "person_id")

# Print the removed rows
if (nrow(removed_rows) > 0) {
  cat("Rows that were removed during filtering:\n")
  print(removed_rows)
} else {
  cat("No rows were removed during filtering.\n")
}

# Step 8: Verify the risk values
summary(consistent_data$risk_5_years)
summary(consistent_data$risk_2_years)

# Count the number of person_id entries with event == 1 and event == 0
event_count <- consistent_data %>%
  group_by(event) %>%
  summarise(count = n_distinct(person_id))

# Print the results
cat("Number of person_id with event == 1:", event_count$count[event_count$event == 1], "\n")
cat("Number of person_id with event == 0:", event_count$count[event_count$event == 0], "\n")

# Check the event distribution for the 2-year and 5-year outcomes
cat("\n2-Year Outcome Distribution:\n")
print(table(consistent_data$outcome_2_years))

cat("\n5-Year Outcome Distribution:\n")
print(table(consistent_data$outcome_5_years))


# Evaluation
# Load necessary libraries
library(dplyr)
library(pROC)
library(survival)
library(foreach)
library(doParallel)
library(ggplot2)

# Function to calculate metrics with bootstrapping and specificity adjustment
calculate_metrics_with_bootstrap <- function(data, risk_col, outcome_col, time_col, time_limit, n_bootstrap = 5000) {
  
  # Record the start time
  start_time <- Sys.time()
  
  # Apply censoring: keep all data but treat events beyond the time limit as censored
  data_filtered <- data %>%
    mutate(
      censored_time = pmin(!!sym(time_col), time_limit * 365),
      event_status = ifelse(!!sym(outcome_col) == 1 & !!sym(time_col) <= time_limit * 365, 1, 0)
    )
  
  # Filter out rows with missing or invalid data
  data_filtered <- data_filtered %>%
    filter(!is.na(!!sym(risk_col)) & !is.na(event_status) & !is.na(censored_time))
  
  # If no events or no controls exist in the data, return NA
  if (length(unique(data_filtered$event_status)) < 2) {
    return(list(
      AUC = NA, AUC_Lower = NA, AUC_Upper = NA, C_Statistic = NA,
      C_Stat_Lower = NA, C_Stat_Upper = NA, Sensitivity = NA,
      Specificity = NA, PPV = NA, NPV = NA, Optimal_Cutoff = NA,
      Cutoff_Method = "Not applicable", Cases = NA, Controls = NA
    ))
  }
  
  # Calculate ROC and AUC
  roc_curve <- roc(data_filtered$event_status, data_filtered[[risk_col]], levels = c(0, 1), direction = "<")
  auc_value <- as.numeric(auc(roc_curve))
  
  # Try to find the cutoff for specificity target 0.9
  cutoff_method <- "Specificity 0.9"
  optimal_cutoff <- tryCatch(
    {
      coords(roc_curve, x = 0.9, input = "specificity", ret = "threshold", transpose = TRUE)[1]
    },
    error = function(e) {
      NA
    }
  )
  
  # If the cutoff for specificity 0.9 is NA, try specificity 0.8
  if (is.na(optimal_cutoff)) {
    cutoff_method <- "Specificity 0.8"
    optimal_cutoff <- tryCatch(
      {
        coords(roc_curve, x = 0.8, input = "specificity", ret = "threshold", transpose = TRUE)[1]
      },
      error = function(e) {
        NA
      }
    )
  }
  
  # If the cutoff for specificity 0.8 is also NA, use the optimal cutoff
  if (is.na(optimal_cutoff)) {
    cutoff_method <- "Optimal Cutoff"
    optimal_cutoff <- tryCatch(
      {
        coords(roc_curve, x = "best", best.method = "closest.topleft", ret = "threshold", transpose = TRUE)[1]
      },
      error = function(e) {
        NA
      }
    )
  }
  
  # If the optimal_cutoff is not valid, return NA for metrics
  if (is.na(optimal_cutoff)) {
    return(list(
      AUC = auc_value, AUC_Lower = NA, AUC_Upper = NA, C_Statistic = NA,
      C_Stat_Lower = NA, C_Stat_Upper = NA, Sensitivity = NA,
      Specificity = NA, PPV = NA, NPV = NA, Optimal_Cutoff = NA,
      Cutoff_Method = "Failed", Cases = sum(data_filtered$event_status == 1),
      Controls = sum(data_filtered$event_status == 0)
    ))
  }

  # Calculate predictions based on the new cutoff
  pred_class <- ifelse(data_filtered[[risk_col]] >= optimal_cutoff, 1, 0)

  # Calculate confusion matrix statistics based on the new cutoff
  confusion_mat <- table(Predicted = pred_class, Actual = data_filtered$event_status)

  # If confusion matrix is not 2x2, set metrics to NA
  if (!all(c(0, 1) %in% rownames(confusion_mat)) || !all(c(0, 1) %in% colnames(confusion_mat))) {
    sensitivity <- NA
    specificity <- NA
    ppv <- NA
    npv <- NA
  } else {
    sensitivity <- confusion_mat[2, 2] / sum(confusion_mat[, 2])
    specificity <- confusion_mat[1, 1] / sum(confusion_mat[, 1])
    ppv <- confusion_mat[2, 2] / sum(confusion_mat[2, ])
    npv <- confusion_mat[1, 1] / sum(confusion_mat[1, ])
  }

  # Bootstrapping AUC and C-statistic for confidence intervals
  auc_values <- numeric(n_bootstrap)
  c_stat_values <- numeric(n_bootstrap)

  bootstrapped_results <- foreach(i = 1:n_bootstrap, .combine = rbind, .packages = c("pROC", "survival")) %dopar% {
    bootstrap_sample <- data_filtered[sample(1:nrow(data_filtered), replace = TRUE), ]

    if (length(unique(bootstrap_sample$event_status)) < 2) {
      return(c(NA, NA))
    }

    roc_curve <- roc(bootstrap_sample$event_status, bootstrap_sample[[risk_col]])
    auc_val <- as.numeric(auc(roc_curve))

    cox_model_boot <- coxph(Surv(bootstrap_sample$censored_time, bootstrap_sample$event_status) ~ bootstrap_sample[[risk_col]], data = bootstrap_sample)
    c_stat_val <- as.numeric(summary(cox_model_boot)$concordance[1])

    return(c(auc_val, c_stat_val))
  }

  auc_values <- bootstrapped_results[, 1]
  c_stat_values <- bootstrapped_results[, 2]

  # Calculate 95% confidence intervals
  auc_ci <- quantile(auc_values[!is.na(auc_values)], probs = c(0.025, 0.975), na.rm = TRUE)
  c_stat_ci <- quantile(c_stat_values[!is.na(c_stat_values)], probs = c(0.025, 0.975), na.rm = TRUE)

  # Calculate Harrell's C-statistic for the full dataset
  cox_model <- coxph(Surv(censored_time, event_status) ~ data_filtered[[risk_col]], data = data_filtered)
  c_stat <- as.numeric(summary(cox_model)$concordance[1])

  # Record the end time
  end_time <- Sys.time()
  total_runtime <- end_time - start_time

  message("Number of bootstrap iterations run:", n_bootstrap)
  message("Total runtime for the bootstrapping process:", total_runtime, "seconds")

  # Return the metrics
  return(list(
    AUC = auc_value, AUC_Lower = auc_ci[1], AUC_Upper = auc_ci[2], 
    C_Statistic = c_stat, C_Stat_Lower = c_stat_ci[1], C_Stat_Upper = c_stat_ci[2],
    Sensitivity = sensitivity, Specificity = specificity, PPV = ppv,
    NPV = npv, Optimal_Cutoff = optimal_cutoff, Cutoff_Method = cutoff_method,
    Cases = sum(data_filtered$event_status == 1), Controls = sum(data_filtered$event_status == 0)
  ))
}

# Function to summarize metrics for each racial group
summarize_risk_metrics <- function(data, race_label, time_limit, n_bootstrap = 5000) {
  metrics <- calculate_metrics_with_bootstrap(data, paste0("risk_", time_limit, "_years"), paste0("outcome_", time_limit, "_years"), "time_to_event", time_limit, n_bootstrap)
  
  return(data.frame(
    Race = race_label,
    Time_Horizon = paste0(time_limit, " Years"),
    AUC = metrics$AUC,
    AUC_Lower_CI = metrics$AUC_Lower,
    AUC_Upper_CI = metrics$AUC_Upper,
    Harrell_C_Statistic = metrics$C_Statistic,
    C_Stat_Lower_CI = metrics$C_Stat_Lower,
    C_Stat_Upper_CI = metrics$C_Stat_Upper,
    Sensitivity = metrics$Sensitivity,
    Specificity = metrics$Specificity,
    PPV = metrics$PPV,
    NPV = metrics$NPV,
    Optimal_Cutoff = metrics$Optimal_Cutoff,
    Cutoff_Method = metrics$Cutoff_Method,
    Cases = metrics$Cases,
    Controls = metrics$Controls
  ))
}

# Evaluate metrics for each racial group and time horizon
results_all_2yr <- summarize_risk_metrics(consistent_data, "All Races", 2, n_bootstrap = 5000)
results_all_5yr <- summarize_risk_metrics(consistent_data, "All Races", 5, n_bootstrap = 5000)
results_black_2yr <- summarize_risk_metrics(consistent_data %>% filter(race == "Black or African American"), "Black or African American", 2, n_bootstrap = 5000)
results_black_5yr <- summarize_risk_metrics(consistent_data %>% filter(race == "Black or African American"), "Black or African American", 5, n_bootstrap = 5000)
results_white_2yr <- summarize_risk_metrics(consistent_data %>% filter(race == "White"), "White", 2, n_bootstrap = 5000)
results_white_5yr <- summarize_risk_metrics(consistent_data %>% filter(race == "White"), "White", 5, n_bootstrap = 5000)
results_asian_2yr <- summarize_risk_metrics(consistent_data %>% filter(race == "Asian"), "Asian", 2, n_bootstrap = 5000)
results_asian_5yr <- summarize_risk_metrics(consistent_data %>% filter(race == "Asian"), "Asian", 5, n_bootstrap = 5000)

# Combine results including "All Races" for the final table
final_results <- bind_rows(
  results_all_2yr, results_all_5yr,
  results_black_2yr, results_black_5yr,
  results_white_2yr, results_white_5yr,
  results_asian_2yr, results_asian_5yr
)

# Exclude "All Races" for the forest plot
final_results_filtered_CKD2021 <- final_results %>%
  filter(Race != "All Races")

# Display the final results table including "All Races"
print(final_results)

# Function to plot forest plot for metrics
plot_forest <- function(results_df, metric, lower_ci, upper_ci, title, reference_line) {
  ggplot(results_df, aes_string(x = metric, y = "Race")) +
    geom_point(size = 3, shape = 16, color = "blue") +  # Metric point estimates
    geom_errorbarh(aes_string(xmin = lower_ci, xmax = upper_ci), height = 0.2, color = "black") +  # 95% CI
    geom_vline(xintercept = reference_line, linetype = "dashed", color = "red") +  # Reference line
    facet_wrap(~ Time_Horizon, scales = "free_y") +  # Separate panels for each time horizon
    labs(
      title = title,
      x = metric,
      y = "Race"
    ) +
    theme_minimal() +
    theme(
      axis.text.y = element_text(size = 14, face = "bold"),
      axis.text.x = element_text(size = 14),
      strip.text = element_text(size = 23, face = "bold"),
      panel.spacing = unit(1, "lines"),
      plot.title = element_text(size = 18, face = "bold", hjust = 0.5)
    )
}

# Plot the forest plots for AUC and Harrell's C-statistic using the filtered data
forest_plot_auc <- plot_forest(final_results_filtered_CKD2021, "AUC", "AUC_Lower_CI", "AUC_Upper_CI", "", reference_line = 0.5)
forest_plot_cstat <- plot_forest(final_results_filtered_CKD2021, "Harrell_C_Statistic", "C_Stat_Lower_CI", "C_Stat_Upper_CI", "", reference_line = 0.5)

# Display the forest plots
print(forest_plot_auc)
print(forest_plot_cstat)

# Save the AUC forest plot
ggsave("CKD2021_VHR_forest_plot_auc_filtered.png", plot = forest_plot_auc, width = 10, height = 8, dpi = 300)

# Save the Harrell's C-statistic forest plot
ggsave("CKD2021_VHR_forest_plot_cstat_filtered.png", plot = forest_plot_cstat, width = 10, height = 8, dpi = 300)

```

#### CKD-EPI-2009

```{r}
# Install doParallel package if it's not installed
# install.packages("doParallel")

# Load the parallel processing libraries
library(doParallel)

# Set up parallel backend to use multiple processors
cores <- detectCores() - 1  # Detect the number of cores and use all but one
cl <- makeCluster(cores)
registerDoParallel(cl)

# Load necessary libraries
library(dplyr)
library(survival)


# Step 4: Define the formula for calculating βsum (log hazard)
calculate_beta_sum <- function(age, sex, eGFR, logACR) {
  (-0.2201 * (age / 10 - 7.036)) +
  (0.2467 * (ifelse(sex == "Male", 1, 0) - 0.5642)) -
  (0.5567 * (eGFR / 5 - 7.222)) +
  (0.4510 * (logACR - 5.137))
}

# Apply the formula to calculate βsum
complete_cases <- complete_cases %>%
  mutate(beta_sum = calculate_beta_sum(age_precise, sex_at_birth, eGFR_ckd_epi_2009, logACR))

# Remove any rows with infinite or NA beta_sum
complete_cases <- complete_cases %>%
  filter(is.finite(beta_sum) & !is.na(beta_sum))

# Step 5: Calculate the risk for two and five years using the updated formulas
complete_cases <- complete_cases %>%
  mutate(
    risk_2_years = 100 * (1 - 0.975^exp(beta_sum)),  # Two-Year Risk calculation
    risk_5_years = 100 * (1 - 0.9240^exp(beta_sum))  # Five-Year Risk calculation
  )

# Save the data before filtering
before_filtering <- complete_cases

# Step 6: Apply the filtering to create consistent_data
consistent_data <- complete_cases %>%
  filter(!is.na(risk_5_years) & !is.na(risk_2_years))

# Step 7: Define and create the 2-year and 5-year outcomes based on disease_status and time_to_event
consistent_data <- consistent_data %>%
  mutate(
    outcome_2_years = ifelse(disease_status == 1 & time_to_event <= 2 * 365.25, 1, 0),
    outcome_5_years = ifelse(disease_status == 1 & time_to_event <= 5 * 365.25, 1, 0)
  )

# Find the rows that were removed by comparing before and after datasets
removed_rows <- anti_join(before_filtering, consistent_data, by = "person_id")

# Print the removed rows
if (nrow(removed_rows) > 0) {
  cat("Rows that were removed during filtering:\n")
  print(removed_rows)
} else {
  cat("No rows were removed during filtering.\n")
}

# Step 8: Verify the risk values
summary(consistent_data$risk_5_years)
summary(consistent_data$risk_2_years)

# Count the number of person_id entries with event == 1 and event == 0
event_count <- consistent_data %>%
  group_by(event) %>%
  summarise(count = n_distinct(person_id))

# Print the results
cat("Number of person_id with event == 1:", event_count$count[event_count$event == 1], "\n")
cat("Number of person_id with event == 0:", event_count$count[event_count$event == 0], "\n")

# Check the event distribution for the 2-year and 5-year outcomes
cat("\n2-Year Outcome Distribution:\n")
print(table(consistent_data$outcome_2_years))

cat("\n5-Year Outcome Distribution:\n")
print(table(consistent_data$outcome_5_years))

# Evaluation
# Load necessary libraries
library(dplyr)
library(pROC)
library(survival)
library(foreach)
library(doParallel)
library(ggplot2)

# Function to calculate metrics with bootstrapping and specificity adjustment
calculate_metrics_with_bootstrap <- function(data, risk_col, outcome_col, time_col, time_limit, n_bootstrap = 5000) {
  
  # Record the start time
  start_time <- Sys.time()
  
  # Apply censoring: keep all data but treat events beyond the time limit as censored
  data_filtered <- data %>%
    mutate(
      censored_time = pmin(!!sym(time_col), time_limit * 365),
      event_status = ifelse(!!sym(outcome_col) == 1 & !!sym(time_col) <= time_limit * 365, 1, 0)
    )
  
  # Filter out rows with missing or invalid data
  data_filtered <- data_filtered %>%
    filter(!is.na(!!sym(risk_col)) & !is.na(event_status) & !is.na(censored_time))
  
  # If no events or no controls exist in the data, return NA
  if (length(unique(data_filtered$event_status)) < 2) {
    return(list(
      AUC = NA, AUC_Lower = NA, AUC_Upper = NA, C_Statistic = NA,
      C_Stat_Lower = NA, C_Stat_Upper = NA, Sensitivity = NA,
      Specificity = NA, PPV = NA, NPV = NA, Optimal_Cutoff = NA,
      Cutoff_Method = "Not applicable", Cases = NA, Controls = NA
    ))
  }
  
  # Calculate ROC and AUC
  roc_curve <- roc(data_filtered$event_status, data_filtered[[risk_col]], levels = c(0, 1), direction = "<")
  auc_value <- as.numeric(auc(roc_curve))
  
  # Try to find the cutoff for specificity target 0.9
  cutoff_method <- "Specificity 0.9"
  optimal_cutoff <- tryCatch(
    {
      coords(roc_curve, x = 0.9, input = "specificity", ret = "threshold", transpose = TRUE)[1]
    },
    error = function(e) {
      NA
    }
  )
  
  # If the cutoff for specificity 0.9 is NA, try specificity 0.8
  if (is.na(optimal_cutoff)) {
    cutoff_method <- "Specificity 0.8"
    optimal_cutoff <- tryCatch(
      {
        coords(roc_curve, x = 0.8, input = "specificity", ret = "threshold", transpose = TRUE)[1]
      },
      error = function(e) {
        NA
      }
    )
  }
  
  # If the cutoff for specificity 0.8 is also NA, use the optimal cutoff
  if (is.na(optimal_cutoff)) {
    cutoff_method <- "Optimal Cutoff"
    optimal_cutoff <- tryCatch(
      {
        coords(roc_curve, x = "best", best.method = "closest.topleft", ret = "threshold", transpose = TRUE)[1]
      },
      error = function(e) {
        NA
      }
    )
  }
  
  # If the optimal_cutoff is not valid, return NA for metrics
  if (is.na(optimal_cutoff)) {
    return(list(
      AUC = auc_value, AUC_Lower = NA, AUC_Upper = NA, C_Statistic = NA,
      C_Stat_Lower = NA, C_Stat_Upper = NA, Sensitivity = NA,
      Specificity = NA, PPV = NA, NPV = NA, Optimal_Cutoff = NA,
      Cutoff_Method = "Failed", Cases = sum(data_filtered$event_status == 1),
      Controls = sum(data_filtered$event_status == 0)
    ))
  }

  # Calculate predictions based on the new cutoff
  pred_class <- ifelse(data_filtered[[risk_col]] >= optimal_cutoff, 1, 0)

  # Calculate confusion matrix statistics based on the new cutoff
  confusion_mat <- table(Predicted = pred_class, Actual = data_filtered$event_status)

  # If confusion matrix is not 2x2, set metrics to NA
  if (!all(c(0, 1) %in% rownames(confusion_mat)) || !all(c(0, 1) %in% colnames(confusion_mat))) {
    sensitivity <- NA
    specificity <- NA
    ppv <- NA
    npv <- NA
  } else {
    sensitivity <- confusion_mat[2, 2] / sum(confusion_mat[, 2])
    specificity <- confusion_mat[1, 1] / sum(confusion_mat[, 1])
    ppv <- confusion_mat[2, 2] / sum(confusion_mat[2, ])
    npv <- confusion_mat[1, 1] / sum(confusion_mat[1, ])
  }

  # Bootstrapping AUC and C-statistic for confidence intervals
  auc_values <- numeric(n_bootstrap)
  c_stat_values <- numeric(n_bootstrap)

  bootstrapped_results <- foreach(i = 1:n_bootstrap, .combine = rbind, .packages = c("pROC", "survival")) %dopar% {
    bootstrap_sample <- data_filtered[sample(1:nrow(data_filtered), replace = TRUE), ]

    if (length(unique(bootstrap_sample$event_status)) < 2) {
      return(c(NA, NA))
    }

    roc_curve <- roc(bootstrap_sample$event_status, bootstrap_sample[[risk_col]])
    auc_val <- as.numeric(auc(roc_curve))

    cox_model_boot <- coxph(Surv(bootstrap_sample$censored_time, bootstrap_sample$event_status) ~ bootstrap_sample[[risk_col]], data = bootstrap_sample)
    c_stat_val <- as.numeric(summary(cox_model_boot)$concordance[1])

    return(c(auc_val, c_stat_val))
  }

  auc_values <- bootstrapped_results[, 1]
  c_stat_values <- bootstrapped_results[, 2]

  # Calculate 95% confidence intervals
  auc_ci <- quantile(auc_values[!is.na(auc_values)], probs = c(0.025, 0.975), na.rm = TRUE)
  c_stat_ci <- quantile(c_stat_values[!is.na(c_stat_values)], probs = c(0.025, 0.975), na.rm = TRUE)

  # Calculate Harrell's C-statistic for the full dataset
  cox_model <- coxph(Surv(censored_time, event_status) ~ data_filtered[[risk_col]], data = data_filtered)
  c_stat <- as.numeric(summary(cox_model)$concordance[1])

  # Record the end time
  end_time <- Sys.time()
  total_runtime <- end_time - start_time

  message("Number of bootstrap iterations run:", n_bootstrap)
  message("Total runtime for the bootstrapping process:", total_runtime, "seconds")

  # Return the metrics
  return(list(
    AUC = auc_value, AUC_Lower = auc_ci[1], AUC_Upper = auc_ci[2], 
    C_Statistic = c_stat, C_Stat_Lower = c_stat_ci[1], C_Stat_Upper = c_stat_ci[2],
    Sensitivity = sensitivity, Specificity = specificity, PPV = ppv,
    NPV = npv, Optimal_Cutoff = optimal_cutoff, Cutoff_Method = cutoff_method,
    Cases = sum(data_filtered$event_status == 1), Controls = sum(data_filtered$event_status == 0)
  ))
}

# Function to summarize metrics for each racial group
summarize_risk_metrics <- function(data, race_label, time_limit, n_bootstrap = 5000) {
  metrics <- calculate_metrics_with_bootstrap(data, paste0("risk_", time_limit, "_years"), paste0("outcome_", time_limit, "_years"), "time_to_event", time_limit, n_bootstrap)
  
  return(data.frame(
    Race = race_label,
    Time_Horizon = paste0(time_limit, " Years"),
    AUC = metrics$AUC,
    AUC_Lower_CI = metrics$AUC_Lower,
    AUC_Upper_CI = metrics$AUC_Upper,
    Harrell_C_Statistic = metrics$C_Statistic,
    C_Stat_Lower_CI = metrics$C_Stat_Lower,
    C_Stat_Upper_CI = metrics$C_Stat_Upper,
    Sensitivity = metrics$Sensitivity,
    Specificity = metrics$Specificity,
    PPV = metrics$PPV,
    NPV = metrics$NPV,
    Optimal_Cutoff = metrics$Optimal_Cutoff,
    Cutoff_Method = metrics$Cutoff_Method,
    Cases = metrics$Cases,
    Controls = metrics$Controls
  ))
}

# Evaluate metrics for each racial group and time horizon
results_all_2yr <- summarize_risk_metrics(consistent_data, "All Races", 2, n_bootstrap = 5000)
results_all_5yr <- summarize_risk_metrics(consistent_data, "All Races", 5, n_bootstrap = 5000)
results_black_2yr <- summarize_risk_metrics(consistent_data %>% filter(race == "Black or African American"), "Black or African American", 2, n_bootstrap = 5000)
results_black_5yr <- summarize_risk_metrics(consistent_data %>% filter(race == "Black or African American"), "Black or African American", 5, n_bootstrap = 5000)
results_white_2yr <- summarize_risk_metrics(consistent_data %>% filter(race == "White"), "White", 2, n_bootstrap = 5000)
results_white_5yr <- summarize_risk_metrics(consistent_data %>% filter(race == "White"), "White", 5, n_bootstrap = 5000)
results_asian_2yr <- summarize_risk_metrics(consistent_data %>% filter(race == "Asian"), "Asian", 2, n_bootstrap = 5000)
results_asian_5yr <- summarize_risk_metrics(consistent_data %>% filter(race == "Asian"), "Asian", 5, n_bootstrap = 5000)

# Combine results including "All Races" for the final table
final_results <- bind_rows(
  results_all_2yr, results_all_5yr,
  results_black_2yr, results_black_5yr,
  results_white_2yr, results_white_5yr,
  results_asian_2yr, results_asian_5yr
)

# Exclude "All Races" for the forest plot
final_results_filtered_CKD2009 <- final_results %>%
  filter(Race != "All Races")

# Display the final results table including "All Races"
print(final_results)

# Function to plot forest plot for metrics
plot_forest <- function(results_df, metric, lower_ci, upper_ci, title, reference_line) {
  ggplot(results_df, aes_string(x = metric, y = "Race")) +
    geom_point(size = 3, shape = 16, color = "blue") +  # Metric point estimates
    geom_errorbarh(aes_string(xmin = lower_ci, xmax = upper_ci), height = 0.2, color = "black") +  # 95% CI
    geom_vline(xintercept = reference_line, linetype = "dashed", color = "red") +  # Reference line
    facet_wrap(~ Time_Horizon, scales = "free_y") +  # Separate panels for each time horizon
    labs(
      title = title,
      x = metric,
      y = "Race"
    ) +
    theme_minimal() +
    theme(
      axis.text.y = element_text(size = 14, face = "bold"),
      axis.text.x = element_text(size = 14),
      strip.text = element_text(size = 23, face = "bold"),
      panel.spacing = unit(1, "lines"),
      plot.title = element_text(size = 18, face = "bold", hjust = 0.5)
    )
}

# Plot the forest plots for AUC and Harrell's C-statistic using the filtered data
forest_plot_auc <- plot_forest(final_results_filtered_CKD2009, "AUC", "AUC_Lower_CI", "AUC_Upper_CI", "", reference_line = 0.5)
forest_plot_cstat <- plot_forest(final_results_filtered_CKD2009, "Harrell_C_Statistic", "C_Stat_Lower_CI", "C_Stat_Upper_CI", "", reference_line = 0.5)

# Display the forest plots
print(forest_plot_auc)
print(forest_plot_cstat)

# Save the AUC forest plot
ggsave("CKD2009_VHR_forest_plot_auc_filtered.png", plot = forest_plot_auc, width = 10, height = 8, dpi = 300)

# Save the Harrell's C-statistic forest plot
ggsave("CKD2009_VHR_forest_plot_cstat_filtered.png", plot = forest_plot_cstat, width = 10, height = 8, dpi = 300)

# Stop the parallel backend after use
stopCluster(cl)


```

#### MDRD

```{r}
# Install doParallel package if it's not installed
# install.packages("doParallel")

# Load the parallel processing libraries
library(doParallel)

# Set up parallel backend to use multiple processors
cores <- detectCores() - 1  # Detect the number of cores and use all but one
cl <- makeCluster(cores)
registerDoParallel(cl)

# Load necessary libraries
library(dplyr)
library(survival)


# Step 4: Define the formula for calculating βsum (log hazard)
calculate_beta_sum <- function(age, sex, eGFR, logACR) {
  (-0.2201 * (age / 10 - 7.036)) +
  (0.2467 * (ifelse(sex == "Male", 1, 0) - 0.5642)) -
  (0.5567 * (eGFR / 5 - 7.222)) +
  (0.4510 * (logACR - 5.137))
}

# Apply the formula to calculate βsum
complete_cases <- complete_cases %>%
  mutate(beta_sum = calculate_beta_sum(age_precise, sex_at_birth, eGFR_MDRD, logACR))

# Remove any rows with infinite or NA beta_sum
complete_cases <- complete_cases %>%
  filter(is.finite(beta_sum) & !is.na(beta_sum))

# Step 5: Calculate the risk for two and five years using the updated formulas
complete_cases <- complete_cases %>%
  mutate(
    risk_2_years = 100 * (1 - 0.975^exp(beta_sum)),  # Two-Year Risk calculation
    risk_5_years = 100 * (1 - 0.9240^exp(beta_sum))  # Five-Year Risk calculation
  )

# Save the data before filtering
before_filtering <- complete_cases

# Step 6: Apply the filtering to create consistent_data
consistent_data <- complete_cases %>%
  filter(!is.na(risk_5_years) & !is.na(risk_2_years))

# Step 7: Define and create the 2-year and 5-year outcomes based on disease_status and time_to_event
consistent_data <- consistent_data %>%
  mutate(
    outcome_2_years = ifelse(disease_status == 1 & time_to_event <= 2 * 365.25, 1, 0),
    outcome_5_years = ifelse(disease_status == 1 & time_to_event <= 5 * 365.25, 1, 0)
  )

# Find the rows that were removed by comparing before and after datasets
removed_rows <- anti_join(before_filtering, consistent_data, by = "person_id")

# Print the removed rows
if (nrow(removed_rows) > 0) {
  cat("Rows that were removed during filtering:\n")
  print(removed_rows)
} else {
  cat("No rows were removed during filtering.\n")
}

# Step 8: Verify the risk values
summary(consistent_data$risk_5_years)
summary(consistent_data$risk_2_years)

# Count the number of person_id entries with event == 1 and event == 0
event_count <- consistent_data %>%
  group_by(event) %>%
  summarise(count = n_distinct(person_id))

# Print the results
cat("Number of person_id with event == 1:", event_count$count[event_count$event == 1], "\n")
cat("Number of person_id with event == 0:", event_count$count[event_count$event == 0], "\n")

# Check the event distribution for the 2-year and 5-year outcomes
cat("\n2-Year Outcome Distribution:\n")
print(table(consistent_data$outcome_2_years))

cat("\n5-Year Outcome Distribution:\n")
print(table(consistent_data$outcome_5_years))


# Evaluation
# Load necessary libraries
library(dplyr)
library(pROC)
library(survival)
library(foreach)
library(doParallel)
library(ggplot2)

# Function to calculate metrics with bootstrapping and specificity adjustment
calculate_metrics_with_bootstrap <- function(data, risk_col, outcome_col, time_col, time_limit, n_bootstrap = 5000) {
  
  # Record the start time
  start_time <- Sys.time()
  
  # Apply censoring: keep all data but treat events beyond the time limit as censored
  data_filtered <- data %>%
    mutate(
      censored_time = pmin(!!sym(time_col), time_limit * 365),
      event_status = ifelse(!!sym(outcome_col) == 1 & !!sym(time_col) <= time_limit * 365, 1, 0)
    )
  
  # Filter out rows with missing or invalid data
  data_filtered <- data_filtered %>%
    filter(!is.na(!!sym(risk_col)) & !is.na(event_status) & !is.na(censored_time))
  
  # If no events or no controls exist in the data, return NA
  if (length(unique(data_filtered$event_status)) < 2) {
    return(list(
      AUC = NA, AUC_Lower = NA, AUC_Upper = NA, C_Statistic = NA,
      C_Stat_Lower = NA, C_Stat_Upper = NA, Sensitivity = NA,
      Specificity = NA, PPV = NA, NPV = NA, Optimal_Cutoff = NA,
      Cutoff_Method = "Not applicable", Cases = NA, Controls = NA
    ))
  }
  
  # Calculate ROC and AUC
  roc_curve <- roc(data_filtered$event_status, data_filtered[[risk_col]], levels = c(0, 1), direction = "<")
  auc_value <- as.numeric(auc(roc_curve))
  
  # Try to find the cutoff for specificity target 0.9
  cutoff_method <- "Specificity 0.9"
  optimal_cutoff <- tryCatch(
    {
      coords(roc_curve, x = 0.9, input = "specificity", ret = "threshold", transpose = TRUE)[1]
    },
    error = function(e) {
      NA
    }
  )
  
  # If the cutoff for specificity 0.9 is NA, try specificity 0.8
  if (is.na(optimal_cutoff)) {
    cutoff_method <- "Specificity 0.8"
    optimal_cutoff <- tryCatch(
      {
        coords(roc_curve, x = 0.8, input = "specificity", ret = "threshold", transpose = TRUE)[1]
      },
      error = function(e) {
        NA
      }
    )
  }
  
  # If the cutoff for specificity 0.8 is also NA, use the optimal cutoff
  if (is.na(optimal_cutoff)) {
    cutoff_method <- "Optimal Cutoff"
    optimal_cutoff <- tryCatch(
      {
        coords(roc_curve, x = "best", best.method = "closest.topleft", ret = "threshold", transpose = TRUE)[1]
      },
      error = function(e) {
        NA
      }
    )
  }
  
  # If the optimal_cutoff is not valid, return NA for metrics
  if (is.na(optimal_cutoff)) {
    return(list(
      AUC = auc_value, AUC_Lower = NA, AUC_Upper = NA, C_Statistic = NA,
      C_Stat_Lower = NA, C_Stat_Upper = NA, Sensitivity = NA,
      Specificity = NA, PPV = NA, NPV = NA, Optimal_Cutoff = NA,
      Cutoff_Method = "Failed", Cases = sum(data_filtered$event_status == 1),
      Controls = sum(data_filtered$event_status == 0)
    ))
  }

  # Calculate predictions based on the new cutoff
  pred_class <- ifelse(data_filtered[[risk_col]] >= optimal_cutoff, 1, 0)

  # Calculate confusion matrix statistics based on the new cutoff
  confusion_mat <- table(Predicted = pred_class, Actual = data_filtered$event_status)

  # If confusion matrix is not 2x2, set metrics to NA
  if (!all(c(0, 1) %in% rownames(confusion_mat)) || !all(c(0, 1) %in% colnames(confusion_mat))) {
    sensitivity <- NA
    specificity <- NA
    ppv <- NA
    npv <- NA
  } else {
    sensitivity <- confusion_mat[2, 2] / sum(confusion_mat[, 2])
    specificity <- confusion_mat[1, 1] / sum(confusion_mat[, 1])
    ppv <- confusion_mat[2, 2] / sum(confusion_mat[2, ])
    npv <- confusion_mat[1, 1] / sum(confusion_mat[1, ])
  }

  # Bootstrapping AUC and C-statistic for confidence intervals
  auc_values <- numeric(n_bootstrap)
  c_stat_values <- numeric(n_bootstrap)

  bootstrapped_results <- foreach(i = 1:n_bootstrap, .combine = rbind, .packages = c("pROC", "survival")) %dopar% {
    bootstrap_sample <- data_filtered[sample(1:nrow(data_filtered), replace = TRUE), ]

    if (length(unique(bootstrap_sample$event_status)) < 2) {
      return(c(NA, NA))
    }

    roc_curve <- roc(bootstrap_sample$event_status, bootstrap_sample[[risk_col]])
    auc_val <- as.numeric(auc(roc_curve))

    cox_model_boot <- coxph(Surv(bootstrap_sample$censored_time, bootstrap_sample$event_status) ~ bootstrap_sample[[risk_col]], data = bootstrap_sample)
    c_stat_val <- as.numeric(summary(cox_model_boot)$concordance[1])

    return(c(auc_val, c_stat_val))
  }

  auc_values <- bootstrapped_results[, 1]
  c_stat_values <- bootstrapped_results[, 2]

  # Calculate 95% confidence intervals
  auc_ci <- quantile(auc_values[!is.na(auc_values)], probs = c(0.025, 0.975), na.rm = TRUE)
  c_stat_ci <- quantile(c_stat_values[!is.na(c_stat_values)], probs = c(0.025, 0.975), na.rm = TRUE)

  # Calculate Harrell's C-statistic for the full dataset
  cox_model <- coxph(Surv(censored_time, event_status) ~ data_filtered[[risk_col]], data = data_filtered)
  c_stat <- as.numeric(summary(cox_model)$concordance[1])

  # Record the end time
  end_time <- Sys.time()
  total_runtime <- end_time - start_time

  message("Number of bootstrap iterations run:", n_bootstrap)
  message("Total runtime for the bootstrapping process:", total_runtime, "seconds")

  # Return the metrics
  return(list(
    AUC = auc_value, AUC_Lower = auc_ci[1], AUC_Upper = auc_ci[2], 
    C_Statistic = c_stat, C_Stat_Lower = c_stat_ci[1], C_Stat_Upper = c_stat_ci[2],
    Sensitivity = sensitivity, Specificity = specificity, PPV = ppv,
    NPV = npv, Optimal_Cutoff = optimal_cutoff, Cutoff_Method = cutoff_method,
    Cases = sum(data_filtered$event_status == 1), Controls = sum(data_filtered$event_status == 0)
  ))
}

# Function to summarize metrics for each racial group
summarize_risk_metrics <- function(data, race_label, time_limit, n_bootstrap = 5000) {
  metrics <- calculate_metrics_with_bootstrap(data, paste0("risk_", time_limit, "_years"), paste0("outcome_", time_limit, "_years"), "time_to_event", time_limit, n_bootstrap)
  
  return(data.frame(
    Race = race_label,
    Time_Horizon = paste0(time_limit, " Years"),
    AUC = metrics$AUC,
    AUC_Lower_CI = metrics$AUC_Lower,
    AUC_Upper_CI = metrics$AUC_Upper,
    Harrell_C_Statistic = metrics$C_Statistic,
    C_Stat_Lower_CI = metrics$C_Stat_Lower,
    C_Stat_Upper_CI = metrics$C_Stat_Upper,
    Sensitivity = metrics$Sensitivity,
    Specificity = metrics$Specificity,
    PPV = metrics$PPV,
    NPV = metrics$NPV,
    Optimal_Cutoff = metrics$Optimal_Cutoff,
    Cutoff_Method = metrics$Cutoff_Method,
    Cases = metrics$Cases,
    Controls = metrics$Controls
  ))
}

# Evaluate metrics for each racial group and time horizon
results_all_2yr <- summarize_risk_metrics(consistent_data, "All Races", 2, n_bootstrap = 5000)
results_all_5yr <- summarize_risk_metrics(consistent_data, "All Races", 5, n_bootstrap = 5000)
results_black_2yr <- summarize_risk_metrics(consistent_data %>% filter(race == "Black or African American"), "Black or African American", 2, n_bootstrap = 5000)
results_black_5yr <- summarize_risk_metrics(consistent_data %>% filter(race == "Black or African American"), "Black or African American", 5, n_bootstrap = 5000)
results_white_2yr <- summarize_risk_metrics(consistent_data %>% filter(race == "White"), "White", 2, n_bootstrap = 5000)
results_white_5yr <- summarize_risk_metrics(consistent_data %>% filter(race == "White"), "White", 5, n_bootstrap = 5000)
results_asian_2yr <- summarize_risk_metrics(consistent_data %>% filter(race == "Asian"), "Asian", 2, n_bootstrap = 5000)
results_asian_5yr <- summarize_risk_metrics(consistent_data %>% filter(race == "Asian"), "Asian", 5, n_bootstrap = 5000)

# Combine results including "All Races" for the final table
final_results <- bind_rows(
  results_all_2yr, results_all_5yr,
  results_black_2yr, results_black_5yr,
  results_white_2yr, results_white_5yr,
  results_asian_2yr, results_asian_5yr
)

# Exclude "All Races" for the forest plot
final_results_filtered_MDRD <- final_results %>%
  filter(Race != "All Races")

# Display the final results table including "All Races"
print(final_results)

# Function to plot forest plot for metrics
plot_forest <- function(results_df, metric, lower_ci, upper_ci, title, reference_line) {
  ggplot(results_df, aes_string(x = metric, y = "Race")) +
    geom_point(size = 3, shape = 16, color = "blue") +  # Metric point estimates
    geom_errorbarh(aes_string(xmin = lower_ci, xmax = upper_ci), height = 0.2, color = "black") +  # 95% CI
    geom_vline(xintercept = reference_line, linetype = "dashed", color = "red") +  # Reference line
    facet_wrap(~ Time_Horizon, scales = "free_y") +  # Separate panels for each time horizon
    labs(
      title = title,
      x = metric,
      y = "Race"
    ) +
    theme_minimal() +
    theme(
      axis.text.y = element_text(size = 14, face = "bold"),
      axis.text.x = element_text(size = 14),
      strip.text = element_text(size = 23, face = "bold"),
      panel.spacing = unit(1, "lines"),
      plot.title = element_text(size = 18, face = "bold", hjust = 0.5)
    )
}

# Plot the forest plots for AUC and Harrell's C-statistic using the filtered data
forest_plot_auc <- plot_forest(final_results_filtered_MDRD, "AUC", "AUC_Lower_CI", "AUC_Upper_CI", "", reference_line = 0.5)
forest_plot_cstat <- plot_forest(final_results_filtered_MDRD, "Harrell_C_Statistic", "C_Stat_Lower_CI", "C_Stat_Upper_CI", "", reference_line = 0.5)

# Display the forest plots
print(forest_plot_auc)
print(forest_plot_cstat)

# Save the AUC forest plot
ggsave("MDRD_VHR_forest_plot_auc_filtered.png", plot = forest_plot_auc, width = 10, height = 8, dpi = 300)

# Save the Harrell's C-statistic forest plot
ggsave("MDRD_VHR_forest_plot_cstat_filtered.png", plot = forest_plot_cstat, width = 10, height = 8, dpi = 300)

# Stop the parallel backend after use
stopCluster(cl)


```

##### Group AUC Comparison

```{r}
# Load necessary libraries
library(ggplot2)
library(dplyr)

# Assuming final_results_filtered_MDRD, final_results_filtered_CKD2009, and final_results_filtered_CKD2021 
# are the data frames containing the results for each model

# Add a column to identify each model
final_results_filtered_MDRD$Model <- "MDRD"
final_results_filtered_CKD2009$Model <- "CKD-EPI-2009"
final_results_filtered_CKD2021$Model <- "CKD-EPI-2021"

# Combine all the data frames
combined_results <- bind_rows(
  final_results_filtered_MDRD,
  final_results_filtered_CKD2009,
  final_results_filtered_CKD2021
)

# Reorder the levels for the Race factor to start with Black, followed by White, then Asian
combined_results$Race <- factor(combined_results$Race, levels = c("Black or African American", "White", "Asian"))

# Plotting function to create facets by time horizon and race, comparing the AUC of each model
plot_auc_comparison <- function(data) {
  ggplot(data, aes(x = Model, y = AUC, color = Model)) +
    geom_point(size = 6, stroke = 1.5) +  # Larger points with bold outline
    geom_errorbar(aes(ymin = AUC_Lower_CI, ymax = AUC_Upper_CI), width = 0.3, linewidth = 1.5) +  # Thicker error bars
    facet_grid(Time_Horizon ~ Race, scales = "free") +
    labs(
      title = "AUC Comparison Across Models",
      x = "Model",
      y = "AUC"
    ) +
    theme_minimal() +
    theme(
      axis.text.x = element_text(size = 14, angle = 45, hjust = 1, face = "bold"),  # Larger and bold text
      axis.text.y = element_text(size = 14, face = "bold"),  # Larger and bold text
      axis.title.x = element_text(size = 16, face = "bold"),  # Bold axis title
      axis.title.y = element_text(size = 16, face = "bold"),  # Bold axis title
      strip.text = element_text(size = 18, face = "bold"),  # Larger and bold facet labels for Race
      strip.placement = "outside",  # Move strip text outside the plot area
      strip.background = element_blank(),  # Remove default strip background
      plot.title = element_text(size = 20, face = "bold", hjust = 0.5, margin = margin(b = 15)),  # Add space below the title
      legend.title = element_text(size = 14, face = "bold"),  # Bold legend title
      legend.text = element_text(size = 12, face = "bold"),   # Bold legend text
      panel.spacing = unit(1.5, "lines")  # Increase spacing between panels
    ) +
    scale_color_manual(values = c("MDRD" = "blue", "CKD-EPI-2009" = "green", "CKD-EPI-2021" = "red")) +  # Distinct colors
    guides(color = guide_legend(override.aes = list(size = 5)))  # Larger legend key sizes
}

# Create the plot using the combined results
auc_comparison_plot <- plot_auc_comparison(combined_results)

# Display the plot
print(auc_comparison_plot)

# Save the plot as a high-resolution image
ggsave("AUC_Comparison_Facet_Plot.png", plot = auc_comparison_plot, width = 14, height = 8, dpi = 300)

```

##### Group c statistics Comparison

```{r}
# Load necessary libraries
library(ggplot2)
library(dplyr)

# Assuming final_results_filtered_MDRD, final_results_filtered_CKD2009, and final_results_filtered_CKD2021 
# are the data frames containing the results for each model

# Add a column to identify each model
final_results_filtered_MDRD$Model <- "MDRD"
final_results_filtered_CKD2009$Model <- "CKD-EPI-2009"
final_results_filtered_CKD2021$Model <- "CKD-EPI-2021"

# Combine all the data frames
combined_results <- bind_rows(
  final_results_filtered_MDRD,
  final_results_filtered_CKD2009,
  final_results_filtered_CKD2021
)

# Reorder the levels for the Race factor to start with Black, followed by White, then Asian
combined_results$Race <- factor(combined_results$Race, levels = c("Black or African American", "White", "Asian"))

# Plotting function to create facets by time horizon and race, comparing the Harrell's C-statistic of each model
plot_cstat_comparison <- function(data) {
  ggplot(data, aes(x = Model, y = Harrell_C_Statistic, color = Model)) +
    geom_point(size = 4) +
    geom_errorbar(aes(ymin = C_Stat_Lower_CI, ymax = C_Stat_Upper_CI), width = 0.2, linewidth = 1.2) +  # Use linewidth for thicker error bars
    facet_grid(Time_Horizon ~ Race, scales = "free") +
    labs(
      title = "Harrell's C-Statistic Comparison Across Models",
      x = "Model",
      y = "Harrell's C-Statistic"
    ) +
    theme_minimal() +
    theme(
      axis.text.x = element_text(size = 14, angle = 45, hjust = 1, face = "bold"),  # Larger and bold text
      axis.text.y = element_text(size = 14, face = "bold"),  # Larger and bold text
      axis.title.x = element_text(size = 16, face = "bold"),  # Bold axis title
      axis.title.y = element_text(size = 16, face = "bold"),  # Bold axis title
      strip.text = element_text(size = 16, face = "bold"),  # Bold and larger facet labels
      strip.placement = "outside",  # Place strip labels outside the plot area
      strip.background = element_blank(),  # Remove default strip background
      plot.title = element_text(size = 20, face = "bold", hjust = 0.5, margin = margin(b = 20)),  # Add space below the title
      legend.title = element_text(size = 14, face = "bold"),  # Bold legend title
      legend.text = element_text(size = 12, face = "bold"),   # Bold legend text
      panel.spacing = unit(1.5, "lines")  # Increase space between panels for clarity
    ) +
    scale_color_manual(values = c("MDRD" = "blue", "CKD-EPI-2009" = "green", "CKD-EPI-2021" = "red")) +  # Distinct colors
    guides(color = guide_legend(override.aes = list(size = 4)))  # Larger legend key sizes
}

# Create the plot using the combined results
cstat_comparison_plot <- plot_cstat_comparison(combined_results)

# Display the plot
print(cstat_comparison_plot)

# Save the plot as a high-resolution image
ggsave("C_Statistic_Comparison_Facet_Plot.png", plot = cstat_comparison_plot, width = 14, height = 8, dpi = 300)

```

# Model comparison: Observed vs predicted

```{r}
library(knitr)
library(dplyr)
library(tidyr)
library(lubridate)
library(purrr)
library(stringr)

#install.packages("survival")
#install.packages("survminer")

# Load the necessary libraries
library(survival)

# install.packages("RColorBrewer")
library(RColorBrewer)

# Install and load necessary packages
#install.packages("caret")
#install.packages("ggplot2")
library(caret)
library(ggplot2)

# Load necessary libraries
#library(pROC)



# Create the MDRD 60 data frame with updated observed and predicted values
mdrd_greaterthan60 <- data.frame(
  race = c("Asian", "Black or African American", "White"),
  
  # Observed and predicted 2-year event rates from the plot
  observed_2_years = c(28 / 2750, 177 / 14593, 207 / 56992),
  predicted_2_years = c(4 / 2750, 33 / 14593, 62 / 56992),
  
  # Observed and predicted 5-year event rates from the plot
  observed_5_years = c(35 / 2750, 283 / 14593, 321 / 56992),
  predicted_5_years = c(12 / 2750, 88 / 14593, 178 / 56992),
  
  # Counts and totals for observed 2-year data
  observed_count_2_years = c(28, 177, 207),
  observed_total_2_years = c(2750, 14593, 56992),
  
  # Counts and totals for predicted 2-year data
  predicted_count_2_years = c(4, 33, 62),
  predicted_total_2_years = c(2750, 14593, 56992),
  
  # Counts and totals for observed 5-year data
  observed_count_5_years = c(35, 283, 321),
  observed_total_5_years = c(2750, 14593, 56992),
  
  # Counts and totals for predicted 5-year data
  predicted_count_5_years = c(12, 88, 178),
  predicted_total_5_years = c(2750, 14593, 56992)
)



# Save the data frame to a CSV file
#write.csv(mdrd_60, "mdrd_60.csv", row.names = FALSE)


# Create the CKD-EPI-2009-50 data frame with updated observed and predicted values from the image
ckd_epi_2009_greaterthan60 <- data.frame(
  race = c("Asian", "Black or African American", "White"),
  
  # Observed and predicted 2-year event rates from the plot
  observed_2_years = c(28 / 2750, 177 / 14593, 207 / 56992),
  predicted_2_years = c(4 / 2750, 36 / 14593, 64 / 56992),
  
  # Observed and predicted 5-year event rates from the plot
  observed_5_years = c(35 / 2750, 283 / 14593, 321 / 56992),
  predicted_5_years = c(12 / 2750, 98 / 14593, 184 / 56992),
  
  # Counts and totals for observed 2-year data
  observed_count_2_years = c(28, 177, 207),
  observed_total_2_years = c(2750, 14593, 56992),
  
  # Counts and totals for predicted 2-year data
  predicted_count_2_years = c(4, 36, 64),
  predicted_total_2_years = c(2750, 14593, 56992),
  
  # Counts and totals for observed 5-year data
  observed_count_5_years = c(35, 283, 321),
  observed_total_5_years = c(2750, 14593, 56992),
  
  # Counts and totals for predicted 5-year data
  predicted_count_5_years = c(12, 98, 184),
  predicted_total_5_years = c(2750, 14593, 56992)
)


# Save the data frame to a CSV file
#write.csv(ckd_epi_2009_60, "ckd_epi_2009_50.csv", row.names = FALSE)


# Create the CKD-EPI-2021-60 data frame with updated observed and predicted values from the image
ckd_epi_2021_greaterthan60 <- data.frame(
  race = c("Asian", "Black or African American", "White"),
  
  # Observed and predicted 2-year event rates from the plot
  observed_2_years = c(28 / 2750, 177 / 14593, 207 / 56992),
  predicted_2_years = c(4 / 2750, 42 / 14593, 55 / 56992),
  
  # Observed and predicted 5-year event rates from the plot
  observed_5_years = c(35 / 2750, 283 / 14593, 321 / 56992),
  predicted_5_years = c(11 / 2750, 114 / 14593, 155 / 56992),
  
  # Counts and totals for observed 2-year data
  observed_count_2_years = c(28, 177, 207),
  observed_total_2_years = c(2750, 14593, 56992),
  
  # Counts and totals for predicted 2-year data
  predicted_count_2_years = c(4, 42, 55),
  predicted_total_2_years = c(2750, 14593, 56992),
  
  # Counts and totals for observed 5-year data
  observed_count_5_years = c(35, 283, 321),
  observed_total_5_years = c(2750, 14593, 56992),
  
  # Counts and totals for predicted 5-year data
  predicted_count_5_years = c(11, 114, 155),
  predicted_total_5_years = c(2750, 14593, 56992)
)


# Load necessary libraries
library(ggplot2)
library(dplyr)
library(tidyr)

# Combine the datasets into a single data frame with specified model names
combined_data <- bind_rows(
  mdrd_greaterthan60 %>% mutate(model = "MDRD"),
  ckd_epi_2009_greaterthan60 %>% mutate(model = "CKD-EPI 2009"),
  ckd_epi_2021_greaterthan60 %>% mutate(model = "CKD-EPI 2021")
)

# Create a separate column for the observed and predicted counts in the long format
combined_long <- combined_data %>%
  pivot_longer(
    cols = c(observed_2_years, predicted_2_years, observed_5_years, predicted_5_years),
    names_to = "measure_time",
    values_to = "probability"
  ) %>%
  # Split measure_time correctly into measure and time_period
  separate(measure_time, into = c("measure", "time_period"), sep = "_", extra = "drop", fill = "right") %>%
  mutate(
    measure = ifelse(measure == "observed", "Observed", "Predicted"),
    time_period = ifelse(time_period == "2", "2 Years", "5 Years"),
    observed_count = case_when(
      time_period == "2 Years" ~ observed_count_2_years,
      time_period == "5 Years" ~ observed_count_5_years
    ),
    observed_total = case_when(
      time_period == "2 Years" ~ observed_total_2_years,
      time_period == "5 Years" ~ observed_total_5_years
    ),
    predicted_count = case_when(
      time_period == "2 Years" ~ predicted_count_2_years,
      time_period == "5 Years" ~ predicted_count_5_years
    ),
    predicted_total = case_when(
      time_period == "2 Years" ~ predicted_total_2_years,
      time_period == "5 Years" ~ predicted_total_5_years
    ),
    label = ifelse(measure == "Observed", 
                   paste0(observed_count, "/", observed_total), 
                   paste0(predicted_count, "/", predicted_total))
  )

# Ensure that 'model' and 'race' columns are in the correct order
combined_long$model <- factor(combined_long$model, levels = c("MDRD", "CKD-EPI 2009", "CKD-EPI 2021"))
combined_long$race <- factor(combined_long$race, levels = c("White", "Black or African American", "Asian"))

# Create a dataset for the common observed bars for each race and time period
observed_data <- combined_long %>%
  filter(measure == "Observed") %>%
  group_by(race, time_period) %>%
  summarise(probability = mean(probability), observed_count = mean(observed_count), observed_total = mean(observed_total), .groups = 'drop') %>%
  mutate(model = "Observed", measure = "Observed", 
         label = paste0(observed_count, "/", observed_total))  # Add label for observed counts

# Combine observed and predicted data for the plot
combined_for_plot <- bind_rows(combined_long %>% filter(measure == "Predicted"), observed_data)

# Ensure 'model' is ordered correctly with "Observed" appearing first
combined_for_plot$model <- factor(combined_for_plot$model, levels = c("Observed", "MDRD", "CKD-EPI 2009", "CKD-EPI 2021"))

# Define colors for a more professional, grayscale-like palette
colors <- c("Observed" = "black", "Predicted" = "gray70")

# Plot with the common observed bar for each race and time period
final_plot <- ggplot(combined_for_plot, aes(x = model, y = probability * 100, fill = measure)) +
  geom_bar(stat = "identity", position = "dodge", width = 0.6) +  # Reduced bar width for better clarity
  geom_text(aes(label = label), 
            position = position_dodge(width = 0.6), 
            vjust = 1.5, size = 4.5, color = "white", fontface = "bold") +  # Adjust label size, position, and color
  facet_wrap(~ race + time_period, ncol = 2, scales = "free_y", shrink = TRUE) +  # Use 'ncol = 2' to get 2 graphs per row
  labs(
    title = "Observed vs Predicted Probability of Kidney Failure by Race and Model",
    x = "Model Type",
    y = "Probability of Event (%)",
    fill = "Measurement"
  ) +
  scale_fill_manual(values = colors, 
                    labels = c("Observed", "Predicted")) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 12),  # Increase x-axis text size
    axis.text.y = element_text(size = 12),  # Increase y-axis text size
    plot.title = element_text(hjust = 0.5, size = 18, face = "bold"),  # Increase title size and bold it
    legend.position = "bottom",
    legend.text = element_text(size = 12),  # Adjust legend text size
    strip.text = element_text(size = 14, face = "bold"),  # Adjust facet label font size and make it bold
    panel.spacing = unit(1.5, "lines"),  # Increase spacing between facets for clarity
    panel.grid.major = element_blank(),  # Remove major gridlines for a cleaner look
    panel.grid.minor = element_blank(),  # Remove minor gridlines
    axis.line = element_line(color = "black"),  # Add axis lines for better separation
    legend.title = element_text(size = 12, face = "bold")  # Add bold legend title
  ) +
  coord_cartesian(ylim = c(0, NA))  # Let the y-axis automatically adjust per facet

# Display the improved plot
print(final_plot)

# Save the improved plot with increased resolution and adjusted dimensions for publication
ggsave("Lancet_Style_Observed_vs_Predicted_Probability_Figures_Inside_Bars.png", 
       plot = final_plot, dpi = 300, width = 16, height = 12)  # Adjusted width and height for publication

```

# Part 2

# MISSING DATA CHECK

## Remove Features with count less than 10

left with 43 predictors (excluding person_id, disease_status, time_to_event, and only counting 1 eGFR)

```{r}
# Load necessary libraries
library(dplyr)
library(tidyr)

# Load dataset
filtered_person_with_summary <- read.csv("person_with_summary_inrange_indexdate_creatnine.csv")
colnames(filtered_person_with_summary)

# Step 1: Select and rename relevant columns
filtered_person_with_summary <- filtered_person_with_summary %>%
  select(-age_reported,
         -Glomerular.filtration.rate.1.73.sq.M.predicted..Volume.Rate.Area..in.Serum..Plasma.or.Blood.by.Creatinine.based.formula..MDRD.,
         -Tobacco.smoking.status,
         -height,
         -weight,
         -gender,
         -Creatinine..Mass.volume..in.Body.fluid,
         -highest_smoking_status_rank) 

# Step 2: Filter out non-numeric columns and pivot longer
person_long <- filtered_person_with_summary %>%
  select(person_id, disease_status, where(is.numeric)) %>%
  pivot_longer(
    cols = -c(person_id, disease_status),
    names_to = "variable",
    values_to = "value"
  ) %>%
  filter(!is.na(value) & value != 0)  # Remove rows where the value is NA or zero

# Step 3: Count occurrences of disease_status for each variable
status_counts <- person_long %>%
  group_by(variable, disease_status) %>%
  summarise(count = n_distinct(person_id), .groups = 'drop')

# Step 4: Pivot the summary table to a wide format
summary_wide <- status_counts %>%
  pivot_wider(
    names_from = disease_status,
    values_from = count,
    names_prefix = "disease_status_",
    values_fill = list(count = 0)
  )

# Step 5: Filter out variables that have less than a count of 10 for either disease_status_0 or disease_status_1
filtered_summary <- summary_wide %>%
  filter(disease_status_0 >= 10 & disease_status_1 >= 10)

# Step 6: Extract the variable names that passed the filter
remaining_variables <- filtered_summary$variable

# Step 7: Retain only the person_id, disease_status, race, sex_at_birth, ethnicity, and the remaining variables
filtered_person_with_summary <- filtered_person_with_summary %>%
  select(person_id, disease_status, race, sex_at_birth, ethnicity, all_of(remaining_variables))

# Step 8: Identify the dropped variables
all_variables <- names(select(filtered_person_with_summary, where(is.numeric)))
dropped_variables <- setdiff(all_variables, remaining_variables)

# Print the dropped variables if needed
#print(dropped_variables)

# View the updated column names
colnames(filtered_person_with_summary)

```

# Remove Conditions with perfect separation

48 with 43 predictors. No conditions were removed due to perfect separation

```{r}
# Check for perfect separation: columns with all 0s or all 1s
perfect_separation_cols <- filtered_person_with_summary %>%
  select(where(is.numeric)) %>%  # Select numeric columns (conditions are likely numeric binary)
  summarise_all(~ all(. == 0) | all(. == 1)) %>%  # Check for columns that are all 0s or all 1s
  pivot_longer(cols = everything(), names_to = "condition", values_to = "perfect_separation") %>%
  filter(perfect_separation) %>%  # Filter columns with perfect separation
  pull(condition)  # Get the names of these columns

# List the names of conditions with perfect separation
cat("Conditions removed due to perfect separation:\n")
print(perfect_separation_cols)

# Remove these columns from the dataset
filtered_person_with_summary <- filtered_person_with_summary %>%
  select(-all_of(perfect_separation_cols))

# View the updated dataset without conditions that have perfect separation
print(colnames(filtered_person_with_summary))

```

# Check for multicolinearity

```{r}
# Load necessary libraries
library(dplyr)
library(corrr)
library(knitr)
library(kableExtra)

# Assuming filtered_person_with_summary is the dataset after filtration

# Step 1: Ensure that only numeric columns with variance are checked for correlation
filtered_data_no_constant <- filtered_person_with_summary %>%
  select(-person_id, -disease_status) %>%
  select(where(is.numeric)) %>%  # Ensure we are only dealing with numeric columns
  select_if(~ sd(., na.rm = TRUE) > 0)  # Ensure non-zero standard deviation

# Step 2: Calculate the correlation matrix for the filtered dataset
correlation_matrix <- filtered_data_no_constant %>%
  correlate(method = "pearson", use = "pairwise.complete.obs")

# Step 3: Identify pairs of variables with high correlations (e.g., |correlation| > 0.9)
highly_correlated_pairs <- correlation_matrix %>%
  stretch() %>%  # Convert the correlation matrix into a long format
  filter(abs(r) > 0.9, r != 1) %>%  # Keep only pairs with |correlation| > 0.9 and not self-correlation
  mutate(pair = paste(pmin(x, y), pmax(x, y), sep = "_")) %>%  # Create a unique identifier for pairs
  distinct(pair, .keep_all = TRUE) %>%  # Remove duplicates (e.g., (x,y) and (y,x))
  select(-pair)  # Remove the auxiliary 'pair' column

# Step 4: Prepare the table for the number of observations for each correlated pair
if (nrow(highly_correlated_pairs) > 0) {
  result_table <- data.frame(
    `Variable 1` = character(),
    `Variable 2` = character(),
    `Correlation` = numeric(),
    `Cases (Var 1)` = integer(),
    `Controls (Var 1)` = integer(),
    `Cases (Var 2)` = integer(),
    `Controls (Var 2)` = integer(),
    stringsAsFactors = FALSE
  )
  
  for (i in 1:nrow(highly_correlated_pairs)) {
    var1 <- highly_correlated_pairs$x[i]
    var2 <- highly_correlated_pairs$y[i]
    correlation <- highly_correlated_pairs$r[i]
    
    # Number of non-zero cases (disease_status == 1) and non-zero controls (disease_status == 0) for var1
    cases_var1 <- sum(filtered_person_with_summary$disease_status == 1 & 
                      filtered_person_with_summary[[var1]] != 0 & !is.na(filtered_person_with_summary[[var1]]))
    controls_var1 <- sum(filtered_person_with_summary$disease_status == 0 & 
                         filtered_person_with_summary[[var1]] != 0 & !is.na(filtered_person_with_summary[[var1]]))
    
    # Number of non-zero cases (disease_status == 1) and non-zero controls (disease_status == 0) for var2
    cases_var2 <- sum(filtered_person_with_summary$disease_status == 1 & 
                      filtered_person_with_summary[[var2]] != 0 & !is.na(filtered_person_with_summary[[var2]]))
    controls_var2 <- sum(filtered_person_with_summary$disease_status == 0 & 
                         filtered_person_with_summary[[var2]] != 0 & !is.na(filtered_person_with_summary[[var2]]))
    
    # Add the results to the table
    result_table <- rbind(result_table, data.frame(
      `Variable 1` = var1,
      `Variable 2` = var2,
      `Correlation` = round(correlation, 2),  # Round correlation to 2 decimal places
      `Cases (Var 1)` = cases_var1,
      `Controls (Var 1)` = controls_var1,
      `Cases (Var 2)` = cases_var2,
      `Controls (Var 2)` = controls_var2
    ))
  }
  
  # Print the result table with formatting for publication
  print(result_table)
} else {
  cat("No highly correlated variable pairs found (|correlation| > 0.9).\n")
}

# Print the column names of the original dataset
print(colnames(filtered_person_with_summary))

```

### Remove correlated data

```{r}
filtered_person_with_summary <- filtered_person_with_summary %>%
  select(-Iron.binding.capacity..Mass.volume..in.Serum.or.Plasma, -eGFR_ckd_epi_2009)

```

Remove MDRD eGFR

```{r}
filtered_person_with_summary <- filtered_person_with_summary %>%
  select(-eGFR_MDRD)
```

### Recheck for multicolinearity

```{r}
# Load necessary libraries
library(dplyr)
library(corrr)
library(knitr)
library(kableExtra)

# Assuming filtered_person_with_summary is the dataset after filtration

# Step 1: Ensure that only numeric columns are checked for variance
filtered_data_no_constant <- filtered_person_with_summary %>%
  select(-person_id, -disease_status) %>%
  select(where(is.numeric)) %>%  # Ensure we are only dealing with numeric columns
  select_if(~ !all(is.na(.)) && sd(., na.rm = TRUE) > 0)  # Ensure non-zero standard deviation, no NA values, and proper data type

# Step 2: Calculate the correlation matrix for the filtered dataset
correlation_matrix <- filtered_data_no_constant %>%
  correlate(method = "pearson", use = "pairwise.complete.obs")

# Step 3: Identify pairs of variables with high correlations (e.g., |correlation| > 0.9)
highly_correlated_pairs <- correlation_matrix %>%
  stretch() %>%  # Convert the correlation matrix into a long format
  filter(abs(r) > 0.9, r != 1)  # Keep only pairs with |correlation| > 0.9 and not self-correlation

# Step 4: Prepare the table for the number of observations for each correlated pair
if (nrow(highly_correlated_pairs) > 0) {
  result_table <- data.frame(
    `Variable 1` = character(),
    `Variable 2` = character(),
    `Correlation` = numeric(),
    `Cases (Var 1)` = integer(),
    `Controls (Var 1)` = integer(),
    `Cases (Var 2)` = integer(),
    `Controls (Var 2)` = integer(),
    stringsAsFactors = FALSE
  )
  
  for (i in 1:nrow(highly_correlated_pairs)) {
    var1 <- highly_correlated_pairs$x[i]
    var2 <- highly_correlated_pairs$y[i]
    correlation <- highly_correlated_pairs$r[i]
    
    # Number of non-zero cases (disease_status == 1) and non-zero controls (disease_status == 0) for var1
    cases_var1 <- sum(filtered_person_with_summary$disease_status == 1 & 
                      filtered_person_with_summary[[var1]] != 0 & !is.na(filtered_person_with_summary[[var1]]))
    controls_var1 <- sum(filtered_person_with_summary$disease_status == 0 & 
                         filtered_person_with_summary[[var1]] != 0 & !is.na(filtered_person_with_summary[[var1]]))
    
    # Number of non-zero cases (disease_status == 1) and non-zero controls (disease_status == 0) for var2
    cases_var2 <- sum(filtered_person_with_summary$disease_status == 1 & 
                      filtered_person_with_summary[[var2]] != 0 & !is.na(filtered_person_with_summary[[var2]]))
    controls_var2 <- sum(filtered_person_with_summary$disease_status == 0 & 
                         filtered_person_with_summary[[var2]] != 0 & !is.na(filtered_person_with_summary[[var2]]))
    
    # Add the results to the table
    result_table <- rbind(result_table, data.frame(
      `Variable 1` = var1,
      `Variable 2` = var2,
      `Correlation` = round(correlation, 2),  # Round correlation to 2 decimal places
      `Cases (Var 1)` = cases_var1,
      `Controls (Var 1)` = controls_var1,
      `Cases (Var 2)` = cases_var2,
      `Controls (Var 2)` = controls_var2
    ))
  }
  
  # Print the result table with formatting for publication
  print(result_table)
} else {
  cat("No highly correlated variable pairs found (|correlation| > 0.9).\n")
}

# Print the column names of the original dataset
print(colnames(filtered_person_with_summary))

# Save the data frame to a CSV file
write.csv(filtered_person_with_summary, "filtered_person_with_summary_ml.csv", row.names = FALSE)


```

40 predictors

```{r}
# Load necessary libraries
library(dplyr)
colnames(filtered_person_with_summary)
# Assuming the race column is named 'race' in your dataset 'data_ml'
# Check the distribution of races in the 'race' column
race_distribution <- filtered_person_with_summary %>%
  group_by(race, disease_status) %>%
  summarise(count = n()) %>%
  arrange(desc(count))

# Print the race distribution
print(race_distribution)

# Save the filtered_person_with_summary dataframe as refiltered_person_with_summary.csv
write.csv(filtered_person_with_summary, file = "refiltered_person_with_summary.csv", row.names = FALSE)

```

Data cleaning

```{r}
#filtered_person_with_summary <- read.csv("refiltered_person_with_summary.csv")

# # Create binary columns for creatinine strata based on sex_at_birth and serum_creatinine
# filtered_person_with_summary$Low_Creatinine <- ifelse(
#   (filtered_person_with_summary$sex_at_birth == "Male" & filtered_person_with_summary$serum_creatinine < 0.7) |
#   (filtered_person_with_summary$sex_at_birth == "Female" & filtered_person_with_summary$serum_creatinine < 0.6), 1, 0)
# 
# filtered_person_with_summary$Normal_Creatinine <- ifelse(
#   (filtered_person_with_summary$sex_at_birth == "Male" & filtered_person_with_summary$serum_creatinine >= 0.7 & filtered_person_with_summary$serum_creatinine <= 1.3) |
#   (filtered_person_with_summary$sex_at_birth == "Female" & filtered_person_with_summary$serum_creatinine >= 0.6 & filtered_person_with_summary$serum_creatinine <= 1.1), 1, 0)
# 
# filtered_person_with_summary$High_Creatinine <- ifelse(
#   (filtered_person_with_summary$sex_at_birth == "Male" & filtered_person_with_summary$serum_creatinine > 1.3) |
#   (filtered_person_with_summary$sex_at_birth == "Female" & filtered_person_with_summary$serum_creatinine > 1.1), 1, 0)
# 
# # Remove the original serum_creatinine column
# filtered_person_with_summary <- filtered_person_with_summary[, !colnames(filtered_person_with_summary) %in% "serum_creatinine"]

# 
# # Create binary variables for CKD stages based on eGFR values
# filtered_person_with_summary$eGFR_Normal_or_Stage1 <- ifelse(filtered_person_with_summary$eGFR_ckd_epi_2021 >= 90, 1, 0)
# filtered_person_with_summary$eGFR_Stage2 <- ifelse(filtered_person_with_summary$eGFR_ckd_epi_2021 >= 60 & filtered_person_with_summary$eGFR_ckd_epi_2021 < 90, 1, 0)
# filtered_person_with_summary$eGFR_Stage3a <- ifelse(filtered_person_with_summary$eGFR_ckd_epi_2021 >= 45 & filtered_person_with_summary$eGFR_ckd_epi_2021 < 60, 1, 0)
# filtered_person_with_summary$eGFR_Stage3b <- ifelse(filtered_person_with_summary$eGFR_ckd_epi_2021 >= 30 & filtered_person_with_summary$eGFR_ckd_epi_2021 < 45, 1, 0)
# filtered_person_with_summary$eGFR_Stage4 <- ifelse(filtered_person_with_summary$eGFR_ckd_epi_2021 >= 15 & filtered_person_with_summary$eGFR_ckd_epi_2021 < 30, 1, 0)
# filtered_person_with_summary$eGFR_Stage5 <- ifelse(filtered_person_with_summary$eGFR_ckd_epi_2021 < 15, 1, 0)
# 
# # Remove the original eGFR_ckd_epi_2021 column
# filtered_person_with_summary <- filtered_person_with_summary[, !colnames(filtered_person_with_summary) %in% "eGFR_ckd_epi_2021"]
# 
# 

# # Remove the predictor Ferritin..Mass.volume..in.Serum.or.Plasma from the dataset
# filtered_person_with_summary <- filtered_person_with_summary[, !(colnames(filtered_person_with_summary) == "Ferritin..Mass.volume..in.Serum.or.Plasma")]



# # Save the filtered_person_with_summary dataframe as refiltered_person_with_summary.csv
# write.csv(filtered_person_with_summary, file = "rerefiltered_person_with_summary.csv", row.names = FALSE)

```

# XG Boost Model

## Data preparation

```{r}
library(knitr)
library(dplyr)
library(tidyr)
library(lubridate)
library(purrr)
library(stringr)

#install.packages("survival")
#install.packages("survminer")

# Load the necessary libraries
library(survival)

# install.packages("RColorBrewer")
library(RColorBrewer)

# Install and load necessary packages
#install.packages("caret")
#install.packages("ggplot2")
library(caret)
library(ggplot2)

# Load necessary libraries
#library(pROC)

filtered_person_with_summary <- read.csv("rerefiltered_person_with_summary.csv")
# Filter the dataset to only include observations with eGFR_ckd_epi_2021 < 60
#filtered_person_with_summary <- filtered_person_with_summary %>% filter(eGFR_ckd_epi_2021 < 60)

# Create 'SexMale' and 'SexFemale' columns based on 'sex_at_birth'
filtered_person_with_summary$SexMale <- ifelse(filtered_person_with_summary$sex_at_birth == "Male", 1, 0)
filtered_person_with_summary$SexFemale <- ifelse(filtered_person_with_summary$sex_at_birth == "Female", 1, 0)

# Remove the original 'sex_at_birth' column
filtered_person_with_summary <- filtered_person_with_summary[, !colnames(filtered_person_with_summary) %in% "sex_at_birth"]

colnames(filtered_person_with_summary)
# Load necessary libraries
library(dplyr)
library(xgboost)
library(survival)
# Step 1: Replace all NA values in the dataset with 0
filtered_person_with_summary[is.na(filtered_person_with_summary)] <- 0

# Step 2: Ensure there are no more NA values
if (sum(is.na(filtered_person_with_summary)) != 0) {
  stop("There are still NA values in the data.")
}

# Step 3: Define the 5-year and 2-year time limits in days
five_years_in_days <- 5 * 365.25  # 1826.25 days
two_years_in_days <- 2 * 365.25   # 730.5 days

# Step 4: Censor or filter data for 5-year and 2-year events based on `time_to_event`
if (!"time_to_event" %in% colnames(filtered_person_with_summary)) {
  stop("Error: time_to_event column not found in the dataset.")
}

# Create event_5yr and time_to_event_5yr
filtered_person_with_summary$event_5yr <- ifelse(filtered_person_with_summary$time_to_event <= five_years_in_days & 
                                                 filtered_person_with_summary$disease_status == 1, 1, 0)
filtered_person_with_summary$time_to_event_5yr <- pmin(filtered_person_with_summary$time_to_event, five_years_in_days)

# Create event_2yr and time_to_event_2yr
filtered_person_with_summary$event_2yr <- ifelse(filtered_person_with_summary$time_to_event <= two_years_in_days & 
                                                 filtered_person_with_summary$disease_status == 1, 1, 0)
filtered_person_with_summary$time_to_event_2yr <- pmin(filtered_person_with_summary$time_to_event, two_years_in_days)

# Step 5: Remove the `time_to_event` column as it's no longer needed
filtered_person_with_summary <- subset(filtered_person_with_summary, select = -c(time_to_event))

# Step 6: Normalize numeric variables using log(variable + 1)
predictor_cols <- colnames(filtered_person_with_summary)[5:41]  # Adjust index for numeric columns
filtered_person_with_summary[predictor_cols] <- log(filtered_person_with_summary[predictor_cols] + 1)

# Step 7: Create binary race variables based on the race column
filtered_person_with_summary$raceBlack <- ifelse(filtered_person_with_summary$race == "Black or African American", 1, 0)
filtered_person_with_summary$raceWhite <- ifelse(filtered_person_with_summary$race == "White", 1, 0)
filtered_person_with_summary$raceAsian <- ifelse(filtered_person_with_summary$race == "Asian", 0, 0)  # Put 0 for train, Asians will only be in test set

# Step 8: Split Black and White data into 70% training and 30% test
set.seed(123)  # For reproducibility
# Splitting Black data
sample_size_black <- floor(0.7 * nrow(subset(filtered_person_with_summary, raceBlack == 1)))
train_indices_black <- sample(seq_len(nrow(subset(filtered_person_with_summary, raceBlack == 1))), size = sample_size_black)
train_data_black <- subset(filtered_person_with_summary, raceBlack == 1)[train_indices_black, ]
test_data_black <- subset(filtered_person_with_summary, raceBlack == 1)[-train_indices_black, ]

# Splitting White data
sample_size_white <- floor(0.7 * nrow(subset(filtered_person_with_summary, raceWhite == 1)))
train_indices_white <- sample(seq_len(nrow(subset(filtered_person_with_summary, raceWhite == 1))), size = sample_size_white)
train_data_white <- subset(filtered_person_with_summary, raceWhite == 1)[train_indices_white, ]
test_data_white <- subset(filtered_person_with_summary, raceWhite == 1)[-train_indices_white, ]

# Step 9: For Asians, 100% of data goes to the test set
test_data_asian <- subset(filtered_person_with_summary, race == "Asian")
test_data_asian$raceAsian <- 1  # Mark Asians for test

# Step 10: Combine the training data (only Black and White individuals)
train_data <- rbind(train_data_black, train_data_white)

# Step 11: Combine the test data (30% Black, 30% White, and 100% Asian)
test_data <- rbind(test_data_black, test_data_white, test_data_asian)

# Step 12: Filter out non-positive event times from training and test data
# For 2-year training data
valid_indices_train_2yr <- train_data$time_to_event_2yr > 0  # Keep only rows with positive time_to_event
train_data_2yr <- train_data[valid_indices_train_2yr, ]

# For 2-year test data
valid_indices_test_2yr <- test_data$time_to_event_2yr > 0
test_data_2yr <- test_data[valid_indices_test_2yr, ]

# For 5-year training data
valid_indices_train_5yr <- train_data$time_to_event_5yr > 0
train_data_5yr <- train_data[valid_indices_train_5yr, ]

# For 5-year test data
valid_indices_test_5yr <- test_data$time_to_event_5yr > 0
test_data_5yr <- test_data[valid_indices_test_5yr, ]

# Step 13: Create model matrices for 2-year and 5-year predictions

# Exclude the original "race" column to avoid duplicates
non_predictor_cols <- c("person_id", "disease_status", "event_2yr", "event_5yr", "time_to_event_2yr", "time_to_event_5yr", "race")

# 2-year training data (including the smoked_100_Cigs_lifetime variable)
X_train_2yr <- model.matrix(~ raceBlack + raceWhite + raceAsian + ethnicity + Acidosis + 
                              Acute.renal.failure.syndrome + Anemia + BMI + smoked_100_Cigs_lifetime + . - 1, 
                            data = train_data_2yr[, !(colnames(train_data_2yr) %in% non_predictor_cols)])
y_train_2yr <- train_data_2yr$event_2yr
time_train_2yr <- train_data_2yr$time_to_event_2yr

# 2-year test data (including the smoked_100_Cigs_lifetime variable)
X_test_2yr <- model.matrix(~ raceBlack + raceWhite + raceAsian + ethnicity + Acidosis + 
                             Acute.renal.failure.syndrome + Anemia + BMI + smoked_100_Cigs_lifetime + . - 1, 
                           data = test_data_2yr[, !(colnames(test_data_2yr) %in% non_predictor_cols)])
y_test_2yr <- test_data_2yr$event_2yr
time_test_2yr <- test_data_2yr$time_to_event_2yr

# 5-year training data (including the smoked_100_Cigs_lifetime variable)
X_train_5yr <- model.matrix(~ raceBlack + raceWhite + raceAsian + ethnicity + Acidosis + 
                              Acute.renal.failure.syndrome + Anemia + BMI + smoked_100_Cigs_lifetime + . - 1, 
                            data = train_data_5yr[, !(colnames(train_data_5yr) %in% non_predictor_cols)])
y_train_5yr <- train_data_5yr$event_5yr
time_train_5yr <- train_data_5yr$time_to_event_5yr

# 5-year test data (including the smoked_100_Cigs_lifetime variable)
X_test_5yr <- model.matrix(~ raceBlack + raceWhite + raceAsian + ethnicity + Acidosis + 
                             Acute.renal.failure.syndrome + Anemia + BMI + smoked_100_Cigs_lifetime + . - 1, 
                           data = test_data_5yr[, !(colnames(test_data_5yr) %in% non_predictor_cols)])
y_test_5yr <- test_data_5yr$event_5yr
time_test_5yr <- test_data_5yr$time_to_event_5yr

# Step 14: Print cases and controls with non-positive time to event for train and test data
# For 2-year training data
non_positive_train_2yr <- train_data[train_data$time_to_event_2yr <= 0, ]
non_positive_cases_train_2yr <- sum(non_positive_train_2yr$event_2yr == 1)
non_positive_controls_train_2yr <- sum(non_positive_train_2yr$event_2yr == 0)

# For 2-year test data
non_positive_test_2yr <- test_data[test_data$time_to_event_2yr <= 0, ]
non_positive_cases_test_2yr <- sum(non_positive_test_2yr$event_2yr == 1)
non_positive_controls_test_2yr <- sum(non_positive_test_2yr$event_2yr == 0)

# For 5-year training data
non_positive_train_5yr <- train_data[train_data$time_to_event_5yr <= 0, ]
non_positive_cases_train_5yr <- sum(non_positive_train_5yr$event_5yr == 1)
non_positive_controls_train_5yr <- sum(non_positive_train_5yr$event_5yr == 0)

# For 5-year test data
non_positive_test_5yr <- test_data[test_data$time_to_event_5yr <= 0, ]
non_positive_cases_test_5yr <- sum(non_positive_test_5yr$event_5yr == 1)
non_positive_controls_test_5yr <- sum(non_positive_test_5yr$event_5yr == 0)

# Step 15: Print the results
cat("2-Year Training Data: \n")
cat("Non-positive cases:", non_positive_cases_train_2yr, "\n")
cat("Non-positive controls:", non_positive_controls_train_2yr, "\n")
cat("Unique non-positive time to events:", unique(non_positive_train_2yr$time_to_event_2yr), "\n\n")

cat("2-Year Test Data: \n")
cat("Non-positive cases:", non_positive_cases_test_2yr, "\n")
cat("Non-positive controls:", non_positive_controls_test_2yr, "\n")
cat("Unique non-positive time to events:", unique(non_positive_test_2yr$time_to_event_2yr), "\n\n")

cat("5-Year Training Data: \n")
cat("Non-positive cases:", non_positive_cases_train_5yr, "\n")
cat("Non-positive controls:", non_positive_controls_train_5yr, "\n")
cat("Unique non-positive time to events:", unique(non_positive_train_5yr$time_to_event_5yr), "\n\n")

cat("5-Year Test Data: \n")
cat("Non-positive cases:", non_positive_cases_test_5yr, "\n")
cat("Non-positive controls:", non_positive_controls_test_5yr, "\n")
cat("Unique non-positive time to events:", unique(non_positive_test_5yr$time_to_event_5yr), "\n\n")

```

## Testing Proportional hazards assumptions: Multivariate

```{r}

# Load necessary libraries
library(survival)
library(dplyr)

# Function to test proportional hazards assumption and return a table with results
test_proportional_hazards_multivariate <- function(cox_model, outcome_type) {
  # Perform the proportional hazards assumption test
  ph_test <- cox.zph(cox_model)
  
  # Extract the p-values from the test
  ph_results <- ph_test$table[, "p"]
  
  # Create a data frame to store the covariate names and their PH assumption results
  results_df <- data.frame(
    Covariate = rownames(ph_test$table),
    PH_Assumption = ifelse(ph_results >= 0.05, "Holds", "Violated")
  )
  
  # Print the result table
  cat("\nProportional Hazards Assumption Test Results for", outcome_type, "Outcome:\n")
  print(results_df)
  
  return(results_df)  # Return the table for further use if needed
}

# Fit a multivariate Cox proportional hazards model for 2-year outcome
cox_model_2yr <- coxph(
  Surv(time_to_event_2yr, event_2yr) ~ race + ethnicity + Acidosis + Acute.renal.failure.syndrome +
    Anemia + Anemia.in.chronic.kidney.disease + Atherosclerosis.of.coronary.artery.without.angina.pectoris +
    BMI + Chronic.kidney.disease.stage.3 + Chronic.kidney.disease.stage.4 + Congestive.heart.failure +
     Diabetes.mellitus + Disorder.of.kidney.and.or.ureter + Disorder.of.muscle +
    Erythrocyte.distribution.width..Ratio..by.Automated.count + Essential.hypertension + Gout + 
    Hemoglobin.A1c.Hemoglobin.total.in.Blood + Hepatitis.B.virus.surface.Ag..Presence..in.Serum.or.Plasma.by.Immunoassay +
    Hyperkalemia + Hypothyroidism + Iron..Mass.volume..in.Serum.or.Plasma + Iron.deficiency.anemia + 
    Iron.saturation..Mass.Fraction..in.Serum.or.Plasma + Parathyrin.intact..Mass.volume..in.Serum.or.Plasma +
    Peripheral.vascular.disease + Polyneuropathy.due.to.diabetes.mellitus + Protein..Mass.volume..in.Urine + Proteinuria +
    Renal.disorder.due.to.type.2.diabetes.mellitus + Systemic.lupus.erythematosus + Systolic.blood.pressure + 
    Transplanted.kidney.present + Triglyceride..Mass.volume..in.Serum.or.Plasma + Type.2.diabetes.mellitus +
    age_precise + smoked_100_Cigs_lifetime  +
    SexFemale + SexMale,
  data = filtered_person_with_summary
)

# Test proportional hazards assumption for the 2-year model
ph_violation_table_2yr <- test_proportional_hazards_multivariate(cox_model_2yr, "2-Year")


# Fit a multivariate Cox proportional hazards model for 5-year outcome
cox_model_5yr <- coxph(
  Surv(time_to_event_5yr, event_5yr) ~ race + ethnicity + Acidosis + Acute.renal.failure.syndrome +
    Anemia + Anemia.in.chronic.kidney.disease + Atherosclerosis.of.coronary.artery.without.angina.pectoris +
    BMI + Chronic.kidney.disease.stage.3 + Chronic.kidney.disease.stage.4 + Congestive.heart.failure +
     Diabetes.mellitus + Disorder.of.kidney.and.or.ureter + Disorder.of.muscle +
    Erythrocyte.distribution.width..Ratio..by.Automated.count + Essential.hypertension + Gout + 
    Hemoglobin.A1c.Hemoglobin.total.in.Blood + Hepatitis.B.virus.surface.Ag..Presence..in.Serum.or.Plasma.by.Immunoassay +
    Hyperkalemia + Hypothyroidism + Iron..Mass.volume..in.Serum.or.Plasma + Iron.deficiency.anemia + 
    Iron.saturation..Mass.Fraction..in.Serum.or.Plasma + Parathyrin.intact..Mass.volume..in.Serum.or.Plasma +
    Peripheral.vascular.disease + Polyneuropathy.due.to.diabetes.mellitus + Protein..Mass.volume..in.Urine + Proteinuria +
    Renal.disorder.due.to.type.2.diabetes.mellitus + Systemic.lupus.erythematosus + Systolic.blood.pressure + 
    Transplanted.kidney.present + Triglyceride..Mass.volume..in.Serum.or.Plasma + Type.2.diabetes.mellitus +
    age_precise + smoked_100_Cigs_lifetime +
    SexFemale + SexMale,
  data = filtered_person_with_summary
)

# Test proportional hazards assumption for the 5-year model
ph_violation_table_5yr <- test_proportional_hazards_multivariate(cox_model_5yr, "5-Year")

```

### 

#### Counts before cleaning

```{r}
# Load necessary library for better data manipulation
library(dplyr)

# Define the race groups we are interested in
race_groups <- c("White", "Black or African American", "Asian")

# Function to calculate the number of cases and controls for both 2-year and 5-year events
count_cases_controls <- function(data, race_variable, event_2yr_variable, event_5yr_variable) {
  
  # Initialize an empty result data frame to store results
  result <- data.frame(Race = race_groups, 
                       Cases_2yr = integer(length(race_groups)), Controls_2yr = integer(length(race_groups)),
                       Cases_5yr = integer(length(race_groups)), Controls_5yr = integer(length(race_groups)))

  # Loop through each race group
  for (race in race_groups) {
    
    # Filter the data for the current race
    race_data <- data[data[[race_variable]] == race, ]
    
    # Calculate the number of 2-year cases and controls
    num_cases_2yr <- sum(race_data[[event_2yr_variable]] == 1, na.rm = TRUE)
    num_controls_2yr <- sum(race_data[[event_2yr_variable]] == 0, na.rm = TRUE)
    
    # Calculate the number of 5-year cases and controls
    num_cases_5yr <- sum(race_data[[event_5yr_variable]] == 1, na.rm = TRUE)
    num_controls_5yr <- sum(race_data[[event_5yr_variable]] == 0, na.rm = TRUE)
    
    # Store the results in the result data frame for each race group
    result[result$Race == race, "Cases_2yr"] <- num_cases_2yr
    result[result$Race == race, "Controls_2yr"] <- num_controls_2yr
    result[result$Race == race, "Cases_5yr"] <- num_cases_5yr
    result[result$Race == race, "Controls_5yr"] <- num_controls_5yr
  }
  
  # Add a row for the total (across all races)
  total_cases_2yr <- sum(data[[event_2yr_variable]] == 1, na.rm = TRUE)
  total_controls_2yr <- sum(data[[event_2yr_variable]] == 0, na.rm = TRUE)
  total_cases_5yr <- sum(data[[event_5yr_variable]] == 1, na.rm = TRUE)
  total_controls_5yr <- sum(data[[event_5yr_variable]] == 0, na.rm = TRUE)
  
  result <- rbind(result, 
                  data.frame(Race = "Total", 
                             Cases_2yr = total_cases_2yr, Controls_2yr = total_controls_2yr,
                             Cases_5yr = total_cases_5yr, Controls_5yr = total_controls_5yr))
  
  return(result)
}

# Calculate cases and controls for the training data for both 2-year and 5-year events
train_summary <- count_cases_controls(train_data, "race", "event_2yr", "event_5yr")

# Calculate cases and controls for the test data for both 2-year and 5-year events
test_summary <- count_cases_controls(test_data, "race", "event_2yr", "event_5yr")

# Combine the results for both training and test data into one dataset
combined_summary <- list(Train = train_summary, Test = test_summary)

# Print the results
print("Training Data Summary (2-year and 5-year):")
print(combined_summary$Train)

print("Test Data Summary (2-year and 5-year):")
print(combined_summary$Test)

```

#### Counts after cleaning

```{r}
# Load necessary library for better data manipulation
library(dplyr)

# Define the race groups we are interested in
race_groups <- c("White", "Black or African American", "Asian")

# Function to calculate the number of cases and controls for both 2-year and 5-year events, with positive time-to-event
count_cases_controls_positive_time <- function(data, race_variable, event_2yr_variable, event_5yr_variable, time_2yr_variable, time_5yr_variable) {
  
  # Initialize an empty result data frame to store results
  result <- data.frame(Race = race_groups, 
                       Cases_2yr_Positive = integer(length(race_groups)), Controls_2yr_Positive = integer(length(race_groups)),
                       Cases_5yr_Positive = integer(length(race_groups)), Controls_5yr_Positive = integer(length(race_groups)))
  
  # Loop through each race group
  for (race in race_groups) {
    
    # Filter the data for the current race and keep only rows with positive time-to-event
    race_data_2yr <- data[data[[race_variable]] == race & data[[time_2yr_variable]] > 0, ]
    race_data_5yr <- data[data[[race_variable]] == race & data[[time_5yr_variable]] > 0, ]
    
    # Calculate the number of 2-year cases and controls with positive time-to-event
    num_cases_2yr <- sum(race_data_2yr[[event_2yr_variable]] == 1, na.rm = TRUE)
    num_controls_2yr <- sum(race_data_2yr[[event_2yr_variable]] == 0, na.rm = TRUE)
    
    # Calculate the number of 5-year cases and controls with positive time-to-event
    num_cases_5yr <- sum(race_data_5yr[[event_5yr_variable]] == 1, na.rm = TRUE)
    num_controls_5yr <- sum(race_data_5yr[[event_5yr_variable]] == 0, na.rm = TRUE)
    
    # Store the results in the result data frame for each race group
    result[result$Race == race, "Cases_2yr_Positive"] <- num_cases_2yr
    result[result$Race == race, "Controls_2yr_Positive"] <- num_controls_2yr
    result[result$Race == race, "Cases_5yr_Positive"] <- num_cases_5yr
    result[result$Race == race, "Controls_5yr_Positive"] <- num_controls_5yr
  }
  
  # Add a row for the total (across all races) with positive time-to-event
  total_cases_2yr <- sum(data[[event_2yr_variable]] == 1 & data[[time_2yr_variable]] > 0, na.rm = TRUE)
  total_controls_2yr <- sum(data[[event_2yr_variable]] == 0 & data[[time_2yr_variable]] > 0, na.rm = TRUE)
  total_cases_5yr <- sum(data[[event_5yr_variable]] == 1 & data[[time_5yr_variable]] > 0, na.rm = TRUE)
  total_controls_5yr <- sum(data[[event_5yr_variable]] == 0 & data[[time_5yr_variable]] > 0, na.rm = TRUE)
  
  result <- rbind(result, 
                  data.frame(Race = "Total", 
                             Cases_2yr_Positive = total_cases_2yr, Controls_2yr_Positive = total_controls_2yr,
                             Cases_5yr_Positive = total_cases_5yr, Controls_5yr_Positive = total_controls_5yr))
  
  return(result)
}

# Calculate cases and controls with positive time-to-event for the training data
train_summary <- count_cases_controls_positive_time(train_data, "race", "event_2yr", "event_5yr", "time_to_event_2yr", "time_to_event_5yr")

# Calculate cases and controls with positive time-to-event for the test data
test_summary <- count_cases_controls_positive_time(test_data, "race", "event_2yr", "event_5yr", "time_to_event_2yr", "time_to_event_5yr")

# Combine the results for both training and test data into one dataset
combined_summary <- list(Train = train_summary, Test = test_summary)

# Print the results
cat("Training Data Summary (2-year and 5-year with positive time-to-event):\n")
print(combined_summary$Train)

cat("\nTest Data Summary (2-year and 5-year with positive time-to-event):\n")
print(combined_summary$Test)

```

## Race specific model

```{r}
# Load necessary libraries
library(xgboost)
library(pROC)
library(Hmisc)  # For Harrell's C-statistic
library(survival)
library(doParallel)
library(ggplot2)
library(dplyr)
library(caret)  # For cross-validation and parameter tuning
library(boot)   # For bootstrapping AUC confidence intervals

# Set up parallel backend for faster execution
cores <- detectCores() - 2  # Leave two cores free
registerDoParallel(cores)

# Set random seed for reproducibility
set.seed(123)

# Function to normalize data using log(x + 1)
normalize_data <- function(df) {
  return(log(df + 1))
}

# Define a robust but efficient hyperparameter grid for tuning
param_grid <- expand.grid(
  max_depth = c(4, 6),  
  min_child_weight = c(1, 2),  
  gamma = c(0, 0.2),  
  eta = c(0.05, 0.1),  
  subsample = c(0.8, 0.9),  
  colsample_bytree = c(0.8, 1),  
  alpha = c(0, 0.5),  
  lambda = c(1, 1.2),  
  n_estimators = c(300, 500)  
)

# Function for hyperparameter tuning using grid search with cross-validation
grid_search_xgboost <- function(X_train, time_train, event_status_train, X_val, time_val, event_status_val, param_grid, nfold = 5, early_stopping_rounds = 5) {
  best_score <- Inf
  best_model <- NULL
  best_params <- NULL
  
  # Convert training and validation data to DMatrix for XGBoost
  dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = time_train)
  dval <- xgb.DMatrix(data = as.matrix(X_val), label = time_val)
  setinfo(dtrain, "base_margin", event_status_train)
  setinfo(dval, "base_margin", event_status_val)
  
  # Watchlist to monitor training and validation
  watchlist <- list(train = dtrain, val = dval)
  
  # Iterate over each combination of parameters in the grid
  for (i in 1:nrow(param_grid)) {
    params <- list(
      max_depth = param_grid$max_depth[i],
      min_child_weight = param_grid$min_child_weight[i],
      gamma = param_grid$gamma[i],
      eta = param_grid$eta[i],
      subsample = param_grid$subsample[i],
      colsample_bytree = param_grid$colsample_bytree[i],
      alpha = param_grid$alpha[i],
      lambda = param_grid$lambda[i],
      objective = "survival:cox",
      eval_metric = "cox-nloglik",
      nthread = cores
    )
    
    # Train the model with early stopping using the validation set
    model <- xgb.train(
      params = params,
      data = dtrain,
      nrounds = param_grid$n_estimators[i],
      watchlist = watchlist,  
      early_stopping_rounds = early_stopping_rounds,
      verbose = 0
    )
    
    # Get the final evaluation metric (cox-nloglik)
    final_score <- model$best_score
    
    # Update the best model if we find a better score
    if (final_score < best_score) {
      best_score <- final_score
      best_model <- model
      best_params <- params
    }
  }
  
  # Print the best parameters
  print("Best Parameters:")
  print(best_params)
  
  return(list(best_model = best_model, best_params = best_params))
}

# Function to fit XGBoost model with early stopping and return variable importance
fit_xgboost_and_display <- function(X_data, time_to_event, event_status, title_prefix, early_stopping_rounds = 5, nfold = 3, val_size = 0.2) {
  
  # Split the data into training and validation sets
  set.seed(123)
  train_indices <- createDataPartition(event_status, p = 1 - val_size, list = FALSE)
  X_train <- X_data[train_indices, ]
  X_val <- X_data[-train_indices, ]
  time_train <- time_to_event[train_indices]
  time_val <- time_to_event[-train_indices]
  event_status_train <- event_status[train_indices]
  event_status_val <- event_status[-train_indices]
  
  # Run grid search to find the best hyperparameters using cross-validation
  grid_search_result <- grid_search_xgboost(X_train, time_train, event_status_train, X_val, time_val, event_status_val, param_grid, nfold, early_stopping_rounds)
  
  # Get the best model and parameters
  xgboost_model <- grid_search_result$best_model
  best_params <- grid_search_result$best_params
  
  # Print the best parameters
  print("Best Hyperparameters Found:")
  print(best_params)
  
  # Get variable importance
  importance_matrix <- xgb.importance(model = xgboost_model)
  
  # Plot variable importance
  importance_plot <- ggplot(importance_matrix, aes(x = reorder(Feature, Gain), y = Gain)) +
    geom_bar(stat = "identity") +
    coord_flip() +
    labs(title = paste(title_prefix, "Variable Importance"), x = "Variables", y = "Importance (Gain)") +
    theme_minimal()
  
  print(importance_plot)  # Display the plot
  
  return(list(model = xgboost_model, importance = importance_matrix, best_params = best_params))
}

# Function to prepare and normalize the data, ensuring clean input
prepare_data <- function(data, time_col, event_col, time_threshold) {
  valid_indices <- data[[time_col]] > 0
  
  # Censor observations beyond the threshold and normalize
  filtered_data <- data[valid_indices, ]
  filtered_data[[event_col]] <- ifelse(filtered_data[[time_col]] > time_threshold, 0, filtered_data[[event_col]])
  
  # Normalize numeric predictor variables
  normalized_data <- filtered_data %>%
    mutate(across(where(is.numeric), ~ normalize_data(.)))
  
  return(normalized_data)
}

# Function to evaluate the XGBoost model with metrics and bootstrapping for confidence intervals
evaluate_xgboost <- function(xgboost_model, X_test_data, y_test_data, time_test_data, train_column_names, n_bootstrap = 3000) {
  
  # Ensure the test data has the same columns as the training data
  X_test_data <- X_test_data[, train_column_names, drop = FALSE]
  
  # Create a DMatrix for testing
  dtest <- xgb.DMatrix(data = as.matrix(X_test_data))
  
  # Predict risk scores
  risk_scores <- predict(xgboost_model, newdata = dtest)
  
  # Calculate Harrell's C-statistic (concordance index)
  survival_object <- Surv(time_test_data, y_test_data)
  c_stat <- rcorr.cens(risk_scores, survival_object)["C Index"]
  
  # Bootstrapping AUC to get confidence intervals
  auc_bootstrap <- boot(data = data.frame(y_test_data, risk_scores), 
                        statistic = function(data, indices) {
                          auc(roc(data[indices, 1], data[indices, 2]))
                        }, 
                        R = n_bootstrap)
  auc_ci <- boot.ci(auc_bootstrap, type = "perc")
  
  auc_value <- auc(roc(y_test_data, risk_scores))
  
  return(list(c_stat = c_stat, auc = auc_value, auc_ci = auc_ci))
}

# Measure the start time
start_time <- Sys.time()

# Data preparation for 2-year and 5-year predictions
two_years_in_days <- 2 * 365.25
five_years_in_days <- 5 * 365.25

# Prepare training data
train_data_2yr <- prepare_data(train_data, "time_to_event_2yr", "event_2yr", two_years_in_days)
X_train_2yr <- model.matrix(~ . - event_2yr - event_5yr - time_to_event_2yr - time_to_event_5yr - disease_status - person_id - race - 1, data = train_data_2yr)
time_train_2yr <- train_data_2yr$time_to_event_2yr
event_status_2yr <- train_data_2yr$event_2yr

train_data_5yr <- prepare_data(train_data, "time_to_event_5yr", "event_5yr", five_years_in_days)
X_train_5yr <- model.matrix(~ . - event_2yr - event_5yr - time_to_event_2yr - time_to_event_5yr - disease_status - person_id - race - 1, data = train_data_5yr)
time_train_5yr <- train_data_5yr$time_to_event_5yr
event_status_5yr <- train_data_5yr$event_5yr

# Train models
result_2yr <- fit_xgboost_and_display(X_train_2yr, time_train_2yr, event_status_2yr, "2-Year", early_stopping_rounds = 5, nfold = 3)
xgboost_model_2yr <- result_2yr$model

result_5yr <- fit_xgboost_and_display(X_train_5yr, time_train_5yr, event_status_5yr, "5-Year", early_stopping_rounds = 5, nfold = 3)
xgboost_model_5yr <- result_5yr$model

# Measure the end time and calculate total runtime
end_time <- Sys.time()
total_runtime <- end_time - start_time
cat("Total runtime: ", total_runtime, "\n")

# Function to perform bootstrapping to calculate AUC confidence intervals
bootstrap_auc <- function(xgboost_model, X_test, y_test, train_column_names, n_bootstrap = 3000) {
  # Ensure test data has the same columns as training data
  X_test <- X_test[, train_column_names, drop = FALSE]
  
  # Create a DMatrix for the test data
  dtest <- xgb.DMatrix(data = as.matrix(X_test))
  
  # Define a function to calculate the AUC for a bootstrapped sample
  auc_bootstrap <- function(data, indices) {
    # Subset the data using the provided indices
    y_test_sample <- data[indices, 1]
    predictions_sample <- predict(xgboost_model, newdata = dtest)[indices]
    
    # Calculate the AUC for the sample
    auc_value <- auc(roc(y_test_sample, predictions_sample))
    return(auc_value)
  }
  
  # Perform bootstrapping
  boot_results <- boot(data = data.frame(y_test, predict(xgboost_model, newdata = dtest)), 
                       statistic = auc_bootstrap, R = n_bootstrap)
  
  # Calculate the 95% confidence interval for the AUC
  ci <- boot.ci(boot_results, type = "perc")
  
  # Extract the mean AUC and the confidence interval limits
  auc_mean <- mean(boot_results$t)
  auc_ci_lower <- ci$perc[4]
  auc_ci_upper <- ci$perc[5]
  
  return(list(auc_mean = auc_mean, auc_ci_lower = auc_ci_lower, auc_ci_upper = auc_ci_upper))
}

# Function to perform evaluation with bootstrapping for different race subgroups
evaluate_with_bootstrapping <- function(race_group, X_test, y_test, time_test, xgboost_model, train_column_names, n_bootstrap = 3000) {
  if (!is.null(race_group)) {
    race_indices <- X_test[, race_group] == 1
    X_test <- X_test[race_indices, ]
    y_test <- y_test[race_indices]
    time_test <- time_test[race_indices]
  }
  
  bootstrap_result <- bootstrap_auc(xgboost_model, X_test, y_test, train_column_names, n_bootstrap)
  
  return(bootstrap_result)
}

# List to store results
results_2yr_bootstrap <- list()
results_5yr_bootstrap <- list()

# Evaluate for the entire population and subgroups for both 2-year and 5-year models
population_and_subgroups <- list(
  "2-Year All" = NULL, 
  "2-Year Black" = "raceBlack", 
  "2-Year White" = "raceWhite", 
  "2-Year Asian" = "raceAsian", 
  "5-Year All" = NULL, 
  "5-Year Black" = "raceBlack", 
  "5-Year White" = "raceWhite", 
  "5-Year Asian" = "raceAsian"
)

# Run bootstrapping for each group
for (name in names(population_and_subgroups)) {
  group <- population_and_subgroups[[name]]
  if (grepl("^2-Year", name)) {
    results_2yr_bootstrap[[name]] <- evaluate_with_bootstrapping(group, X_test_2yr, y_test_2yr, time_test_2yr, xgboost_model_2yr, colnames(X_train_2yr), n_bootstrap = 3000)
  } else {
    results_5yr_bootstrap[[name]] <- evaluate_with_bootstrapping(group, X_test_5yr, y_test_5yr, time_test_5yr, xgboost_model_5yr, colnames(X_train_5yr), n_bootstrap = 3000)
  }
}

# Combine results into a data frame for easy reporting
results_table_bootstrap <- data.frame(
  Model = c(names(results_2yr_bootstrap), names(results_5yr_bootstrap)),
  AUC_Mean = c(sapply(results_2yr_bootstrap, function(x) x$auc_mean), sapply(results_5yr_bootstrap, function(x) x$auc_mean)),
  AUC_CI_Lower = c(sapply(results_2yr_bootstrap, function(x) x$auc_ci_lower), sapply(results_5yr_bootstrap, function(x) x$auc_ci_lower)),
  AUC_CI_Upper = c(sapply(results_2yr_bootstrap, function(x) x$auc_ci_upper), sapply(results_5yr_bootstrap, function(x) x$auc_ci_upper))
)

# Print the results table
print(results_table_bootstrap)

# Create a forest plot to display AUC with 95% confidence intervals, excluding "All" categories
results_table_bootstrap_filtered <- results_table_bootstrap %>%
  filter(!grepl("All", Model))

forest_plot <- ggplot(results_table_bootstrap_filtered, aes(x = AUC_Mean, y = Model)) +
  geom_point(size = 3, color = "black") +  # Add points for the mean AUC
  geom_errorbarh(aes(xmin = AUC_CI_Lower, xmax = AUC_CI_Upper), height = 0.2, size = 1) +  # Add horizontal error bars
  geom_vline(xintercept = 0.5, linetype = "dashed", color = "red", size = 0.8) +  # Add a vertical dashed line at AUC = 0.5
  labs(title = "Forest Plot of AUC with 95% Confidence Intervals for 2-Year and 5-Year Models",
       x = "Area Under the Curve (AUC)", y = "Model") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 18, face = "bold"),
    axis.title.x = element_text(size = 16, face = "bold"),
    axis.title.y = element_text(size = 16, face = "bold"),
    axis.text.y = element_text(size = 14),  
    axis.text.x = element_text(size = 14),  
    panel.grid.major = element_blank(),  
    panel.grid.minor = element_blank()
  ) +
  coord_cartesian(xlim = c(0, 1))  

# Display the plot
print(forest_plot)

# Stop parallel processing
stopImplicitCluster()

# Save the plot as a high-resolution PNG file
ggsave("Race_Specific_Forest_Plot_AUC_95CI_No_All_With_Reference.png", plot = forest_plot, dpi = 300, width = 10, height = 6)

# Optionally, save as a PDF for high-quality printing
ggsave("Race_Specific_Forest_Plot_AUC_95CI_No_All_With_Reference.pdf", plot = forest_plot, width = 10, height = 6)

```

## Race_free model:

```{r}
# Load necessary libraries
library(xgboost)
library(pROC)
library(Hmisc)  # For Harrell's C-statistic
library(survival)
library(doParallel)
library(ggplot2)
library(dplyr)
library(caret)  # For cross-validation and parameter tuning
library(boot)   # For bootstrapping AUC confidence intervals

# Set up parallel backend for faster execution
cores <- detectCores() - 2  # Leave two cores free
registerDoParallel(cores)

# Set random seed for reproducibility
set.seed(123)

# Function to normalize data using log(x + 1)
normalize_data <- function(df) {
  return(log(df + 1))
}

# Define a robust but efficient hyperparameter grid for tuning
param_grid <- expand.grid(
  max_depth = c(4, 6),  
  min_child_weight = c(1, 2),  
  gamma = c(0, 0.2),  
  eta = c(0.05, 0.1),  
  subsample = c(0.8, 0.9),  
  colsample_bytree = c(0.8, 1),  
  alpha = c(0, 0.5),  
  lambda = c(1, 1.2),  
  n_estimators = c(300, 500)  
)

# Function for hyperparameter tuning using grid search with cross-validation
grid_search_xgboost <- function(X_train, time_train, event_status_train, X_val, time_val, event_status_val, param_grid, nfold = 5, early_stopping_rounds = 5) {
  best_score <- Inf
  best_model <- NULL
  best_params <- NULL
  
  # Convert training and validation data to DMatrix for XGBoost
  dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = time_train)
  dval <- xgb.DMatrix(data = as.matrix(X_val), label = time_val)
  setinfo(dtrain, "base_margin", event_status_train)
  setinfo(dval, "base_margin", event_status_val)
  
  # Watchlist to monitor training and validation
  watchlist <- list(train = dtrain, val = dval)
  
  # Iterate over each combination of parameters in the grid
  for (i in 1:nrow(param_grid)) {
    params <- list(
      max_depth = param_grid$max_depth[i],
      min_child_weight = param_grid$min_child_weight[i],
      gamma = param_grid$gamma[i],
      eta = param_grid$eta[i],
      subsample = param_grid$subsample[i],
      colsample_bytree = param_grid$colsample_bytree[i],
      alpha = param_grid$alpha[i],
      lambda = param_grid$lambda[i],
      objective = "survival:cox",
      eval_metric = "cox-nloglik",
      nthread = cores
    )
    
    # Train the model with early stopping using the validation set
    model <- xgb.train(
      params = params,
      data = dtrain,
      nrounds = param_grid$n_estimators[i],
      watchlist = watchlist,  
      early_stopping_rounds = early_stopping_rounds,
      verbose = 0
    )
    
    # Get the final evaluation metric (cox-nloglik)
    final_score <- model$best_score
    
    # Update the best model if we find a better score
    if (final_score < best_score) {
      best_score <- final_score
      best_model <- model
      best_params <- params
    }
  }
  
  # Print the best parameters
  print("Best Parameters:")
  print(best_params)
  
  return(list(best_model = best_model, best_params = best_params))
}

# Function to fit XGBoost model with early stopping and return variable importance
fit_xgboost_and_display <- function(X_data, time_to_event, event_status, title_prefix, early_stopping_rounds = 5, nfold = 3, val_size = 0.2) {
  
  # Split the data into training and validation sets
  set.seed(123)
  train_indices <- createDataPartition(event_status, p = 1 - val_size, list = FALSE)
  X_train <- X_data[train_indices, ]
  X_val <- X_data[-train_indices, ]
  time_train <- time_to_event[train_indices]
  time_val <- time_to_event[-train_indices]
  event_status_train <- event_status[train_indices]
  event_status_val <- event_status[-train_indices]
  
  # Run grid search to find the best hyperparameters using cross-validation
  grid_search_result <- grid_search_xgboost(X_train, time_train, event_status_train, X_val, time_val, event_status_val, param_grid, nfold, early_stopping_rounds)
  
  # Get the best model and parameters
  xgboost_model <- grid_search_result$best_model
  best_params <- grid_search_result$best_params
  
  # Print the best parameters
  print("Best Hyperparameters Found:")
  print(best_params)
  
  # Get variable importance
  importance_matrix <- xgb.importance(model = xgboost_model)
  
  # Plot variable importance
  importance_plot <- ggplot(importance_matrix, aes(x = reorder(Feature, Gain), y = Gain)) +
    geom_bar(stat = "identity") +
    coord_flip() +
    labs(title = paste(title_prefix, "Variable Importance"), x = "Variables", y = "Importance (Gain)") +
    theme_minimal()
  
  print(importance_plot)  # Display the plot
  
  return(list(model = xgboost_model, importance = importance_matrix, best_params = best_params))
}

# Function to prepare and normalize the data, ensuring clean input
prepare_data <- function(data, time_col, event_col, time_threshold) {
  valid_indices <- data[[time_col]] > 0
  
  # Censor observations beyond the threshold and normalize
  filtered_data <- data[valid_indices, ]
  filtered_data[[event_col]] <- ifelse(filtered_data[[time_col]] > time_threshold, 0, filtered_data[[event_col]])
  
  # Normalize numeric predictor variables
  normalized_data <- filtered_data %>%
    mutate(across(where(is.numeric), ~ normalize_data(.)))
  
  return(normalized_data)
}

# Function to evaluate the XGBoost model with metrics and bootstrapping for confidence intervals
evaluate_xgboost <- function(xgboost_model, X_test_data, y_test_data, time_test_data, train_column_names, n_bootstrap = 3000) {
  
  # Ensure the test data has the same columns as the training data
  X_test_data <- X_test_data[, train_column_names, drop = FALSE]
  
  # Create a DMatrix for testing
  dtest <- xgb.DMatrix(data = as.matrix(X_test_data))
  
  # Predict risk scores
  risk_scores <- predict(xgboost_model, newdata = dtest)
  
  # Calculate Harrell's C-statistic (concordance index)
  survival_object <- Surv(time_test_data, y_test_data)
  c_stat <- rcorr.cens(risk_scores, survival_object)["C Index"]
  
  # Bootstrapping AUC to get confidence intervals
  auc_bootstrap <- boot(data = data.frame(y_test_data, risk_scores), 
                        statistic = function(data, indices) {
                          auc(roc(data[indices, 1], data[indices, 2]))
                        }, 
                        R = n_bootstrap)
  auc_ci <- boot.ci(auc_bootstrap, type = "perc")
  
  auc_value <- auc(roc(y_test_data, risk_scores))
  
  return(list(c_stat = c_stat, auc = auc_value, auc_ci = auc_ci))
}

# Measure the start time
start_time <- Sys.time()

# Data preparation for 2-year and 5-year predictions
two_years_in_days <- 2 * 365.25
five_years_in_days <- 5 * 365.25

# Prepare training data
train_data_2yr <- prepare_data(train_data, "time_to_event_2yr", "event_2yr", two_years_in_days)
X_train_2yr <- model.matrix(~ . - event_2yr - event_5yr - time_to_event_2yr - time_to_event_5yr - disease_status - person_id - race - raceBlack -raceWhite -raceAsian - 1, data = train_data_2yr)
time_train_2yr <- train_data_2yr$time_to_event_2yr
event_status_2yr <- train_data_2yr$event_2yr

train_data_5yr <- prepare_data(train_data, "time_to_event_5yr", "event_5yr", five_years_in_days)
X_train_5yr <- model.matrix(~ . - event_2yr - event_5yr - time_to_event_2yr - time_to_event_5yr - disease_status - person_id - race - raceBlack -raceWhite -raceAsian - 1, data = train_data_5yr)
time_train_5yr <- train_data_5yr$time_to_event_5yr
event_status_5yr <- train_data_5yr$event_5yr

# Train models
result_2yr <- fit_xgboost_and_display(X_train_2yr, time_train_2yr, event_status_2yr, "2-Year", early_stopping_rounds = 5, nfold = 3)
xgboost_model_2yr <- result_2yr$model

result_5yr <- fit_xgboost_and_display(X_train_5yr, time_train_5yr, event_status_5yr, "5-Year", early_stopping_rounds = 5, nfold = 3)
xgboost_model_5yr <- result_5yr$model

# Measure the end time and calculate total runtime
end_time <- Sys.time()
total_runtime <- end_time - start_time
cat("Total runtime: ", total_runtime, "\n")

# Function to perform bootstrapping to calculate AUC confidence intervals
bootstrap_auc <- function(xgboost_model, X_test, y_test, train_column_names, n_bootstrap = 3000) {
  # Ensure test data has the same columns as training data
  X_test <- X_test[, train_column_names, drop = FALSE]
  
  # Create a DMatrix for the test data
  dtest <- xgb.DMatrix(data = as.matrix(X_test))
  
  # Define a function to calculate the AUC for a bootstrapped sample
  auc_bootstrap <- function(data, indices) {
    # Subset the data using the provided indices
    y_test_sample <- data[indices, 1]
    predictions_sample <- predict(xgboost_model, newdata = dtest)[indices]
    
    # Calculate the AUC for the sample
    auc_value <- auc(roc(y_test_sample, predictions_sample))
    return(auc_value)
  }
  
  # Perform bootstrapping
  boot_results <- boot(data = data.frame(y_test, predict(xgboost_model, newdata = dtest)), 
                       statistic = auc_bootstrap, R = n_bootstrap)
  
  # Calculate the 95% confidence interval for the AUC
  ci <- boot.ci(boot_results, type = "perc")
  
  # Extract the mean AUC and the confidence interval limits
  auc_mean <- mean(boot_results$t)
  auc_ci_lower <- ci$perc[4]
  auc_ci_upper <- ci$perc[5]
  
  return(list(auc_mean = auc_mean, auc_ci_lower = auc_ci_lower, auc_ci_upper = auc_ci_upper))
}

# Function to perform evaluation with bootstrapping for different race subgroups
evaluate_with_bootstrapping <- function(race_group, X_test, y_test, time_test, xgboost_model, train_column_names, n_bootstrap = 3000) {
  if (!is.null(race_group)) {
    race_indices <- X_test[, race_group] == 1
    X_test <- X_test[race_indices, ]
    y_test <- y_test[race_indices]
    time_test <- time_test[race_indices]
  }
  
  bootstrap_result <- bootstrap_auc(xgboost_model, X_test, y_test, train_column_names, n_bootstrap)
  
  return(bootstrap_result)
}

# List to store results
results_2yr_bootstrap <- list()
results_5yr_bootstrap <- list()

# Evaluate for the entire population and subgroups for both 2-year and 5-year models
population_and_subgroups <- list(
  "2-Year All" = NULL, 
  "2-Year Black" = "raceBlack", 
  "2-Year White" = "raceWhite", 
  "2-Year Asian" = "raceAsian", 
  "5-Year All" = NULL, 
  "5-Year Black" = "raceBlack", 
  "5-Year White" = "raceWhite", 
  "5-Year Asian" = "raceAsian"
)

# Run bootstrapping for each group
for (name in names(population_and_subgroups)) {
  group <- population_and_subgroups[[name]]
  if (grepl("^2-Year", name)) {
    results_2yr_bootstrap[[name]] <- evaluate_with_bootstrapping(group, X_test_2yr, y_test_2yr, time_test_2yr, xgboost_model_2yr, colnames(X_train_2yr), n_bootstrap = 3000)
  } else {
    results_5yr_bootstrap[[name]] <- evaluate_with_bootstrapping(group, X_test_5yr, y_test_5yr, time_test_5yr, xgboost_model_5yr, colnames(X_train_5yr), n_bootstrap = 3000)
  }
}

# Combine results into a data frame for easy reporting
results_table_bootstrap <- data.frame(
  Model = c(names(results_2yr_bootstrap), names(results_5yr_bootstrap)),
  AUC_Mean = c(sapply(results_2yr_bootstrap, function(x) x$auc_mean), sapply(results_5yr_bootstrap, function(x) x$auc_mean)),
  AUC_CI_Lower = c(sapply(results_2yr_bootstrap, function(x) x$auc_ci_lower), sapply(results_5yr_bootstrap, function(x) x$auc_ci_lower)),
  AUC_CI_Upper = c(sapply(results_2yr_bootstrap, function(x) x$auc_ci_upper), sapply(results_5yr_bootstrap, function(x) x$auc_ci_upper))
)

# Print the results table
print(results_table_bootstrap)

# Create a forest plot to display AUC with 95% confidence intervals, excluding "All" categories
results_table_bootstrap_filtered <- results_table_bootstrap %>%
  filter(!grepl("All", Model))

forest_plot <- ggplot(results_table_bootstrap_filtered, aes(x = AUC_Mean, y = Model)) +
  geom_point(size = 3, color = "black") +  # Add points for the mean AUC
  geom_errorbarh(aes(xmin = AUC_CI_Lower, xmax = AUC_CI_Upper), height = 0.2, size = 1) +  # Add horizontal error bars
  geom_vline(xintercept = 0.5, linetype = "dashed", color = "red", size = 0.8) +  # Add a vertical dashed line at AUC = 0.5
  labs(title = "Forest Plot of AUC with 95% Confidence Intervals for 2-Year and 5-Year Models",
       x = "Area Under the Curve (AUC)", y = "Model") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 18, face = "bold"),
    axis.title.x = element_text(size = 16, face = "bold"),
    axis.title.y = element_text(size = 16, face = "bold"),
    axis.text.y = element_text(size = 14),  
    axis.text.x = element_text(size = 14),  
    panel.grid.major = element_blank(),  
    panel.grid.minor = element_blank()
  ) +
  coord_cartesian(xlim = c(0, 1))  

# Display the plot
print(forest_plot)

# Stop parallel processing
stopImplicitCluster()

# Save the plot as a high-resolution PNG file
ggsave("Race_Free_Forest_Plot_AUC_95CI_No_All_With_Reference.png", plot = forest_plot, dpi = 300, width = 10, height = 6)

# Optionally, save as a PDF for high-quality printing
ggsave("Race_Free_Forest_Plot_AUC_95CI_No_All_With_Reference.pdf", plot = forest_plot, width = 10, height = 6)

```

## Lab_free model- with race:

```{r}
# Load necessary libraries
library(xgboost)
library(pROC)
library(Hmisc)  # For Harrell's C-statistic
library(survival)
library(doParallel)
library(ggplot2)
library(dplyr)
library(caret)  # For cross-validation and parameter tuning
library(boot)   # For bootstrapping AUC confidence intervals

# Set up parallel backend for faster execution
cores <- detectCores() - 2  # Leave two cores free
registerDoParallel(cores)

# Set random seed for reproducibility
set.seed(123)

# Define lab columns to exclude
labs_columns <- c(
  "Creatinine..Mass.volume..in.Body.fluid", 
  "Erythrocyte.distribution.width..Ratio..by.Automated.count", 
  "Ferritin..Mass.volume..in.Serum.or.Plasma", 
  "Hemoglobin.A1c.Hemoglobin.total.in.Blood", 
  "Hepatitis.B.virus.surface.Ag..Presence..in.Serum.or.Plasma.by.Immunoassay", 
  "Iron..Mass.volume..in.Serum.or.Plasma", 
  "Iron.saturation..Mass.Fraction..in.Serum.or.Plasma", 
  "Parathyrin.intact..Mass.volume..in.Serum.or.Plasma", 
  "Protein..Mass.volume..in.Urine", 
  "Triglyceride..Mass.volume..in.Serum.or.Plasma", 
  "eGFR_ckd_epi_2021", 
  "serum_creatinine"
)

# Function to normalize data using log(x + 1)
normalize_data <- function(df) {
  return(log(df + 1))
}

# Define a robust but efficient hyperparameter grid for tuning
param_grid <- expand.grid(
  max_depth = c(4, 6),  
  min_child_weight = c(1, 2),  
  gamma = c(0, 0.2),  
  eta = c(0.05, 0.1),  
  subsample = c(0.8, 0.9),  
  colsample_bytree = c(0.8, 1),  
  alpha = c(0, 0.5),  
  lambda = c(1, 1.2),  
  n_estimators = c(300, 500)  
)

# Function for hyperparameter tuning using grid search with cross-validation
grid_search_xgboost <- function(X_train, time_train, event_status_train, X_val, time_val, event_status_val, param_grid, nfold = 5, early_stopping_rounds = 5) {
  best_score <- Inf
  best_model <- NULL
  best_params <- NULL
  
  # Convert training and validation data to DMatrix for XGBoost
  dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = time_train)
  dval <- xgb.DMatrix(data = as.matrix(X_val), label = time_val)
  setinfo(dtrain, "base_margin", event_status_train)
  setinfo(dval, "base_margin", event_status_val)
  
  # Watchlist to monitor training and validation
  watchlist <- list(train = dtrain, val = dval)
  
  # Iterate over each combination of parameters in the grid
  for (i in 1:nrow(param_grid)) {
    params <- list(
      max_depth = param_grid$max_depth[i],
      min_child_weight = param_grid$min_child_weight[i],
      gamma = param_grid$gamma[i],
      eta = param_grid$eta[i],
      subsample = param_grid$subsample[i],
      colsample_bytree = param_grid$colsample_bytree[i],
      alpha = param_grid$alpha[i],
      lambda = param_grid$lambda[i],
      objective = "survival:cox",
      eval_metric = "cox-nloglik",
      nthread = cores
    )
    
    # Train the model with early stopping using the validation set
    model <- xgb.train(
      params = params,
      data = dtrain,
      nrounds = param_grid$n_estimators[i],
      watchlist = watchlist,  
      early_stopping_rounds = early_stopping_rounds,
      verbose = 0
    )
    
    # Get the final evaluation metric (cox-nloglik)
    final_score <- model$best_score
    
    # Update the best model if we find a better score
    if (final_score < best_score) {
      best_score <- final_score
      best_model <- model
      best_params <- params
    }
  }
  
  # Print the best parameters
  print("Best Parameters:")
  print(best_params)
  
  return(list(best_model = best_model, best_params = best_params))
}

# Function to fit XGBoost model with early stopping and return variable importance
fit_xgboost_and_display <- function(X_data, time_to_event, event_status, title_prefix, early_stopping_rounds = 5, nfold = 3, val_size = 0.2) {
  
  # Split the data into training and validation sets
  set.seed(123)
  train_indices <- createDataPartition(event_status, p = 1 - val_size, list = FALSE)
  X_train <- X_data[train_indices, ]
  X_val <- X_data[-train_indices, ]
  time_train <- time_to_event[train_indices]
  time_val <- time_to_event[-train_indices]
  event_status_train <- event_status[train_indices]
  event_status_val <- event_status[-train_indices]
  
  # Run grid search to find the best hyperparameters using cross-validation
  grid_search_result <- grid_search_xgboost(X_train, time_train, event_status_train, X_val, time_val, event_status_val, param_grid, nfold, early_stopping_rounds)
  
  # Get the best model and parameters
  xgboost_model <- grid_search_result$best_model
  best_params <- grid_search_result$best_params
  
  # Print the best parameters
  print("Best Hyperparameters Found:")
  print(best_params)
  
  # Get variable importance
  importance_matrix <- xgb.importance(model = xgboost_model)
  
  # Plot variable importance
  importance_plot <- ggplot(importance_matrix, aes(x = reorder(Feature, Gain), y = Gain)) +
    geom_bar(stat = "identity") +
    coord_flip() +
    labs(title = paste(title_prefix, "Variable Importance"), x = "Variables", y = "Importance (Gain)") +
    theme_minimal()
  
  print(importance_plot)  # Display the plot
  
  return(list(model = xgboost_model, importance = importance_matrix, best_params = best_params))
}

# Function to prepare and normalize the data, ensuring clean input
prepare_data <- function(data, time_col, event_col, time_threshold) {
  valid_indices <- data[[time_col]] > 0
  
  # Censor observations beyond the threshold and normalize
  filtered_data <- data[valid_indices, ]
  filtered_data[[event_col]] <- ifelse(filtered_data[[time_col]] > time_threshold, 0, filtered_data[[event_col]])
  
  # Normalize numeric predictor variables
  normalized_data <- filtered_data %>%
    mutate(across(where(is.numeric), ~ normalize_data(.)))
  
  return(normalized_data)
}

# Function to evaluate the XGBoost model with metrics and bootstrapping for confidence intervals
evaluate_xgboost <- function(xgboost_model, X_test_data, y_test_data, time_test_data, train_column_names, n_bootstrap = 3000) {
  
  # Ensure the test data has the same columns as the training data
  X_test_data <- X_test_data[, train_column_names, drop = FALSE]
  
  # Create a DMatrix for testing
  dtest <- xgb.DMatrix(data = as.matrix(X_test_data))
  
  # Predict risk scores
  risk_scores <- predict(xgboost_model, newdata = dtest)
  
  # Calculate Harrell's C-statistic (concordance index)
  survival_object <- Surv(time_test_data, y_test_data)
  c_stat <- rcorr.cens(risk_scores, survival_object)["C Index"]
  
  # Bootstrapping AUC to get confidence intervals
  auc_bootstrap <- boot(data = data.frame(y_test_data, risk_scores), 
                        statistic = function(data, indices) {
                          auc(roc(data[indices, 1], data[indices, 2]))
                        }, 
                        R = n_bootstrap)
  auc_ci <- boot.ci(auc_bootstrap, type = "perc")
  
  auc_value <- auc(roc(y_test_data, risk_scores))
  
  return(list(c_stat = c_stat, auc = auc_value, auc_ci = auc_ci))
}

# Measure the start time
start_time <- Sys.time()

# Data preparation for 2-year and 5-year predictions
two_years_in_days <- 2 * 365.25
five_years_in_days <- 5 * 365.25


# Prepare training data
train_data_2yr <- prepare_data(train_data, "time_to_event_2yr", "event_2yr", two_years_in_days)

# Exclude the lab columns from the data
train_data_2yr <- train_data_2yr[, !(colnames(train_data_2yr) %in% labs_columns)]

X_train_2yr <- model.matrix(~ . - event_2yr - event_5yr - time_to_event_2yr - time_to_event_5yr - disease_status - person_id - race - 1, data = train_data_2yr)
time_train_2yr <- train_data_2yr$time_to_event_2yr
event_status_2yr <- train_data_2yr$event_2yr

train_data_5yr <- prepare_data(train_data, "time_to_event_5yr", "event_5yr", five_years_in_days)

# Exclude the lab columns from the data
train_data_5yr <- train_data_5yr[, !(colnames(train_data_5yr) %in% labs_columns)]

X_train_5yr <- model.matrix(~ . - event_2yr - event_5yr - time_to_event_2yr - time_to_event_5yr - disease_status - person_id - race  - 1, data = train_data_5yr)
time_train_5yr <- train_data_5yr$time_to_event_5yr
event_status_5yr <- train_data_5yr$event_5yr

# Train models
result_2yr <- fit_xgboost_and_display(X_train_2yr, time_train_2yr, event_status_2yr, "2-Year", early_stopping_rounds = 5, nfold = 3)
xgboost_model_2yr <- result_2yr$model

result_5yr <- fit_xgboost_and_display(X_train_5yr, time_train_5yr, event_status_5yr, "5-Year", early_stopping_rounds = 5, nfold = 3)
xgboost_model_5yr <- result_5yr$model

# Measure the end time and calculate total runtime
end_time <- Sys.time()
total_runtime <- end_time - start_time
cat("Total runtime: ", total_runtime, "\n")

# Function to perform bootstrapping to calculate AUC confidence intervals
bootstrap_auc <- function(xgboost_model, X_test, y_test, train_column_names, n_bootstrap = 3000) {
  # Ensure test data has the same columns as training data
  X_test <- X_test[, train_column_names, drop = FALSE]
  
  # Create a DMatrix for the test data
  dtest <- xgb.DMatrix(data = as.matrix(X_test))
  
  # Define a function to calculate the AUC for a bootstrapped sample
  auc_bootstrap <- function(data, indices) {
    # Subset the data using the provided indices
    y_test_sample <- data[indices, 1]
    predictions_sample <- predict(xgboost_model, newdata = dtest)[indices]
    
    # Calculate the AUC for the sample
    auc_value <- auc(roc(y_test_sample, predictions_sample))
    return(auc_value)
  }
  
  # Perform bootstrapping
  boot_results <- boot(data = data.frame(y_test, predict(xgboost_model, newdata = dtest)), 
                       statistic = auc_bootstrap, R = n_bootstrap)
  
  # Calculate the 95% confidence interval for the AUC
  ci <- boot.ci(boot_results, type = "perc")
  
  # Extract the mean AUC and the confidence interval limits
  auc_mean <- mean(boot_results$t)
  auc_ci_lower <- ci$perc[4]
  auc_ci_upper <- ci$perc[5]
  
  return(list(auc_mean = auc_mean, auc_ci_lower = auc_ci_lower, auc_ci_upper = auc_ci_upper))
}

# Function to perform evaluation with bootstrapping for different race subgroups
evaluate_with_bootstrapping <- function(race_group, X_test, y_test, time_test, xgboost_model, train_column_names, n_bootstrap = 3000) {
  if (!is.null(race_group)) {
    race_indices <- X_test[, race_group] == 1
    X_test <- X_test[race_indices, ]
    y_test <- y_test[race_indices]
    time_test <- time_test[race_indices]
  }
  
  bootstrap_result <- bootstrap_auc(xgboost_model, X_test, y_test, train_column_names, n_bootstrap)
  
  return(bootstrap_result)
}

# List to store results
results_2yr_bootstrap <- list()
results_5yr_bootstrap <- list()

# Evaluate for the entire population and subgroups for both 2-year and 5-year models
population_and_subgroups <- list(
  "2-Year All" = NULL, 
  "2-Year Black" = "raceBlack", 
  "2-Year White" = "raceWhite", 
  "2-Year Asian" = "raceAsian", 
  "5-Year All" = NULL, 
  "5-Year Black" = "raceBlack", 
  "5-Year White" = "raceWhite", 
  "5-Year Asian" = "raceAsian"
)

# Run bootstrapping for each group
for (name in names(population_and_subgroups)) {
  group <- population_and_subgroups[[name]]
  if (grepl("^2-Year", name)) {
    results_2yr_bootstrap[[name]] <- evaluate_with_bootstrapping(group, X_test_2yr, y_test_2yr, time_test_2yr, xgboost_model_2yr, colnames(X_train_2yr), n_bootstrap = 3000)
  } else {
    results_5yr_bootstrap[[name]] <- evaluate_with_bootstrapping(group, X_test_5yr, y_test_5yr, time_test_5yr, xgboost_model_5yr, colnames(X_train_5yr), n_bootstrap = 3000)
  }
}

# Combine results into a data frame for easy reporting
results_table_bootstrap <- data.frame(
  Model = c(names(results_2yr_bootstrap), names(results_5yr_bootstrap)),
  AUC_Mean = c(sapply(results_2yr_bootstrap, function(x) x$auc_mean), sapply(results_5yr_bootstrap, function(x) x$auc_mean)),
  AUC_CI_Lower = c(sapply(results_2yr_bootstrap, function(x) x$auc_ci_lower), sapply(results_5yr_bootstrap, function(x) x$auc_ci_lower)),
  AUC_CI_Upper = c(sapply(results_2yr_bootstrap, function(x) x$auc_ci_upper), sapply(results_5yr_bootstrap, function(x) x$auc_ci_upper))
)

# Print the results table
print(results_table_bootstrap)

# Create a forest plot to display AUC with 95% confidence intervals, excluding "All" categories
results_table_bootstrap_filtered <- results_table_bootstrap %>%
  filter(!grepl("All", Model))

forest_plot <- ggplot(results_table_bootstrap_filtered, aes(x = AUC_Mean, y = Model)) +
  geom_point(size = 3, color = "black") +  # Add points for the mean AUC
  geom_errorbarh(aes(xmin = AUC_CI_Lower, xmax = AUC_CI_Upper), height = 0.2, size = 1) +  # Add horizontal error bars
  geom_vline(xintercept = 0.5, linetype = "dashed", color = "red", size = 0.8) +  # Add a vertical dashed line at AUC = 0.5
  labs(title = "Forest Plot of AUC with 95% Confidence Intervals for 2-Year and 5-Year Models",
       x = "Area Under the Curve (AUC)", y = "Model") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 18, face = "bold"),
    axis.title.x = element_text(size = 16, face = "bold"),
    axis.title.y = element_text(size = 16, face = "bold"),
    axis.text.y = element_text(size = 14),  
    axis.text.x = element_text(size = 14),  
    panel.grid.major = element_blank(),  
    panel.grid.minor = element_blank()
  ) +
  coord_cartesian(xlim = c(0, 1))  

# Display the plot
print(forest_plot)

# Stop parallel processing
stopImplicitCluster()

# Save the plot as a high-resolution PNG file
ggsave("Labfree_withrace_Forest_Plot_AUC_95CI_No_All_With_Reference.png", plot = forest_plot, dpi = 300, width = 10, height = 6)

# Optionally, save as a PDF for high-quality printing
ggsave("Labfree_withrace_Forest_Plot_AUC_95CI_No_All_With_Reference.pdf", plot = forest_plot, width = 10, height = 6)

```

## Lab_free model- without race:

```{r}
# Load necessary libraries
library(xgboost)
library(pROC)
library(Hmisc)  # For Harrell's C-statistic
library(survival)
library(doParallel)
library(ggplot2)
library(dplyr)
library(caret)  # For cross-validation and parameter tuning

# Set up parallel backend for faster execution
cores <- detectCores() - 1
registerDoParallel(cores)

# Set random seed for reproducibility
set.seed(123)

# Define lab columns to exclude
labs_columns <- c(
  "Creatinine..Mass.volume..in.Body.fluid", 
  "Erythrocyte.distribution.width..Ratio..by.Automated.count", 
  "Ferritin..Mass.volume..in.Serum.or.Plasma", 
  "Hemoglobin.A1c.Hemoglobin.total.in.Blood", 
  "Hepatitis.B.virus.surface.Ag..Presence..in.Serum.or.Plasma.by.Immunoassay", 
  "Iron..Mass.volume..in.Serum.or.Plasma", 
  "Iron.saturation..Mass.Fraction..in.Serum.or.Plasma", 
  "Parathyrin.intact..Mass.volume..in.Serum.or.Plasma", 
  "Protein..Mass.volume..in.Urine", 
  "Triglyceride..Mass.volume..in.Serum.or.Plasma", 
  "eGFR_ckd_epi_2021", 
  "serum_creatinine",
  "raceBlack", "raceWhite", "raceAsian"
)

# Define a robust but efficient hyperparameter grid for tuning
param_grid <- expand.grid(
  max_depth = c(4, 6),  
  min_child_weight = c(1, 2),  
  gamma = c(0, 0.2),  
  eta = c(0.05, 0.1),  
  subsample = c(0.8, 0.9),  
  colsample_bytree = c(0.8, 1),  
  alpha = c(0, 0.5),  
  lambda = c(1, 1.2),  
  n_estimators = c(300, 500)  
)

# Normalization function (log(x + 1)) to transform skewed data
normalize_data <- function(df) {
  return(log(df + 1))
}

# Optimal Parameters for XGBoost Cox model
optimal_params <- list(
  max_depth = 6,
  min_child_weight = 2,
  gamma = 0.5,
  eta = 0.1,                    # Learning rate
  subsample = 1,                # Full sampling
  colsample_bytree = 1,         # Full column sampling
  objective = "survival:cox",   # Cox proportional hazards objective
  eval_metric = "cox-nloglik",  # Evaluation metric for Cox models
  nthread = cores               # Number of threads to use
)

# Function to fit XGBoost model and display variable importance
fit_xgboost_and_display <- function(X_data, time_to_event, event_status, title_prefix, params, nrounds) {
  
  # Convert the data to DMatrix, which is optimized for XGBoost
  dtrain <- xgb.DMatrix(data = as.matrix(X_data), label = time_to_event)
  
  # Set base_margin to indicate censoring (1 = event occurred, 0 = censored)
  setinfo(dtrain, "base_margin", event_status)
  
  # Train the XGBoost model with the provided parameters and number of rounds
  xgboost_model <- xgboost(
    params = params, 
    data = dtrain, 
    nrounds = nrounds,  # Use nrounds to control the number of boosting rounds
    verbose = 0  # Suppress output
  )
  
  # Get the feature importance
  importance_matrix <- xgb.importance(model = xgboost_model)
  
  # Display the selected variables and their importance
  print(paste("Selected Variables by XGBoost for", title_prefix, "Prediction:"))
  print(importance_matrix)
  
  # Create a variable importance plot
  p <- ggplot(importance_matrix, aes(x = reorder(Feature, Gain), y = Gain)) +
    geom_bar(stat = "identity") +
    coord_flip() +  # Flip the plot to make it horizontal
    labs(title = paste(title_prefix, "Variable Importance Plot (XGBoost)"), 
         x = "Variables", 
         y = "Importance (Gain)") +
    theme_minimal()
  
  # Display the plot
  print(p)
  
  # Return the final model and feature importance
  return(list(model = xgboost_model, importance = importance_matrix))
}

# Function to prepare and normalize the data
prepare_data <- function(data, time_col, event_col, time_threshold, labs_columns) {
  # Filter out rows with non-positive time_to_event
  valid_indices <- data[[time_col]] > 0
  
  # Filter only rows with positive time to event
  filtered_data <- data[valid_indices, ]
  
  # Censor observations that exceed the time threshold (2 years or 5 years)
  filtered_data[[event_col]] <- ifelse(filtered_data[[time_col]] > time_threshold, 0, filtered_data[[event_col]])
  
  # Normalize the predictor variables with log(x + 1)
  normalized_data <- filtered_data %>%
    mutate(across(where(is.numeric), ~ normalize_data(.)))
  
  # Remove lab columns from the dataset
  normalized_data <- normalized_data[, !colnames(normalized_data) %in% labs_columns]
  
  return(normalized_data)
}

# Function to ensure test data includes all columns present in the training data
clean_test_data <- function(X_test, time_test, event_status, train_colnames) {
  
  # Ensure that the test data includes only observations with positive time-to-event
  valid_indices <- time_test > 0
  X_test_cleaned <- X_test[valid_indices, , drop = FALSE]
  time_test_cleaned <- time_test[valid_indices]
  event_status_cleaned <- event_status[valid_indices]
  
  # Remove columns not present in the training set (e.g., lab-related columns)
  X_test_cleaned <- X_test_cleaned[, colnames(X_test_cleaned) %in% train_colnames, drop = FALSE]
  
  # Add missing columns (if any) from the training set, initializing with 0s
  missing_cols <- setdiff(train_colnames, colnames(X_test_cleaned))
  if (length(missing_cols) > 0) {
    for (col in missing_cols) {
      X_test_cleaned[[col]] <- 0  # Add missing columns with 0s
    }
  }
  
  # Ensure the columns are in the same order as the training data
  X_test_cleaned <- X_test_cleaned[, train_colnames, drop = FALSE]
  
  return(list(X_test = X_test_cleaned, time_test = time_test_cleaned, event_status = event_status_cleaned))
}

# Function to evaluate the XGBoost model on the test data
evaluate_xgboost <- function(xgboost_model, X_test_data, y_test_data, time_test_data, train_column_names) {
  
  # Ensure the test data has the same columns as the training data
  X_test_data <- X_test_data[, train_column_names, drop = FALSE]
  
  # Set feature names for the test data
  dtest <- xgb.DMatrix(data = as.matrix(X_test_data))
  colnames(dtest) <- train_column_names  # Explicitly set column names
  
  # Predict risk scores using the XGBoost model
  risk_scores <- predict(xgboost_model, newdata = dtest)
  
  # Calculate Harrell's C-statistic using time-to-event data
  survival_object <- Surv(time_test_data, y_test_data)
  
  # Calculate Harrell's C-statistic (concordance index)
  c_stat <- rcorr.cens(risk_scores, survival_object)["C Index"]
  
  # Calculate AUC using the ROC curve
  roc_curve <- roc(y_test_data, risk_scores)
  auc_value <- auc(roc_curve)
  
  return(list(c_stat = c_stat, auc = auc_value))
}

# Step 1: Data preparation for 2-year and 5-year predictions
two_years_in_days <- 2 * 365.25
five_years_in_days <- 5 * 365.25

# Prepare the training data for 2-year prediction, excluding lab columns
train_data_2yr <- prepare_data(train_data, "time_to_event_2yr", "event_2yr", two_years_in_days, labs_columns)
X_train_2yr <- model.matrix(~ . - event_2yr - event_5yr - time_to_event_2yr - time_to_event_5yr - disease_status - person_id - race - 1, 
                            data = train_data_2yr)
time_train_2yr <- train_data_2yr$time_to_event_2yr
event_status_2yr <- train_data_2yr$event_2yr

# Prepare the training data for 5-year prediction, excluding lab columns
train_data_5yr <- prepare_data(train_data, "time_to_event_5yr", "event_5yr", five_years_in_days, labs_columns)
X_train_5yr <- model.matrix(~ . - event_2yr - event_5yr - time_to_event_2yr - time_to_event_5yr - disease_status - person_id - race - 1, 
                            data = train_data_5yr)
time_train_5yr <- train_data_5yr$time_to_event_5yr
event_status_5yr <- train_data_5yr$event_5yr

# Step 2: Train XGBoost models for 2-year and 5-year predictions using the optimal parameters

# Train and display results for the 2-year XGBoost model
result_2yr <- fit_xgboost_and_display(X_train_2yr, time_train_2yr, event_status_2yr, "2-Year", optimal_params, nrounds = 100)
xgboost_model_2yr <- result_2yr$model
importance_2yr <- result_2yr$importance  # Variable importance for 2-year model

# Train and display results for the 5-year XGBoost model
result_5yr <- fit_xgboost_and_display(X_train_5yr, time_train_5yr, event_status_5yr, "5-Year", optimal_params, nrounds = 100)
xgboost_model_5yr <- result_5yr$model
importance_5yr <- result_5yr$importance  # Variable importance for 5-year model

# Step 3: Display the final parameters
print("Final Hyperparameters for the model:")
print(optimal_params)

# Step 4: Stop parallel processing
stopImplicitCluster()

library(dplyr)
library(pROC)
library(boot)
library(ggplot2)

# Function to perform bootstrapping to calculate AUC confidence intervals
bootstrap_auc <- function(xgboost_model, X_test, y_test, train_column_names, n_bootstrap = 3000) {
  # Ensure test data has the same columns as training data
  X_test <- X_test[, train_column_names, drop = FALSE]
  
  # Create a DMatrix for the test data
  dtest <- xgb.DMatrix(data = as.matrix(X_test))
  
  # Define a function to calculate the AUC for a bootstrapped sample
  auc_bootstrap <- function(data, indices) {
    # Subset the data using the provided indices
    y_test_sample <- data[indices, 1]
    predictions_sample <- predict(xgboost_model, newdata = dtest)[indices]
    
    # Calculate the AUC for the sample
    auc_value <- auc(roc(y_test_sample, predictions_sample))
    return(auc_value)
  }
  
  # Perform bootstrapping
  boot_results <- boot(data = data.frame(y_test, predict(xgboost_model, newdata = dtest)), 
                       statistic = auc_bootstrap, R = n_bootstrap)
  
  # Calculate the 95% confidence interval for the AUC
  ci <- boot.ci(boot_results, type = "perc")
  
  # Extract the mean AUC and the confidence interval limits
  auc_mean <- mean(boot_results$t)
  auc_ci_lower <- ci$perc[4]
  auc_ci_upper <- ci$perc[5]
  
  return(list(auc_mean = auc_mean, auc_ci_lower = auc_ci_lower, auc_ci_upper = auc_ci_upper))
}

# Perform evaluation with bootstrapping for the full population and subgroups
evaluate_with_bootstrapping <- function(race_group, X_test, y_test, time_test, xgboost_model, train_column_names, n_bootstrap = 3000) {
  if (!is.null(race_group)) {
    race_indices <- X_test[, race_group] == 1
    X_test <- X_test[race_indices, ]
    y_test <- y_test[race_indices]
    time_test <- time_test[race_indices]
  }
  
  bootstrap_result <- bootstrap_auc(xgboost_model, X_test, y_test, train_column_names, n_bootstrap)
  
  return(bootstrap_result)
}

# List to store results
results_2yr_bootstrap <- list()
results_5yr_bootstrap <- list()

# Evaluate for the entire population and subgroups for both 2-year and 5-year models
population_and_subgroups <- list(
  "2-Year All" = NULL, 
  "2-Year Black" = "raceBlack", 
  "2-Year White" = "raceWhite", 
  "2-Year Asian" = "raceAsian", 
  "5-Year All" = NULL, 
  "5-Year Black" = "raceBlack", 
  "5-Year White" = "raceWhite", 
  "5-Year Asian" = "raceAsian"
)

# Run bootstrapping for each group
for (name in names(population_and_subgroups)) {
  group <- population_and_subgroups[[name]]
  if (grepl("^2-Year", name)) {
    results_2yr_bootstrap[[name]] <- evaluate_with_bootstrapping(group, X_test_2yr, y_test_2yr, time_test_2yr, xgboost_model_2yr, colnames(X_train_2yr), n_bootstrap = 3000)
  } else {
    results_5yr_bootstrap[[name]] <- evaluate_with_bootstrapping(group, X_test_5yr, y_test_5yr, time_test_5yr, xgboost_model_5yr, colnames(X_train_5yr), n_bootstrap = 3000)
  }
}

# Combine results into a data frame for easy reporting
results_table_bootstrap <- data.frame(
  Model = c(names(results_2yr_bootstrap), names(results_5yr_bootstrap)),
  AUC_Mean = c(sapply(results_2yr_bootstrap, function(x) x$auc_mean), sapply(results_5yr_bootstrap, function(x) x$auc_mean)),
  AUC_CI_Lower = c(sapply(results_2yr_bootstrap, function(x) x$auc_ci_lower), sapply(results_5yr_bootstrap, function(x) x$auc_ci_lower)),
  AUC_CI_Upper = c(sapply(results_2yr_bootstrap, function(x) x$auc_ci_upper), sapply(results_5yr_bootstrap, function(x) x$auc_ci_upper))
)

# Print the results table
print(results_table_bootstrap)

library(ggplot2)

# Filter out "All" categories from the dataset
results_table_bootstrap_filtered <- results_table_bootstrap %>%
  filter(!grepl("All", Model))

# Create a forest plot to display AUC with 95% confidence intervals, excluding "All" categories
forest_plot <- ggplot(results_table_bootstrap_filtered, aes(x = AUC_Mean, y = Model)) +
  geom_point(size = 3, color = "black") +  # Add points for the mean AUC
  geom_errorbarh(aes(xmin = AUC_CI_Lower, xmax = AUC_CI_Upper), height = 0.2, size = 1) +  # Add horizontal error bars
  geom_vline(xintercept = 0.5, linetype = "dashed", color = "red", size = 0.8) +  # Add a vertical dashed line at AUC = 0.5
  labs(title = "Forest Plot of AUC with 95% Confidence Intervals for 2-Year and 5-Year Models",
       x = "Area Under the Curve (AUC)", y = "Model") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 18, face = "bold"),
    axis.title.x = element_text(size = 16, face = "bold"),
    axis.title.y = element_text(size = 16, face = "bold"),
    axis.text.y = element_text(size = 14),  # Increase the y-axis text size for better readability
    axis.text.x = element_text(size = 14),  # Increase the x-axis text size for better readability
    panel.grid.major = element_blank(),  # Remove major grid lines for a cleaner look
    panel.grid.minor = element_blank()
  ) +
  coord_cartesian(xlim = c(0, 1))  # Set the x-axis limits to 0-1 for better interpretation

# Display the plot
print(forest_plot)

# Save the plot as a high-resolution PNG file
ggsave("Labfree_withoutrace_Forest_Plot_AUC_95CI_No_All_With_Reference.png", plot = forest_plot, dpi = 300, width = 10, height = 6)

# Optionally, save as a PDF for high-quality printing
ggsave("Labfree_withoutrace_Forest_Plot_AUC_95CI_No_All_With_Reference.pdf", plot = forest_plot, width = 10, height = 6)

```

## 

# Cox Model

##### Missing data check

Remove Features with count less than 10

```{r}
# Load necessary libraries
library(dplyr)
library(tidyr)

# Load dataset
filtered_person_with_summary <- read.csv("person_with_summary_inrange_indexdate_creatnine.csv")


# Step 1: Select and rename relevant columns
filtered_person_with_summary <- filtered_person_with_summary %>%
  select(-age_reported,
         -Glomerular.filtration.rate.1.73.sq.M.predicted..Volume.Rate.Area..in.Serum..Plasma.or.Blood.by.Creatinine.based.formula..MDRD.,
         -Tobacco.smoking.status,
         -height,
         -weight,
         -gender) %>%
  rename(smoking = highest_smoking_status_rank)

# Step 2: Filter out non-numeric columns and pivot longer
person_long <- filtered_person_with_summary %>%
  select(person_id, disease_status, where(is.numeric)) %>%
  pivot_longer(
    cols = -c(person_id, disease_status),
    names_to = "variable",
    values_to = "value"
  ) %>%
  filter(!is.na(value) & value != 0)  # Remove rows where the value is NA or zero

# Step 3: Count occurrences of disease_status for each variable
status_counts <- person_long %>%
  group_by(variable, disease_status) %>%
  summarise(count = n_distinct(person_id), .groups = 'drop')

# Step 4: Pivot the summary table to a wide format
summary_wide <- status_counts %>%
  pivot_wider(
    names_from = disease_status,
    values_from = count,
    names_prefix = "disease_status_",
    values_fill = list(count = 0)
  )

# Step 5: Filter out variables that have less than a count of 10 for either disease_status_0 or disease_status_1
filtered_summary <- summary_wide %>%
  filter(disease_status_0 >= 10 & disease_status_1 >= 10)

# Step 6: Extract the variable names that passed the filter
remaining_variables <- filtered_summary$variable

# Step 7: Retain only the person_id, disease_status, race, sex_at_birth, ethnicity, and the remaining variables
filtered_person_with_summary <- filtered_person_with_summary %>%
  select(person_id, disease_status, race, sex_at_birth, ethnicity, all_of(remaining_variables))

# Step 8: Identify the dropped variables
all_variables <- names(select(filtered_person_with_summary, where(is.numeric)))
dropped_variables <- setdiff(all_variables, remaining_variables)

# Print the dropped variables if needed
#print(dropped_variables)

# View the updated column names
colnames(filtered_person_with_summary)

# Save the filtered_person_with_summary dataset as a .csv file
write.csv(filtered_person_with_summary, "my_dataframe.csv", row.names = FALSE)


```

### Add real lab values

```{r}
# Define the specific standard_concept_codes you are interested in
selected_codes <- c("2498-4", "2731-8", "2888-6", "2571-8", "8480-6")

# Filter the dataset based on standard_concept_code and select specific columns
filtered_data <- dataset_13140188_measurement_df[
  dataset_13140188_measurement_df$standard_concept_code %in% selected_codes,
  c("standard_concept_code", "person_id", "measurement_datetime", "value_as_number")
]

filtered_person_with_summary <- read.csv("person_with_summary_inrange_indexdate_creatnine.csv")



# Select only person_id and index_date columns from filtered_person_with_summary
index_date_data <- filtered_person_with_summary[, c("person_id", "index_date")]

# Merge filtered_data with index_date_data based on person_id
filtered_data_with_index_date <- merge(filtered_data, index_date_data, by = "person_id", all.x = TRUE)

# Step 2


# Convert measurement_datetime and index_date to Date type if they are not already
filtered_data_with_index_date$measurement_datetime <- as.Date(filtered_data_with_index_date$measurement_datetime)
filtered_data_with_index_date$index_date <- as.Date(filtered_data_with_index_date$index_date)

# Filter the data to include only rows where measurement_datetime is within 365 days of index_date
filtered_data_within_365_days <- subset(
  filtered_data_with_index_date,
  abs(difftime(measurement_datetime, index_date, units = "days")) <= 365
)


# Step 3

# Remove rows with NA in value_as_number
filtered_data_no_na <- subset(filtered_data_within_365_days, !is.na(value_as_number))

# Define a function to find the row with measurement_datetime closest to index_date
get_closest_measurement <- function(df) {
  df$time_diff <- abs(difftime(df$measurement_datetime, df$index_date, units = "days"))
  df <- df[order(df$time_diff), ]  # Order by closest time difference
  return(df[1, ])  # Select the row with the smallest time difference
}

# Apply the function by group (standard_concept_code and person_id)
library(dplyr)  # Use dplyr for grouping
closest_measurements <- filtered_data_no_na %>%
  group_by(standard_concept_code, person_id) %>%
  do(get_closest_measurement(.)) %>%
  ungroup() %>%
  select(-time_diff)  # Remove the temporary time_diff column

# Convert back to data frame if necessary
closest_measurements <- as.data.frame(closest_measurements)


# Load dplyr if not already loaded
library(dplyr)

# Rename standard_concept_code values using recode
closest_measurements <- closest_measurements %>%
  mutate(standard_concept_code = recode(
    standard_concept_code,
    "2498-4" = "Iron [Mass/volume] in Serum or Plasma",
    "2571-8" = "Triglyceride [Mass/volume] in Serum or Plasma",
    "2731-8" = "Parathyrin.intact [Mass/volume] in Serum or Plasma",
    "2888-6" = "Protein [Mass/volume] in Urine",
    "8480-6" = "Systolic blood pressure"
  ))


# Load tidyr for pivot_wider
library(tidyr)

# Reshape the data so that each unique standard_concept_code becomes a column
reshaped_data <- closest_measurements %>%
  select(person_id, standard_concept_code, value_as_number) %>%
  pivot_wider(
    names_from = standard_concept_code,
    values_from = value_as_number
  )

# Step 4

my_dataframe <- read.csv("my_dataframe.csv")

# Load necessary libraries
library(dplyr)

# Step 1: Remove specified columns from my_dataframe
columns_to_remove <- c(
  "Triglyceride..Mass.volume..in.Serum.or.Plasma",
  "Systolic.blood.pressure",
  "Protein..Mass.volume..in.Urine",
  "Iron..Mass.volume..in.Serum.or.Plasma",
  "Parathyrin.intact..Mass.volume..in.Serum.or.Plasma"
)
my_dataframe <- my_dataframe %>% select(-all_of(columns_to_remove))

# Step 2: Join reshaped_data to my_dataframe by person_id
# Assuming reshaped_data is already created and has person_id as a key column
# Ensure reshaped_data only has unique person_id rows for correct merging
reshaped_data <- reshaped_data %>% distinct(person_id, .keep_all = TRUE)

# Merge reshaped_data into my_dataframe by person_id
my_dataframe <- my_dataframe %>%
  left_join(reshaped_data, by = "person_id")

# Step 3: Save the updated dataset as CSV
write.csv(my_dataframe, "my_dataframe_lab_values.csv", row.names = FALSE)

# Print message to confirm the operation
cat("Data saved as my_dataframe_lab_values.csv\n")

my_dataframe_labs <- read.csv("my_dataframe_lab_values.csv")
colnames(my_dataframe_labs)
```

##### 

#### NA Count in lab values

```{r}
my_dataframe <- read.csv("my_dataframe_lab_values.csv")
# Define the columns of interest
columns_of_interest <- c(
  "Iron..Mass.volume..in.Serum.or.Plasma",
  "Triglyceride..Mass.volume..in.Serum.or.Plasma",
  "Parathyrin.intact..Mass.volume..in.Serum.or.Plasma",
  "Protein..Mass.volume..in.Urine",
  "Systolic.blood.pressure"
)

# Count the number of NA values in each of these columns
na_counts <- sapply(my_dataframe[columns_of_interest], function(x) sum(is.na(x)))

# Display the result
na_counts

# Filter rows with NA in Systolic.blood.pressure
na_rows <- my_dataframe[is.na(my_dataframe$Systolic.blood.pressure), ]

# Count cases and controls among the NA rows in Systolic.blood.pressure
na_counts_by_status <- table(na_rows$disease_status)

# Display the result
print(na_counts_by_status)

```

# TABLE 1

```{r}
my_dataframe <- read.csv("my_dataframe_labs_socioeconomic.csv")

# Explicitly set `race` as a factor with levels
table1_data <- table1_data %>%
  mutate(race = factor(race, levels = c("Asian", "Black or African American", "White")))

# Load necessary libraries
library(dplyr)
library(tableone)

# Prepare the data, interpreting counts as binary (Yes/No) for conditions and healthcare usage
table1_data <- my_dataframe %>%
  dplyr::select(
    race, sex_at_birth, age_precise, Health_Insurance_Yes, Health_Insurance_No,
    disease_status, BMI, eGFR_ckd_epi_2021, serum_creatinine,
    Acidosis, Acute.renal.failure.syndrome, Anemia, Essential.hypertension, Diabetes.mellitus, 
    Chronic.kidney.disease.stage.3, Chronic.kidney.disease.stage.4,
    Uses_only_Urgent_care, Uses_only_Doctors_office, Uses_only_Emergency_room,
    Systolic.blood.pressure, time_to_event
  ) %>%
  mutate(
    # Explicitly set `race` as a factor with all expected levels
    race = factor(race, levels = c("Asian", "Black or African American", "White")),
    
    # Create Health Insurance variable
    Health_Insurance = case_when(
      Health_Insurance_Yes == 1 ~ "Yes",
      Health_Insurance_No == 1 ~ "No",
      TRUE ~ "Unknown"
    ),
    
    # Convert disease status to Case/Control
    Disease_Status = ifelse(disease_status == 1, "Case", "Control"),
    
    # Convert conditions to binary (Yes/No) based on >0 criterion
    Acidosis = ifelse(Acidosis > 0, "Yes", "No"),
    Acute.renal.failure.syndrome = ifelse(Acute.renal.failure.syndrome > 0, "Yes", "No"),
    Anemia = ifelse(Anemia > 0, "Yes", "No"),
    Essential.hypertension = ifelse(Essential.hypertension > 0, "Yes", "No"),
    Diabetes.mellitus = ifelse(Diabetes.mellitus > 0, "Yes", "No"),
    Chronic.kidney.disease.stage.3 = ifelse(Chronic.kidney.disease.stage.3 > 0, "Yes", "No"),
    Chronic.kidney.disease.stage.4 = ifelse(Chronic.kidney.disease.stage.4 > 0, "Yes", "No"),
    
    # Convert healthcare usage variables to Yes/No
    Uses_only_Urgent_care = ifelse(Uses_only_Urgent_care == 1, "Yes", "No"),
    Uses_only_Doctors_office = ifelse(Uses_only_Doctors_office == 1, "Yes", "No"),
    Uses_only_Emergency_room = ifelse(Uses_only_Emergency_room == 1, "Yes", "No")
  )

# Define categorical and continuous variables for summary
cat_vars <- c("Disease_Status", "race", "sex_at_birth", "Health_Insurance",
              "Acidosis", "Acute.renal.failure.syndrome", "Anemia", 
              "Essential.hypertension", "Diabetes.mellitus", 
              "Chronic.kidney.disease.stage.3", "Chronic.kidney.disease.stage.4",
              "Uses_only_Urgent_care", "Uses_only_Doctors_office", "Uses_only_Emergency_room")

cont_vars <- c("age_precise", "BMI", "eGFR_ckd_epi_2021", "serum_creatinine", 
               "Systolic.blood.pressure", "time_to_event")

# Create Table 1 with race stratification
table1 <- CreateTableOne(vars = c(cat_vars, cont_vars), strata = "race", data = table1_data, factorVars = cat_vars)

# Print the table with formatting for publication
print(table1, showAllLevels = TRUE, formatOptions = list(big.mark = ",", scientific = FALSE))

```

```{r}
# Load necessary libraries
library(dplyr)
library(tableone)
library(kableExtra)

# Prepare the data as before
table1_data <- my_dataframe %>%
  dplyr::select(
    race, sex_at_birth, age_precise, Health_Insurance_Yes, Health_Insurance_No,
    disease_status, BMI, eGFR_ckd_epi_2021, serum_creatinine,
    Acidosis, Acute.renal.failure.syndrome, Anemia, Essential.hypertension, Diabetes.mellitus, 
    Chronic.kidney.disease.stage.3, Chronic.kidney.disease.stage.4,
    Uses_only_Urgent_care, Uses_only_Doctors_office, Uses_only_Emergency_room,
    Systolic.blood.pressure, time_to_event
  ) %>%
  mutate(
    race = factor(race, levels = c("Asian", "Black or African American", "White")),
    Health_Insurance = case_when(
      Health_Insurance_Yes == 1 ~ "Yes",
      Health_Insurance_No == 1 ~ "No",
      TRUE ~ "Unknown"
    ),
    Disease_Status = ifelse(disease_status == 1, "Case", "Control"),
    Acidosis = ifelse(Acidosis > 0, "Yes", "No"),
    Acute.renal.failure.syndrome = ifelse(Acute.renal.failure.syndrome > 0, "Yes", "No"),
    Anemia = ifelse(Anemia > 0, "Yes", "No"),
    Essential.hypertension = ifelse(Essential.hypertension > 0, "Yes", "No"),
    Diabetes.mellitus = ifelse(Diabetes.mellitus > 0, "Yes", "No"),
    Chronic.kidney.disease.stage.3 = ifelse(Chronic.kidney.disease.stage.3 > 0, "Yes", "No"),
    Chronic.kidney.disease.stage.4 = ifelse(Chronic.kidney.disease.stage.4 > 0, "Yes", "No"),
    Uses_only_Urgent_care = ifelse(Uses_only_Urgent_care == 1, "Yes", "No"),
    Uses_only_Doctors_office = ifelse(Uses_only_Doctors_office == 1, "Yes", "No"),
    Uses_only_Emergency_room = ifelse(Uses_only_Emergency_room == 1, "Yes", "No")
  )

# Define categorical and continuous variables for summary
cat_vars <- c("Disease_Status", "race", "sex_at_birth", "Health_Insurance",
              "Acidosis", "Acute.renal.failure.syndrome", "Anemia", 
              "Essential.hypertension", "Diabetes.mellitus", 
              "Chronic.kidney.disease.stage.3", "Chronic.kidney.disease.stage.4",
              "Uses_only_Urgent_care", "Uses_only_Doctors_office", "Uses_only_Emergency_room")

cont_vars <- c("age_precise", "BMI", "eGFR_ckd_epi_2021", "serum_creatinine", 
               "Systolic.blood.pressure", "time_to_event")

# Create Table 1 with race stratification
table1 <- CreateTableOne(vars = c(cat_vars, cont_vars), strata = "race", data = table1_data, factorVars = cat_vars)

# Convert the table to a data frame and clean up column names
table1_df <- as.data.frame(print(table1, showAllLevels = TRUE, printToggle = FALSE))

# Clean up the first column by removing "X", "X1", etc.
table1_df[[1]] <- gsub("^X\\d*\\.", "", table1_df[[1]])  # Regex to remove patterns like "X", "X1", "X2"

# Adjust add_header_above to match the number of columns in table1_df
kable_output <- table1_df %>%
  kbl(format = "html", booktabs = TRUE, caption = "Baseline Characteristics of Study Population by Race") %>%
  kable_styling(full_width = FALSE, font_size = 12, bootstrap_options = c("striped", "hover", "condensed")) %>%
  column_spec(1, width = "7em") %>%
  column_spec(2, width = "6em") %>%
  column_spec(3, width = "6em") %>%
  column_spec(4, width = "6em") %>%
  column_spec(5, width = "6em") %>%
  column_spec(6, width = "6em") %>%
  column_spec(7, width = "5em") %>%
  row_spec(0, bold = TRUE, background = "#f0f0f0") %>%
  row_spec(1:nrow(table1_df), hline_after = TRUE, extra_css = "height: 25px;") %>%
  add_header_above(c(" " = 1, "Asian" = 1, "Black or African American" = 1, "White" = 1, "Additional" = 2, "p-value" = 1)) %>%
  kable_classic()

# Display the table or save it as an HTML file for further use
print(kable_output)

```

# 2 YEARS

##### Data preprocessing

```{r}
library(tidyr)
library(knitr) # For kable to display tables nicely


my_dataframe <- read.csv("my_dataframe_labs_socioeconomic.csv")
# Convert raceBlack to a factor with labels "Non-Black" and "Black"
my_dataframe$racial_identity <- factor(my_dataframe$raceBlack, levels = c(0, 1), labels = c("Non-Black", "Black"))

# If you need it as a numeric variable with values 0 and 1 again
#my_dataframe$racial_identity <- as.numeric(my_dataframe$racial_identity) - 1

# # Create eGFR CKD stages as a categorical variable
# my_dataframe$eGFR_ckd_epi_2021_stage <- cut(
#   my_dataframe$eGFR_ckd_epi_2021,
#   breaks = c(-Inf, 15, 30, 60, 90, Inf),  # Define breakpoints for each CKD stage
#   labels = c("CKD Stage 5", "CKD Stage 4", "CKD Stage 3", "CKD Stage 2", "CKD Stage 1"),  # Label each CKD stage
#   include.lowest = TRUE
# )


# Filter `my_dataframe` to include only observations with a positive `time_to_event`
# Step 1: Filter for time_to_event > 0
my_dataframe <- my_dataframe[my_dataframe$time_to_event > 0, ]

# Step 2: Filter for eGFR_ckd_epi_2021 < 60
my_dataframe <- my_dataframe[my_dataframe$eGFR_ckd_epi_2021 < 60, ]

# Step 3: Filter to remove rows with NA in the BMI column
my_dataframe <- my_dataframe[!is.na(my_dataframe$BMI), ]

  
# Define the columns to remove
col_remove <- c(
  # Highly correlated variable/ redundant variables
  "height",
  "weight", 
  "highest_smoking_status_rank", 
  "Complication.of.renal.dialysis", 
  "eGFR_MDRD", "eGFR_ckd_epi_2009", 
  "Glomerular.filtration.rate.1.73.sq.M.predicted..Volume.Rate.Area..in.Serum..Plasma.or.Blood.by.Creatinine.based.formula..MDRD.", 
  "Hypertensive.heart.and.chronic.kidney.disease", 
  "Acute.renal.failure.syndrome",
  "Disorder.of.kidney.and.or.ureter",
  "unknown_if_insured",
  
  # Multicolinear variables at 0.7
  "Iron.binding.capacity..Mass.volume..in.Serum.or.Plasma",
  "Iron.saturation..Mass.Fraction..in.Serum.or.Plasma",
  "serum_creatinine",
  
  # Not significant at p <= 0.25 when population restricted to eGFR < 60 with full sample BMI and systolic blood pressure
  # "Systemic.lupus.erythematosus",
  # "Anemia",
  # "Disorder.of.muscle",
  # ""
  
  
  
  # Tian multicolinearity
  'gender',
  'person_id',
  'date_of_birth',
  'index_date',
  'censored_date',
  'event_date',
  'age_reported',
  'disease_date',
  'Nephrectomy',
  'Hemodialysis',
  'event_type',
  'End-stage renal disease',
  'Chronic renal failure', 'Chronic.kidney.disease.stage.4',
  "Transplanted.kidney.present",
  "Acute.glomerulonephritis",
  "Hemoglobin.A1c.Hemoglobin.total.in.Blood",
  
  #causes convergence issue
  "Ferritin..Mass.volume..in.Serum.or.Plasma",
  "Uses_only_Urgent_care",
  "Uses_only_Doctors_office",
  "Uses_only_Emergency_room",
  "Unknown_Routine_Health_Visit",
  
  
  # Labs
  "Iron..Mass.volume..in.Serum.or.Plasma", 
  "Triglyceride..Mass.volume..in.Serum.or.Plasma", 
  "Parathyrin.intact..Mass.volume..in.Serum.or.Plasma", 
  "Protein..Mass.volume..in.Urine", 
  "Creatinine..Mass.volume..in.Body.fluid", 
  "Erythrocyte.distribution.width..Ratio..by.Automated.count", 
  "Ferritin..Mass.volume..in.Serum.or.Plasma", 
  "Hemoglobin.A1c.Hemoglobin.total.in.Blood", 
  "Hepatitis.B.virus.surface.Ag..Presence..in.Serum.or.Plasma.by.Immunoassay", 
  "Iron.binding.capacity..Mass.volume..in.Serum.or.Plasma", 
  "Iron.saturation..Mass.Fraction..in.Serum.or.Plasma", 
  "serum_creatinine"

  
)

# Load necessary libraries
library(dplyr)
library(knitr)

# Remove specified columns from `my_dataframe`
my_dataframe <- my_dataframe[, !(names(my_dataframe) %in% col_remove)]


# Check if `person_id` is available for merging; if not, create row numbers as IDs
if (!"person_id" %in% colnames(my_dataframe)) {
  my_dataframe <- my_dataframe %>% mutate(person_id = row_number())
}

# Separate "Asian" individuals, who will always go into the test data
asian_data <- my_dataframe %>% filter(race == "Asian")

# Filter data to create training data with `eGFR_ckd_epi_2021 < 60` for Black and White individuals
train_data <- my_dataframe %>% 
  filter(race %in% c("Black or African American", "White"))
# & eGFR_ckd_epi_2021 < 60

# Separate the remaining data (full range of eGFR) for test data, including all Asian individuals
test_data <- my_dataframe %>% anti_join(train_data, by = "person_id") %>% bind_rows(asian_data)

# Separate training data by race for custom sampling
black_train_data <- train_data %>% filter(race == "Black or African American")
white_train_data <- train_data %>% filter(race == "White")

# Separate test data by race for custom sampling
black_test_data <- test_data %>% filter(race == "Black or African American")
white_test_data <- test_data %>% filter(race == "White")

# Set seed for reproducibility
set.seed(123)

# Sample 70% of Black or African American and White from train data for training
black_train <- black_train_data %>% sample_frac(0.7)
white_train <- white_train_data %>% sample_frac(0.7)

# Use the remaining 30% of Black or African American and White individuals in train data as additional test data
black_test <- anti_join(black_train_data, black_train, by = "person_id")
white_test <- anti_join(white_train_data, white_train, by = "person_id")

# Combine training data from sampled Black and White individuals
train_data <- bind_rows(black_train, white_train) %>%
  mutate(raceAsian = 0)  # Add dummy variable `raceAsian` with 0 for all rows in train data

# Combine test data from remaining train data individuals and all unrestricted test data (including all Asian data)
test_data <- bind_rows(black_test, white_test, asian_data, black_test_data, white_test_data) %>%
  mutate(raceAsian = if_else(race == "Asian", 1, 0))

# Calculate race-specific counts for cases and controls in train and test sets
train_race_case_control <- train_data %>%
  group_by(race, disease_status) %>%
  summarise(Count = n(), .groups = 'drop') %>%
  pivot_wider(names_from = disease_status, values_from = Count, values_fill = list(Count = 0)) %>%
  rename(Cases = `1`, Controls = `0`)

test_race_case_control <- test_data %>%
  group_by(race, disease_status) %>%
  summarise(Count = n(), .groups = 'drop') %>%
  pivot_wider(names_from = disease_status, values_from = Count, values_fill = list(Count = 0)) %>%
  rename(Cases = `1`, Controls = `0`)

# Add Set column and combine train and test results
train_race_case_control <- train_race_case_control %>% mutate(Set = "Train")
test_race_case_control <- test_race_case_control %>% mutate(Set = "Test")

# Create a summary table for train and test case/control counts by race
race_case_control_table <- bind_rows(train_race_case_control, test_race_case_control)

# Print the case/control summary table
cat("Race Case-Control Counts:\n")
print(kable(race_case_control_table, format = "markdown"))

# Calculate dimensions for train and test sets
dimension_table <- data.frame(
  Set = c("Train", "Test"),
  Rows = c(nrow(train_data), nrow(test_data)),
  Columns = c(ncol(train_data), ncol(test_data))
)

# Display the dimension table
cat("\nDimension Table:\n")
print(kable(dimension_table, format = "markdown"))


# Calculate race proportions in the train dataset
train_race_counts <- table(train_data$race)
train_race_proportion <- data.frame(
  Set = "Train",
  Race = names(train_race_counts),
  Count = as.numeric(train_race_counts),
  Proportion = as.numeric(train_race_counts) / sum(train_race_counts) * 100
)

# Calculate race proportions in the test dataset
test_race_counts <- table(test_data$race)
test_race_proportion <- data.frame(
  Set = "Test",
  Race = names(test_race_counts),
  Count = as.numeric(test_race_counts),
  Proportion = as.numeric(test_race_counts) / sum(test_race_counts) * 100
)

# Combine the train and test race proportions into a single table
race_proportion_table <- rbind(train_race_proportion, test_race_proportion)

# Print the race proportion table
cat("Race Proportion Table (Train and Test):\n")
print(race_proportion_table)

```

###### One variable cox: train data

```{r}
# Set a seed for reproducibility
set.seed(123)  # Use any integer you like; this ensures reproducibility of random processes

# Load necessary libraries
library(dplyr)
library(survival)

# Cap time_to_event at 2 years (730 days) for those who have not experienced the event
train_data <- train_data %>%
  mutate(
    time_to_event = pmin(time_to_event, 2 * 365.25),  # Cap time_to_event at 730 days
    disease_status = ifelse(time_to_event >= 2 * 365.25, 0, disease_status)  # Censor if event occurs after 2 years
  )

# Replace all NA values in train_data with 0 (considering if this is appropriate for all variables)
train_data[is.na(train_data)] <- 0

# Verify that there are no NA values left in the dataset
cat("Number of NA values in dataset after replacement:", sum(is.na(train_data)), "\n")  # This should print 0 if all NA values are replaced

# Identify predictor variables (excluding outcome, time-to-event, ID, and categorical variables like race)
predictor_vars <- setdiff(names(train_data), c("time_to_event", "disease_status", "person_id", "race"))

# Transform numeric variables and convert binary variables to factors
train_data <- train_data %>%
  mutate(across(all_of(predictor_vars), ~ {
    if (is.numeric(.)) {
      scale(log(. + 1))  # Apply log(x + 1) transformation and scale
    } else if (n_distinct(.) == 2) {
      as.factor(.)  # Convert binary variables to factors
    } else {
      .  # Leave other types as is
    }
  }))

# Initialize an empty data frame to store results
cox_results <- data.frame(Variable = character(),
                          p_value = numeric(),
                          Significance = character(),
                          stringsAsFactors = FALSE)

# Run Cox PH model for each variable individually
for (var in predictor_vars) {
  # Check that the variable is either numeric or factor and has more than one unique value
  if ((is.numeric(train_data[[var]]) || is.factor(train_data[[var]])) && length(unique(train_data[[var]])) > 1) {
    # Dynamically create the formula for Cox model
    formula <- as.formula(paste("Surv(time_to_event, disease_status) ~", var))
    
    # Fit Cox model with one variable at a time and handle warnings/errors
    tryCatch({
      cox_model <- coxph(formula, data = train_data)
      p_val <- summary(cox_model)$coefficients[,"Pr(>|z|)"]
      
      # Check if p-value <= 0.10 for significance
      signif <- ifelse(p_val <= 0.10, "*", "")
      
      # Append results to the results data frame
      cox_results <- rbind(cox_results, data.frame(Variable = var, 
                                                   p_value = p_val, 
                                                   Significance = signif))
    }, warning = function(w) {
      cat("Warning for variable:", var, "-", conditionMessage(w), "\n")
    }, error = function(e) {
      cat("Error for variable:", var, "-", conditionMessage(e), "\n")
    })
  } else {
    message(paste("Skipping variable:", var, "- not numeric/factor or has no variation."))
  }
}

# Arrange results by p-value in ascending order (most significant first)
cox_results <- cox_results[order(cox_results$p_value), ]

# Print the sorted results table with most significant variables first
print(cox_results)

```

###### One variable cox: test data

```{r}
# Set a seed for reproducibility
set.seed(123)  # Use any integer to enforce reproducibility of random processes

# Load necessary libraries
library(dplyr)
library(survival)

# Cap time_to_event at 2 years (730 days) for those who have not experienced the event
test_data <- test_data %>%
  mutate(
    time_to_event = pmin(time_to_event, 5 * 365.25),  # Cap time_to_event at 730 days
    disease_status = ifelse(time_to_event >= 5 * 365.25, 0, disease_status)  # Censor if event occurs after 2 years
  )

# Replace all NA values in test_data with 0 (considering if this is appropriate for all variables)
test_data[is.na(test_data)] <- 0

# Verify that there are no NA values left in the dataset
cat("Number of NA values in dataset after replacement:", sum(is.na(test_data)), "\n")  # This should print 0 if all NA values are replaced

# Identify predictor variables (excluding outcome, time-to-event, ID, and categorical variables like race)
predictor_vars <- setdiff(names(test_data), c("time_to_event", "disease_status", "person_id", "race"))

# Transform numeric variables and convert binary variables to factors
test_data <- test_data %>%
  mutate(across(all_of(predictor_vars), ~ {
    if (is.numeric(.)) {
      scale(log(. + 1))  # Apply log(x + 1) transformation and scale
    } else if (n_distinct(.) == 2) {
      as.factor(.)  # Convert binary variables to factors
    } else {
      .  # Leave other types as is
    }
  }))

# Initialize an empty data frame to store results
cox_results <- data.frame(Variable = character(),
                          p_value = numeric(),
                          Significance = character(),
                          stringsAsFactors = FALSE)

# Run Cox PH model for each variable individually
for (var in predictor_vars) {
  # Check that the variable is either numeric or factor and has more than one unique value
  if ((is.numeric(test_data[[var]]) || is.factor(test_data[[var]])) && length(unique(test_data[[var]])) > 1) {
    # Dynamically create the formula for Cox model
    formula <- as.formula(paste("Surv(time_to_event, disease_status) ~", var))
    
    # Fit Cox model with one variable at a time and handle warnings/errors
    tryCatch({
      cox_model <- coxph(formula, data = test_data)
      p_val <- summary(cox_model)$coefficients[,"Pr(>|z|)"]
      
      # Check if p-value <= 0.10 for significance
      signif <- ifelse(p_val <= 0.10, "*", "")
      
      # Append results to the results data frame
      cox_results <- rbind(cox_results, data.frame(Variable = var, 
                                                   p_value = p_val, 
                                                   Significance = signif))
    }, warning = function(w) {
      cat("Warning for variable:", var, "-", conditionMessage(w), "\n")
    }, error = function(e) {
      cat("Error for variable:", var, "-", conditionMessage(e), "\n")
    })
  } else {
    message(paste("Skipping variable:", var, "- not numeric/factor or has no variation."))
  }
}

# Arrange results by p-value in ascending order (most significant first)
cox_results <- cox_results[order(cox_results$p_value), ]

# Print the sorted results table with most significant variables first
print(cox_results)

```

# 

# Test for multicolinearity

```{r}
# Load necessary libraries
library(dplyr)
library(corrr)
library(knitr)
library(kableExtra)

# Assuming filtered_person_with_summary is the dataset after filtration
filtered_person_with_summary <- train_data

# Step 1: Select only numeric columns and identify columns with non-zero variance
numeric_columns <- sapply(filtered_person_with_summary, is.numeric)
filtered_data_numeric <- filtered_person_with_summary[, numeric_columns]  # Select only numeric columns

# Identify columns with non-zero variance and keep them
non_zero_variance_columns <- which(sapply(filtered_data_numeric, sd, na.rm = TRUE) > 0)

# Check if there are any columns with non-zero variance
if (length(non_zero_variance_columns) > 0) {
  filtered_data_no_constant <- filtered_data_numeric[, non_zero_variance_columns]  # Keep only columns with non-zero variance
} else {
  cat("All numeric columns have zero variance.\n")
  filtered_data_no_constant <- NULL
}

# Continue with correlation calculation only if there are non-constant columns
if (!is.null(filtered_data_no_constant) && ncol(filtered_data_no_constant) > 1) {
  # Step 2: Calculate the correlation matrix for the filtered dataset
  correlation_matrix <- correlate(filtered_data_no_constant, method = "pearson", use = "pairwise.complete.obs")
  
  # Step 3: Identify pairs of variables with high correlations (e.g., |correlation| > 0.7)
  highly_correlated_pairs <- correlation_matrix %>%
    stretch() %>%  # Convert the correlation matrix into a long format
    filter(abs(r) > 0.7, r != 1) %>%  # Keep only pairs with |correlation| > 0.7 and not self-correlation
    mutate(pair = paste(pmin(x, y), pmax(x, y), sep = "_")) %>%  # Create a unique identifier for pairs
    distinct(pair, .keep_all = TRUE)  # Remove duplicates (e.g., (x,y) and (y,x))

  # Remove the auxiliary 'pair' column using base R if `select` fails
  if ("pair" %in% colnames(highly_correlated_pairs)) {
    highly_correlated_pairs <- highly_correlated_pairs[, !colnames(highly_correlated_pairs) %in% "pair"]
  }

  # Step 4: Prepare the table for the number of observations for each correlated pair
  if (nrow(highly_correlated_pairs) > 0) {
    result_table <- data.frame(
      `Variable 1` = character(),
      `Variable 2` = character(),
      `Correlation` = numeric(),
      `Cases (Var 1)` = integer(),
      `Controls (Var 1)` = integer(),
      `Cases (Var 2)` = integer(),
      `Controls (Var 2)` = integer(),
      stringsAsFactors = FALSE
    )
    
    for (i in 1:nrow(highly_correlated_pairs)) {
      var1 <- highly_correlated_pairs$x[i]
      var2 <- highly_correlated_pairs$y[i]
      correlation <- highly_correlated_pairs$r[i]
      
      # Number of non-zero cases (disease_status == 1) and non-zero controls (disease_status == 0) for var1
      cases_var1 <- sum(filtered_person_with_summary$disease_status == 1 & 
                        filtered_person_with_summary[[var1]] != 0 & !is.na(filtered_person_with_summary[[var1]]))
      controls_var1 <- sum(filtered_person_with_summary$disease_status == 0 & 
                           filtered_person_with_summary[[var1]] != 0 & !is.na(filtered_person_with_summary[[var1]]))
      
      # Number of non-zero cases (disease_status == 1) and non-zero controls (disease_status == 0) for var2
      cases_var2 <- sum(filtered_person_with_summary$disease_status == 1 & 
                        filtered_person_with_summary[[var2]] != 0 & !is.na(filtered_person_with_summary[[var2]]))
      controls_var2 <- sum(filtered_person_with_summary$disease_status == 0 & 
                           filtered_person_with_summary[[var2]] != 0 & !is.na(filtered_person_with_summary[[var2]]))
      
      # Add the results to the table
      result_table <- rbind(result_table, data.frame(
        `Variable 1` = var1,
        `Variable 2` = var2,
        `Correlation` = round(correlation, 2),  # Round correlation to 2 decimal places
        `Cases (Var 1)` = cases_var1,
        `Controls (Var 1)` = controls_var1,
        `Cases (Var 2)` = cases_var2,
        `Controls (Var 2)` = controls_var2
      ))
    }
    
    # Print the result table with formatting for publication
    print(result_table)
  } else {
    cat("No highly correlated variable pairs found (|correlation| > 0.7).\n")
  }
} else {
  cat("No numeric columns with non-zero variance found for correlation analysis.\n")
}

# Print the column names of the original dataset
print(colnames(filtered_person_with_summary))

```

# **Stepwise Selection**: Using `stepAIC`

```{r}
# Define the lab-free variables as a vector of column names
cox_formula_lab_free <- c(
#"eGFR_ckd_epi_2021",
"age_precise", 
"raceBlack",
"raceWhite",
"sex_at_birth",
"Anemia.in.chronic.kidney.disease", 
"ethnicity",
"BMI", 
"Acidosis", 
"Essential.hypertension", 
"Anemia", 
"Type.2.diabetes.mellitus", 
"Hyperkalemia", 
"Polyneuropathy.due.to.diabetes.mellitus", 
#"Chronic.kidney.disease.stage.3", 
"Gout", 
"Proteinuria", 
"Renal.disorder.due.to.type.2.diabetes.mellitus",

"Atherosclerosis.of.coronary.artery.without.angina.pectoris",
"Congestive.heart.failure",
"Diabetes.mellitus",
"Disorder.of.muscle",
"Hypothyroidism",
"Iron.deficiency.anemia",
"Peripheral.vascular.disease",
"Systemic.lupus.erythematosus",
"smoking"

#"Health_Insurance_Yes",
#"Health_Insurance_No"
)



# Load necessary libraries
library(survival)
library(MASS)
library(car)
library(ggplot2)

set.seed(123)


# Define the relevant columns, including the survival time and event indicator
relevant_columns <- c("time_to_event", "disease_status", cox_formula_lab_free)

# Filter columns and remove rows with missing values using base R
train_data_filtered <- train_data[, relevant_columns, drop = FALSE]
train_data_filtered <- train_data_filtered[complete.cases(train_data_filtered), ]

# Restrict the time-to-event to 2 years to reduce censoring
train_data_filtered <- train_data_filtered[train_data_filtered$time_to_event <= 2 * 365, ]

# Step 1: Select predictors and convert any factors to numeric
predictors_only <- train_data_filtered[, !(names(train_data_filtered) %in% c("time_to_event", "disease_status"))]
predictors_only <- data.frame(lapply(predictors_only, function(x) {
  if (is.factor(x)) as.numeric(as.character(x)) else x
}))

# Step 2: Filter out constant or all-NA columns
predictors_only <- predictors_only[, sapply(predictors_only, function(x) sd(x, na.rm = TRUE) > 0 & !all(is.na(x)))]

# Step 3: Use QR decomposition to check for collinearity and identify columns to keep
qr_decomp <- qr(as.matrix(predictors_only), tol = 1e-7)
non_collinear_vars <- qr_decomp$pivot[1:qr_decomp$rank]
predictors_only <- predictors_only[, non_collinear_vars]

# Update train_data_filtered to exclude the collinear columns
train_data_filtered <- train_data_filtered[, c("time_to_event", "disease_status", colnames(predictors_only))]

# Define an initial Cox model to calculate VIF
initial_cox_model <- coxph(Surv(time_to_event, disease_status) ~ ., data = train_data_filtered)

# Calculate VIF values and handle warning by interpreting them cautiously
vif_values <- vif(initial_cox_model)
cat("VIF values (interpret with caution due to no intercept):\n")
print(vif_values)

# Identify and remove variables with high VIF values (greater than 5)
high_vif_vars <- names(vif_values)[vif_values > 5]
train_data_filtered <- train_data_filtered[, !(names(train_data_filtered) %in% high_vif_vars)]

# Define the initial Cox proportional hazards model with the reduced set of variables
cox_full_model <- coxph(Surv(time_to_event, disease_status) ~ ., data = train_data_filtered)

# Perform stepwise variable selection based on AIC
stepwise_model <- stepAIC(cox_full_model, direction = "both", trace = FALSE)

# Display selected variables and model summary
summary(stepwise_model)

# Print the AIC of the final model
cat("AIC of the final model after stepwise selection:", AIC(stepwise_model), "\n")

# Extract the coefficients of selected variables
selected_variables <- coef(stepwise_model)
selected_var_names <- names(selected_variables)
cat("Selected variables:\n")
print(selected_var_names)

# Prepare data for variable importance plot
var_importance <- data.frame(
  Variable = selected_var_names,
  Importance = abs(selected_variables)
)

# Plot variable importance
ggplot(var_importance, aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = "Variable Importance in Cox Model", x = "Variables", y = "Absolute Coefficient") +
  theme_minimal()


#########################
# Load necessary libraries
library(survival)
library(ggplot2)

# Ensure test data is prepared in the same way as the train data
# Filter out the columns that are in the final model after variable selection
test_data_filtered <- test_data[, c("time_to_event", "disease_status", selected_var_names), drop = FALSE]
test_data_filtered <- test_data_filtered[complete.cases(test_data_filtered), ]

# Cap time_to_event at 2 years (730 days) in test data to match the training setup
test_data_filtered$time_to_event <- pmin(test_data_filtered$time_to_event, 2 * 365)
test_data_filtered$disease_status <- ifelse(test_data_filtered$time_to_event >= 2 * 365, 0, test_data_filtered$disease_status)

# Predict risk scores on both training and test data using the final model
train_risk_scores <- predict(stepwise_model, newdata = train_data_filtered, type = "risk")
test_risk_scores <- predict(stepwise_model, newdata = test_data_filtered, type = "risk")

# Calculate the C-index for training data
train_c_index <- concordancefit(
  x = train_risk_scores, 
  y = Surv(train_data_filtered$time_to_event, train_data_filtered$disease_status)
)
cat("C-index for training data:", train_c_index$concordance, "\n")

# Calculate the C-index for test data
test_c_index <- concordancefit(
  x = test_risk_scores, 
  y = Surv(test_data_filtered$time_to_event, test_data_filtered$disease_status)
)
cat("C-index for test data:", test_c_index$concordance, "\n")

# Visualize Variable Importance for Selected Variables in the Final Model
var_importance <- data.frame(
  Variable = selected_var_names,
  Importance = abs(selected_variables)
)

# Plot Variable Importance
ggplot(var_importance, aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Variable Importance in Cox Model", x = "Variables", y = "Absolute Coefficient") +
  theme_minimal()


```

## PH Assumption check

```{r}
# Load necessary library
library(survival)

# Perform the proportional hazards test
ph_test <- cox.zph(stepwise_model)

# Print the test results
print(ph_test)

# Convert the test results to a data frame
ph_results <- as.data.frame(ph_test$table)

# Add a new column indicating whether the PH assumption holds or is violated
ph_results$Assumption <- ifelse(ph_results$p < 0.05, "Violated", "Holds")

# Display the results with the new column
print(ph_results)

# Separate the global test result
global_ph <- ph_results[nrow(ph_results), , drop = FALSE]  # Last row is usually the global test

# Print global test result separately
cat("Global PH Test:\n")
print(global_ph)

```

# Lasso variable selector

```{r}
# Load necessary libraries
library(survival)
library(glmnet)
library(ggplot2)

cox_formula_lab_free <- c(
"eGFR_ckd_epi_2021",
"age_precise", 
"raceBlack",
"raceWhite",
"sex_at_birth",
"Anemia.in.chronic.kidney.disease", 
"ethnicity",
"BMI", 
"Acidosis", 
"Essential.hypertension", 
"Anemia", 
"Type.2.diabetes.mellitus", 
"Hyperkalemia", 
"Polyneuropathy.due.to.diabetes.mellitus", 
"Chronic.kidney.disease.stage.3", 
"Gout", 
"Proteinuria", 
"Renal.disorder.due.to.type.2.diabetes.mellitus",

"Atherosclerosis.of.coronary.artery.without.angina.pectoris",
"Congestive.heart.failure",
"Diabetes.mellitus",
"Disorder.of.muscle",
"Hypothyroidism",
"Iron.deficiency.anemia",
"Peripheral.vascular.disease",
"Systemic.lupus.erythematosus",
"smoking"

#"Health_Insurance_Yes",
#"Health_Insurance_No"
)



# Define the relevant columns, including the survival time and event indicator
relevant_columns <- c("time_to_event", "disease_status", cox_formula_lab_free)

# Filter columns and remove rows with missing values using base R
train_data_filtered <- train_data[, relevant_columns, drop = FALSE]
train_data_filtered <- train_data_filtered[complete.cases(train_data_filtered), ]

# Restrict the time-to-event to 2 years to reduce censoring
train_data_filtered <- train_data_filtered[train_data_filtered$time_to_event <= 2 * 365, ]

# Further filter the data for eGFR_ckd_epi_2021 < 60 (assuming eGFR_ckd_epi_2021 is in cox_formula_lab_free)
train_data_filtered <- train_data_filtered[train_data_filtered$eGFR_ckd_epi_2021 < 60, ]
train_data_filtered[is.na(train_data_filtered)] <- 0  # Replace all NA values with 0

# Define predictor and response variables
predictor_vars <- setdiff(names(train_data_filtered), c("time_to_event", "disease_status"))
X <- model.matrix(~ ., data = train_data_filtered[, predictor_vars])[,-1]  # Design matrix without intercept
y <- Surv(train_data_filtered$time_to_event, train_data_filtered$disease_status)  # Survival response

# Set seed for reproducibility
set.seed(123)

# Fit LASSO model with cross-validation to select lambda
lasso_fit <- cv.glmnet(X, y, family = "cox", alpha = 0)  # LASSO (alpha = 1)

# Get the best lambda value that minimizes cross-validation error
best_lambda <- lasso_fit$lambda.min

# Fit final LASSO model with the optimal lambda
final_lasso_model <- glmnet(X, y, family = "cox", alpha = 1, lambda = best_lambda)

# Extract the selected variables (non-zero coefficients)
selected_variables <- rownames(coef(final_lasso_model))[which(coef(final_lasso_model, s = best_lambda) != 0)]
cat("Selected variables using LASSO:\n")
print(selected_variables)

# Display the coefficients of the selected variables
selected_coefficients <- coef(final_lasso_model, s = best_lambda)
print(selected_coefficients[selected_variables,])

# Plot the cross-validation curve
plot(lasso_fit)
title("LASSO Cross-Validation Curve for Cox Model")

# Prepare data for variable importance plot
var_importance <- data.frame(
  Variable = selected_variables,
  Importance = abs(as.numeric(selected_coefficients[selected_variables,]))
)

# Plot variable importance for selected variables
ggplot(var_importance, aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Variable Importance in Cox Model (LASSO)", x = "Variables", y = "Absolute Coefficient") +
  theme_minimal()

```

## PH test on the penalized cox

```{r}
# Load necessary libraries
library(survival)

# Refit the Cox model without penalization using selected variables from LASSO
selected_variables <- rownames(coef(final_lasso_model))[which(coef(final_lasso_model, s = best_lambda) != 0)]
final_formula <- as.formula(paste("Surv(time_to_event, disease_status) ~", paste(selected_variables, collapse = " + ")))
final_cox_model <- coxph(final_formula, data = train_data_filtered)

summary(final_cox_model)
# Perform the proportional hazards test
ph_test <- cox.zph(final_cox_model)

# Extract p-values and add "Assumption" column
ph_results <- as.data.frame(ph_test$table)
ph_results$Assumption <- ifelse(ph_results$p < 0.05, "Violated", "Holds")

# Display the results
print(ph_results)

```

### 

# MODEL 1: Race free

### Train

```{r}
# Load necessary libraries
library(ggplot2)
library(tidyr)
library(dplyr)
library(survival)
library(rms)
library(Hmisc)
library(pec)
library(tibble)
library(boot)
library(parallel)

# Complete case
my_dataframe <- train_data

# Count cases and controls by race
case_control_counts <- aggregate(person_id ~ race + disease_status, data = train_data, FUN = length)
names(case_control_counts) <- c("race", "disease_status", "count")
print(case_control_counts)

# Replace all NA values in train_data with 0
train_data[is.na(train_data)] <- 0
print(sum(is.na(train_data)))  # Should print 0 if all NA values are replaced

# Identify predictor variables (excluding outcome and ID)
predictor_vars <- setdiff(names(train_data), c("time_to_event", "disease_status", "person_id"))

# Step 1: Define truncation year and threshold
stop_year <- 2
threshold <- 60  # Include observations with eGFR_ckd_epi_2021 < threshold

# Step 2: Data preparation
stop_time <- stop_year * 365.25
df_complete <- my_dataframe
df_complete$time_to_event[is.na(df_complete$time_to_event)] <- stop_time

# Create `time_to_event_2` and `outcome` variables
df_complete <- transform(
  df_complete,
  time_to_event_2 = ifelse(time_to_event <= stop_time, time_to_event, stop_time),
  outcome = ifelse(disease_status == 1 & time_to_event <= stop_time, 1, 0)
)

# Filter columns and create race subgroups
selected_columns  <- c(
"time_to_event_2", "outcome","race",

#"eGFR_ckd_epi_2021",
"age_precise",
"sex_at_birth",

"Type.2.diabetes.mellitus",
"Essential.hypertension",
"Proteinuria",
"Anemia",
"Hypothyroidism",
"Iron.deficiency.anemia",
"Gout",
"Acidosis"
)

df_filtered <- df_complete[df_complete$eGFR_ckd_epi_2021 < threshold, selected_columns]

# Split data by race
dat_src <- subset(df_filtered, race %in% c('Black or African American', 'White'))
dat_tar_black <- subset(df_filtered, race == 'Black or African American')
dat_tar_white <- subset(df_filtered, race == 'White')
dat_tar_asian <- subset(df_filtered, race == 'Asian')

# Set up datadist for rms functions
dd <- datadist(df_filtered)
options(datadist = "dd")

# Step 5: Define and fit Cox model formula
cox_formula <- as.formula(paste("Surv(time_to_event_2, outcome) ~", 
                                paste(setdiff(names(df_filtered), c("time_to_event_2", "outcome", "race")), collapse = " + ")))
cox_s <- rms::cph(cox_formula, data = dat_src, x = TRUE, y = TRUE, surv = TRUE)

# Calculate AIC and BIC
cox_s_coxph <- coxph(cox_formula, data = dat_src)
model_aic <- AIC(cox_s_coxph)
model_bic <- BIC(cox_s_coxph)
cat("AIC of the Cox model:", model_aic, "\n")
cat("BIC of the Cox model:", model_bic, "\n")

# Step 6: Proportional Hazards Assumption Test
ph_test <- cox.zph(cox_s_coxph)
ph_results <- as.data.frame(ph_test$table[, "p"])
colnames(ph_results) <- "p_value"
ph_results$Assumption <- ifelse(ph_results$p_value < 0.05, "Violated", "Holds")
ph_results <- tibble::rownames_to_column(ph_results, "Variable")
print(ph_results)

# Step 7: Extract coefficients, significance, and plot variable importance
coef_values <- coef(cox_s)
z_scores <- coef_values / sqrt(diag(cox_s$var))
p_values <- 2 * (1 - pnorm(abs(z_scores)))
significance <- ifelse(p_values < 0.05, "*", "")
cleaned_variable_names <- gsub("\\[1\\]", "", names(coef_values))

signif_results <- data.frame(Variable = cleaned_variable_names,
                             Coefficient = coef_values,
                             `p-value` = p_values,
                             Significance = significance, stringsAsFactors = FALSE)
print(signif_results)

# Merge with PH assumption results
signif_results <- merge(signif_results, ph_results, by = "Variable", all.x = TRUE)
print(signif_results)

var_importance <- signif_results[order(abs(signif_results$Coefficient), decreasing = TRUE), ]
ggplot(var_importance, aes(x = reorder(Variable, abs(Coefficient)), y = abs(Coefficient))) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Variable Importance in Cox Model", x = "Variables", y = "Absolute Coefficient Estimate") +
  theme_minimal()

# Step 8: Bootstrap C-statistic confidence intervals with parallel computing
c_statistic <- function(data, indices) {
  boot_data <- data[indices, ]
  cox_s_boot <- cph(cox_formula, data = boot_data, x = TRUE, y = TRUE, surv = TRUE)
  1 - rcorr.cens(exp(predict(cox_s_boot)), Surv(boot_data$time_to_event_2, boot_data$outcome))['C Index']
}

# Set up parallel processing
n_cores <- detectCores() - 1
cl <- makeCluster(n_cores)
clusterExport(cl, c("cox_formula", "c_statistic", "dat_src", "dat_tar_black", "dat_tar_white", "dat_tar_asian"))
clusterEvalQ(cl, { library(survival); library(rms); library(Hmisc) })

# Perform parallel bootstrapping with checks for non-empty data
set.seed(123)

# Initialize results for each group
c_all_ci <- c_black_ci <- c_white_ci <- c_asian_ci <- c(NA, NA)
c_all_mean <- c_black_mean <- c_white_mean <- c_asian_mean <- NA

# Bootstrap for the "All" group
if (nrow(dat_src) > 0) {
  c_all_boot <- boot(dat_src, c_statistic, R = 50, parallel = "snow", ncpus = n_cores, cl = cl)
  c_all_ci <- boot.ci(c_all_boot, type = "perc")$percent[4:5]
  c_all_mean <- mean(c_all_boot$t)
} else {
  cat("No observations in the 'All' group. Skipping bootstrap.\n")
}

# Bootstrap for the "Black or African American" group
if (nrow(dat_tar_black) > 0) {
  c_black_boot <- boot(dat_tar_black, c_statistic, R = 50, parallel = "snow", ncpus = n_cores, cl = cl)
  c_black_ci <- boot.ci(c_black_boot, type = "perc")$percent[4:5]
  c_black_mean <- mean(c_black_boot$t)
} else {
  cat("No observations in the 'Black or African American' group. Skipping bootstrap.\n")
}

# Bootstrap for the "White" group
if (nrow(dat_tar_white) > 0) {
  c_white_boot <- boot(dat_tar_white, c_statistic, R = 50, parallel = "snow", ncpus = n_cores, cl = cl)
  c_white_ci <- boot.ci(c_white_boot, type = "perc")$percent[4:5]
  c_white_mean <- mean(c_white_boot$t)
} else {
  cat("No observations in the 'White' group. Skipping bootstrap.\n")
}

# Bootstrap for the "Asian" group (only if there is data)
if (nrow(dat_tar_asian) > 0) {
  c_asian_boot <- boot(dat_tar_asian, c_statistic, R = 50, parallel = "snow", ncpus = n_cores, cl = cl)
  c_asian_ci <- boot.ci(c_asian_boot, type = "perc")$percent[4:5]
  c_asian_mean <- mean(c_asian_boot$t)
} else {
  cat("No observations in the 'Asian' group. Skipping bootstrap.\n")
}

# Stop the cluster
stopCluster(cl)

# Display results with confidence intervals
results <- data.frame(
  Group = c('All', 'Black or African American', 'White', 'Asian'),
  C_index = c(c_all_mean, c_black_mean, c_white_mean, c_asian_mean),
  Lower_CI = c(c_all_ci[1], c_black_ci[1], c_white_ci[1], c_asian_ci[1]),
  Upper_CI = c(c_all_ci[2], c_black_ci[2], c_white_ci[2], c_asian_ci[2])
)
print(results)

# Clear datadist option
options(datadist = NULL)

```

### Test

##### AIC, C statistics

```{r}
# Load necessary libraries
library(survival)
library(rms)
library(Hmisc)
library(boot)
library(parallel)
library(ggplot2)

# Step 1: Define the truncation year and threshold
stop_year <- 2  # Truncation at year 3
threshold <- 60  # Include observations with eGFR_ckd_epi_2021 < threshold

# Step 2: Data preparation
stop_time <- stop_year * 365.25
df_complete <- test_data

# Replace NA values in time_to_event with stop_time
df_complete$time_to_event[is.na(df_complete$time_to_event)] <- stop_time

# Create time_to_event_2 and outcome variables
df_complete <- within(df_complete, {
  time_to_event_2 <- ifelse(time_to_event <= stop_time, time_to_event, stop_time)
  outcome <- ifelse(disease_status == 1 & time_to_event <= stop_time, 1, 0)
})

# Step 3: Filter and select relevant columns
required_columns  <- c(
"time_to_event_2", "outcome","race",

#"eGFR_ckd_epi_2021",
"age_precise",
"sex_at_birth",

"Type.2.diabetes.mellitus",
"Essential.hypertension",
"Proteinuria",
"Anemia",
"Hypothyroidism",
"Iron.deficiency.anemia",
"Gout",
"Acidosis"
)
missing_columns <- setdiff(required_columns, names(df_complete))
if (length(missing_columns) > 0) {
  warning("The following required columns are missing in df_complete: ", paste(missing_columns, collapse = ", "))
}

existing_columns <- intersect(required_columns, names(df_complete))
df_filtered <- df_complete[, existing_columns]
#df_filtered <- df_filtered[df_filtered$eGFR_ckd_epi_2021 < threshold, ]

# Step 4: Split data by race
dat_src <- subset(df_filtered, race %in% c('Black or African American', 'White'))
dat_tar_black <- subset(df_filtered, race == 'Black or African American')
dat_tar_white <- subset(df_filtered, race == 'White')
dat_tar_asian <- subset(df_filtered, race == 'Asian')

# Set up datadist for rms functions
dd <- datadist(dat_src)
options(datadist = "dd")

# Step 5: Define Cox model formula
cox_formula <- as.formula(paste("Surv(time_to_event_2, outcome) ~", 
                                paste(setdiff(names(df_filtered), c("time_to_event_2", "outcome", "race")), collapse = " + ")))
cox_s <- rms::cph(cox_formula, data = dat_src, x = TRUE, y = TRUE, surv = TRUE)

# Step 6: Define function to calculate C-statistic for bootstrapping
c_statistic <- function(data, indices) {
  boot_data <- data[indices, ]
  # Use type = "lp" for linear predictor
  predictions <- predict(cox_s, newdata = boot_data, type = "lp")
  1 - rcorr.cens(predictions, Surv(boot_data$time_to_event_2, boot_data$outcome))['C Index']
}

# Step 7: Set up parallel processing for bootstrapping
n_cores <- detectCores() - 1
cl <- makeCluster(n_cores)
clusterExport(cl, c("cox_s", "c_statistic", "dat_src", "dat_tar_black", "dat_tar_white", "dat_tar_asian"))
clusterEvalQ(cl, { library(survival); library(Hmisc) })

# Step 8: Perform 50 bootstraps for each group
set.seed(123)  # For reproducibility

# Initialize results for each group
c_all_ci <- c_black_ci <- c_white_ci <- c_asian_ci <- c(NA, NA)
c_all_mean <- c_black_mean <- c_white_mean <- c_asian_mean <- NA

# Helper function to run bootstraps and calculate CI
calculate_boot_ci <- function(data) {
  if (nrow(data) > 0) {
    boot_results <- boot(data, c_statistic, R = 50, parallel = "snow", ncpus = n_cores, cl = cl)
    ci <- boot.ci(boot_results, type = "perc")$percent[4:5]
    mean_val <- mean(boot_results$t, na.rm = TRUE)
    return(list(mean = mean_val, ci = ci))
  }
  return(list(mean = NA, ci = c(NA, NA)))
}

# Run bootstraps and calculate CI for each group
all_results <- calculate_boot_ci(dat_src)
c_all_mean <- all_results$mean
c_all_ci <- all_results$ci

black_results <- calculate_boot_ci(dat_tar_black)
c_black_mean <- black_results$mean
c_black_ci <- black_results$ci

white_results <- calculate_boot_ci(dat_tar_white)
c_white_mean <- white_results$mean
c_white_ci <- white_results$ci

asian_results <- calculate_boot_ci(dat_tar_asian)
c_asian_mean <- asian_results$mean
c_asian_ci <- asian_results$ci

# Stop the cluster
stopCluster(cl)

# Step 9: Display results with confidence intervals
results <- data.frame(
  Group = c('All', 'Black or African American', 'White', 'Asian'),
  C_index = c(c_all_mean, c_black_mean, c_white_mean, c_asian_mean),
  Lower_CI = c(c_all_ci[1], c_black_ci[1], c_white_ci[1], c_asian_ci[1]),
  Upper_CI = c(c_all_ci[2], c_black_ci[2], c_white_ci[2], c_asian_ci[2])
)
print(results)

# Clear datadist option after you are done
options(datadist = NULL)

```

##### Observed vs Predicted, **Nam and D’Agostino** **chi square statistic**

```{r}
# Load necessary libraries
library(survival)
library(rms)
library(Hmisc)
library(pec)
library(ggplot2)

# Step 1: Define the truncation year and threshold
stop_year <- 5  # Truncation at year 5
threshold <- 60  # Include observations with eGFR_ckd_epi_2021 < threshold
stop_time <- stop_year * 365.25

# Step 2: Data preparation
df_complete <- test_data
df_complete$time_to_event[is.na(df_complete$time_to_event)] <- stop_time

# Create time_to_event_2 and outcome variables using base R
df_complete$time_to_event_2 <- ifelse(df_complete$time_to_event <= stop_time, df_complete$time_to_event, stop_time)
df_complete$outcome <- ifelse(df_complete$disease_status == 1 & df_complete$time_to_event <= stop_time, 1, 0)

# Step 3: Filter and select relevant columns using base R
required_columns  <- c(
"time_to_event_2", "outcome","race",

"eGFR_ckd_epi_2021",
"age_precise",
"sex_at_birth",

"Type.2.diabetes.mellitus",
"Essential.hypertension",
"Proteinuria",
"Anemia",
"Hypothyroidism",
"Iron.deficiency.anemia",
"Gout",
"Acidosis"
)

# Select only existing columns from `required_columns` and filter for eGFR threshold
existing_columns <- intersect(required_columns, names(df_complete))
df_filtered <- df_complete[existing_columns]
df_filtered <- df_filtered[df_filtered$eGFR_ckd_epi_2021 < threshold, ]

# Step 4: Split data by race using base R
dat_src <- subset(df_filtered, race %in% c('Black or African American', 'White'))

# Convert necessary variables to appropriate types
dat_src$eGFR_ckd_epi_2021 <- as.numeric(dat_src$eGFR_ckd_epi_2021)
dat_src$age_precise <- as.numeric(dat_src$age_precise)
dat_src$sex_at_birth <- as.factor(dat_src$sex_at_birth)
dat_src$Essential.hypertension <- as.factor(dat_src$Essential.hypertension)
dat_src$Type.2.diabetes.mellitus <- as.factor(dat_src$Type.2.diabetes.mellitus)

# Ensure factor levels in df_filtered match those in dat_src
for (col in names(dat_src)) {
  if (is.factor(dat_src[[col]])) {
    df_filtered[[col]] <- factor(df_filtered[[col]], levels = levels(dat_src[[col]]))
  }
}

# Set up datadist for rms functions
dd <- datadist(dat_src)
options(datadist = "dd")

# Step 5: Define and fit Cox model formula
cox_formula <- as.formula(paste(
  "Surv(time_to_event_2, outcome) ~", 
  paste(setdiff(names(dat_src), c("time_to_event_2", "outcome", "race")), collapse = " + ")
))
cox_s <- rms::cph(cox_formula, data = dat_src, x = TRUE, y = TRUE, surv = TRUE)

# Step 7: Predict risks for calibration
predicted_surv <- predictSurvProb(cox_s, newdata = df_filtered, times = stop_time)
predicted_risk <- 1 - predicted_surv  # Probability of event (1 - survival probability)

# Create predicted risk column
df_filtered$predicted_risk <- predicted_risk

# Step 8: Create quintiles based on predicted risk
quintile_breaks <- quantile(df_filtered$predicted_risk, probs = seq(0, 1, 0.2), na.rm = TRUE)
df_filtered$predicted_risk_quintile <- cut(df_filtered$predicted_risk, breaks = quintile_breaks, include.lowest = TRUE)

# Calculate observed and predicted event rates in each quintile
calibration_data <- aggregate(
  list(Observed = df_filtered$outcome, Predicted = df_filtered$predicted_risk), 
  by = list(Quintile = df_filtered$predicted_risk_quintile), 
  FUN = mean
)
calibration_data$Observed <- calibration_data$Observed * 100  # Convert to percentages
calibration_data$Predicted <- calibration_data$Predicted * 100  # Convert to percentages

# Print calibration table
print(calibration_data)

# Reshape calibration_data to long format for ggplot
library(reshape2)
calibration_long <- melt(calibration_data, id.vars = "Quintile", variable.name = "Type", value.name = "Probability")

# Plot calibration curve with reduced spacing between Observed and Predicted bars
ggplot(calibration_long, aes(x = Quintile, y = Probability, fill = Type)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.5), width = 0.45) +
  scale_fill_manual(values = c("Observed" = "darkgrey", "Predicted" = "black")) +
  labs(title = "Observed vs Predicted Probability of Event by Predicted Risk Quintile",
       x = "Predicted Risk Quintile",
       y = "Probability of Event (%)") +
  theme_minimal()

# Step 10: Calculate Nam and D'Agostino chi-squared statistic for calibration
observed_counts <- aggregate(outcome ~ predicted_risk_quintile, data = df_filtered, sum)
total_counts <- aggregate(outcome ~ predicted_risk_quintile, data = df_filtered, length)
predicted_counts <- aggregate(predicted_risk ~ predicted_risk_quintile, data = df_filtered, sum)

# Convert total and predicted counts to numeric if they are factors
observed_counts$total <- as.numeric(total_counts$outcome)
predicted_counts$expected <- as.numeric(predicted_counts$predicted_risk) * observed_counts$total
chi_square_stat <- sum((observed_counts$outcome - predicted_counts$expected)^2 / predicted_counts$expected, na.rm = TRUE)
cat("Nam and D'Agostino chi-squared statistic:", chi_square_stat, "\n")

# Final display of calibration data
print(calibration_data)

```

##### Shaply

```{r}
#install.packages("iml")
#install.packages("survival")
library(iml)
library(survival)


# Identify model features (exclude time_to_event_2 and outcome as they're not predictors)
model_features <- all.vars(cox_formula)[-c(1, 2)]

# Filter df_filtered to only include the model features
df_filtered_model <- df_filtered[, model_features, drop = FALSE]

# Define a prediction function for the Cox model
predict_survival <- function(model, newdata) {
  # Get survival probabilities for a specific time (e.g., 5 years)
  time_horizon <- 5 * 365.25  # 5 years in days
  predicted_survival <- predictSurvProb(model, newdata = newdata, times = time_horizon)
  risk_score <- 1 - predicted_survival  # Convert survival probability to risk
  return(risk_score)
}

# Wrap the model and filtered data in a Predictor object
predictor <- Predictor$new(model = cox_s, data = df_filtered_model, y = df_filtered$outcome, predict.function = predict_survival)

# Compute Shapley values for an instance (e.g., the first observation)
shapley_values <- Shapley$new(predictor, x.interest = df_filtered_model[1, ])

# Plot Shapley values without numbers in labels
shapley_plot <- shapley_values$plot()

# Adjust the plot to remove exact values from labels and make it cleaner for publication
shapley_plot + 
  coord_flip() +  # Horizontal bars
  theme_minimal() +  # Clean theme
  labs(title = "Shapley Values for Model Features",
       x = "Feature Contribution (phi)",
       y = "Feature") +
  theme(axis.text.y = element_text(size = 10)) +  # Adjust label text size
  theme(plot.title = element_text(hjust = 0.5))  # Center title, Remove Exact Values from Labels

# Extract Shapley values into a data frame
shapley_df <- shapley_values$results

# Remove numbers from feature labels by extracting only the feature name before the "=" symbol
shapley_df$feature <- gsub("=.*", "", shapley_df$feature.value)  # Keep only text before "="

# Plot Shapley values using ggplot2 without numbers in the labels
library(ggplot2)

ggplot(shapley_df, aes(x = reorder(feature, phi), y = phi)) +
  geom_bar(stat = "identity") +
  coord_flip() +  # Horizontal bars
  theme_minimal() +  # Clean theme
  labs(title = "Shapley Values for Model Features",
       x = "Feature Contribution (phi)",
       y = "Feature") +
  theme(axis.text.y = element_text(size = 10),  # Adjust label text size for readability
        plot.title = element_text(hjust = 0.5))  # Center the plot title

```

# MODEL 2: Race Specific

### Train

```{r}
# Load necessary libraries
library(ggplot2)
library(tidyr)
library(dplyr)
library(survival)
library(rms)
library(Hmisc)
library(pec)
library(tibble)

# Complete case
my_dataframe <- train_data

# Count cases and controls by race
case_control_counts <- aggregate(
  person_id ~ race + disease_status, 
  data = train_data, 
  FUN = length
)
# Rename columns for clarity
names(case_control_counts) <- c("race", "disease_status", "count")
print(case_control_counts)

# Replace all NA values in train_data with 0
train_data[is.na(train_data)] <- 0
print(sum(is.na(train_data)))  # This should print 0 if all NA values are replaced

# Identify predictor variables (excluding outcome and ID)
predictor_vars <- setdiff(names(train_data), c("time_to_event", "disease_status", "person_id"))

# Step 1: Define truncation year and threshold
stop_year <- 2
threshold <- 60  # Include observations with eGFR_ckd_epi_2021 < threshold

# Step 2: Data preparation
stop_time <- stop_year * 365.25
df_complete <- my_dataframe
df_complete$time_to_event[is.na(df_complete$time_to_event)] <- stop_time

# Create `time_to_event_2` and `outcome` variables
df_complete <- transform(
  df_complete,
  time_to_event_2 = ifelse(time_to_event <= stop_time, time_to_event, stop_time),
  outcome = ifelse(disease_status == 1 & time_to_event <= stop_time, 1, 0)
)

# Step 3: Filter data and select relevant columns using base R subsetting
 # BEST Race Model AIC of the Cox model: 3728.815    (With race and ethnicity, PH assumption holds)
selected_columns  <- c(
"time_to_event_2", "outcome", "race",

"racial_identity",
"ethnicity",

"eGFR_ckd_epi_2021",
"age_precise",
"sex_at_birth",


"Gout",
"Congestive.heart.failure",
"Type.2.diabetes.mellitus",
"Essential.hypertension",
"BMI",
"Anemia",
"Atherosclerosis.of.coronary.artery.without.angina.pectoris",
"Hypothyroidism"
)


df_filtered <- df_complete[df_complete$eGFR_ckd_epi_2021 < threshold, selected_columns]

# Step 4: Split data by race
dat_src <- subset(df_filtered, race %in% c('Black or African American', 'White'))
dat_tar_black <- subset(df_filtered, race == 'Black or African American')
dat_tar_white <- subset(df_filtered, race == 'White')
dat_tar_asian <- subset(df_filtered, race == 'Asian')

# Define datadist for rms functions
dd <- datadist(df_filtered)
options(datadist = "dd")

# Step 5: Define and fit Cox model formula
cox_formula <- as.formula(paste("Surv(time_to_event_2, outcome) ~", 
                                paste(setdiff(names(df_filtered), c("time_to_event_2", "outcome", "race")), collapse = " + ")))
cox_s <- rms::cph(cox_formula, data = dat_src, x = TRUE, y = TRUE, surv = TRUE)


# Calculate AIC and BIC for the model using coxph from survival package
cox_s_coxph <- coxph(cox_formula, data = dat_src)
model_aic <- AIC(cox_s_coxph)
model_bic <- BIC(cox_s_coxph)  # BIC calculation
cat("AIC of the Cox model:", model_aic, "\n")
cat("BIC of the Cox model:", model_bic, "\n")


# Extract coefficients, standard errors, z-scores, and p-values
coef_values <- coef(cox_s)
z_scores <- coef_values / sqrt(diag(cox_s$var))
p_values <- 2 * (1 - pnorm(abs(z_scores)))
significance <- ifelse(p_values < 0.05, "*", "")

# Clean up variable names by removing "[1]"
cleaned_variable_names <- gsub("\\[1\\]", "", names(coef_values))

# Create a summary table with cleaned variable names
signif_results <- data.frame(
  Variable = cleaned_variable_names,
  Coefficient = coef_values,
  `p-value` = p_values,
  Significance = significance,
  stringsAsFactors = FALSE
)
print(signif_results)

# Order variables by absolute coefficient values
var_importance <- signif_results[order(abs(signif_results$Coefficient), decreasing = TRUE), ]
print(var_importance)

# Plot Variable Importance
ggplot(var_importance, aes(x = reorder(Variable, abs(Coefficient)), y = abs(Coefficient))) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Variable Importance in Cox Model", x = "Variables", y = "Absolute Coefficient Estimate") +
  theme_minimal()

# Step 6: Predictions and evaluation for each target group
time_test <- seq(0, 1.5, 0.01)
surv_s <- predictSurvProb(cox_s, newdata = dat_src, times = time_test)
surv_t_black <- predictSurvProb(cox_s, newdata = dat_tar_black, times = time_test)
surv_t_white <- predictSurvProb(cox_s, newdata = dat_tar_white, times = time_test)

# Prepare model matrices for evaluation
X_tar_black <- model.matrix(cox_formula, data = dat_tar_black)[, -1]
X_tar_white <- model.matrix(cox_formula, data = dat_tar_white)[, -1]

# C-index Evaluation
C_index <- c(
  Black = 1 - rcorr.cens(exp(X_tar_black %*% coef(cox_s)), Surv(dat_tar_black$time_to_event_2, dat_tar_black$outcome))['C Index'],
  White = 1 - rcorr.cens(exp(X_tar_white %*% coef(cox_s)), Surv(dat_tar_white$time_to_event_2, dat_tar_white$outcome))['C Index']
)
results <- data.frame(Group = c('Black or African American', 'White'), C_index = C_index)
print(results)

# Construct Cox model equation
cox_equation <- paste0("h(t) = h0(t) * exp(", 
                       paste(cleaned_variable_names, round(coef(cox_s), 4), sep = " * ", collapse = " + "), 
                       ")")
cat("Cox Model Equation:\n", cox_equation, "\n")

# Proportional Hazards Test
cox_s_coxph <- coxph(cox_formula, data = dat_src)
ph_test <- cox.zph(cox_s_coxph)
ph_results <- as.data.frame(ph_test$table[, "p"])
colnames(ph_results) <- "p_value"
ph_results$Assumption <- ifelse(ph_results$p_value < 0.05, "Violated", "Holds")
ph_results <- tibble::rownames_to_column(ph_results, "Variable")
print(ph_results)

# Clear datadist option
options(datadist = NULL)

```

```{r}
# Load necessary libraries
library(ggplot2)
library(tidyr)
library(dplyr)
library(survival)
library(rms)
library(Hmisc)
library(pec)
library(boot)
library(parallel)

# Complete case
my_dataframe <- train_data

# Count cases and controls by race
case_control_counts <- aggregate(
  person_id ~ race + disease_status, 
  data = train_data, 
  FUN = length
)
# Rename columns for clarity
names(case_control_counts) <- c("race", "disease_status", "count")
print(case_control_counts)

# Replace all NA values in train_data with 0
train_data[is.na(train_data)] <- 0
print(sum(is.na(train_data)))  # This should print 0 if all NA values are replaced

# Identify predictor variables (excluding outcome and ID)
predictor_vars <- setdiff(names(train_data), c("time_to_event", "disease_status", "person_id"))

# Step 1: Define truncation year and threshold
stop_year <- 2
threshold <- 60

# Step 2: Data preparation
stop_time <- stop_year * 365.25
df_complete <- my_dataframe
df_complete$time_to_event[is.na(df_complete$time_to_event)] <- stop_time

# Create `time_to_event_2` and `outcome` variables
df_complete <- transform(
  df_complete,
  time_to_event_2 = ifelse(time_to_event <= stop_time, time_to_event, stop_time),
  outcome = ifelse(disease_status == 1 & time_to_event <= stop_time, 1, 0)
)

# Filter data
selected_columns <- c(
  "time_to_event_2", "outcome", "race", "racial_identity", "ethnicity",
  "eGFR_ckd_epi_2021", "age_precise", "sex_at_birth",
  "Gout", "Congestive.heart.failure", "Type.2.diabetes.mellitus",
  "Essential.hypertension", "BMI", "Anemia",
  "Atherosclerosis.of.coronary.artery.without.angina.pectoris", "Hypothyroidism"
)
df_filtered <- df_complete[df_complete$eGFR_ckd_epi_2021 < threshold, selected_columns]

# Define datadist for rms functions
dd <- datadist(df_filtered)
options(datadist = "dd")

# Define and fit Cox model formula (excluding race for subgroup analysis)
cox_formula <- as.formula(paste("Surv(time_to_event_2, outcome) ~", 
                                paste(setdiff(names(df_filtered), c("time_to_event_2", "outcome", "race")), collapse = " + ")))
cox_s <- rms::cph(cox_formula, data = df_filtered, x = TRUE, y = TRUE, surv = TRUE)

# Calculate AIC and BIC for the model
cox_s_coxph <- coxph(cox_formula, data = df_filtered)
model_aic <- AIC(cox_s_coxph)
model_bic <- BIC(cox_s_coxph)
cat("AIC of the Cox model:", model_aic, "\n")
cat("BIC of the Cox model:", model_bic, "\n")

# Define a function to calculate C-index for bootstrap samples
c_statistic <- function(data, indices) {
  boot_data <- data[indices, ]
  cox_s_boot <- cph(cox_formula, data = boot_data, x = TRUE, y = TRUE, surv = TRUE)
  return(1 - rcorr.cens(exp(predict(cox_s_boot)), Surv(boot_data$time_to_event_2, boot_data$outcome))['C Index'])
}

# Set up parallel processing
n_cores <- detectCores() - 1
cl <- makeCluster(n_cores)
clusterExport(cl, c("cox_formula", "c_statistic", "df_filtered"))
clusterEvalQ(cl, { library(survival); library(rms); library(Hmisc) })

# Run bootstrap with 5000 replications
set.seed(123)  # For reproducibility
boot_results <- boot(df_filtered, c_statistic, R = 50, parallel = "snow", ncpus = n_cores, cl = cl)

# Stop the cluster
stopCluster(cl)

# Calculate 95% Confidence Intervals from bootstrap results
ci <- boot.ci(boot_results, type = "perc")$percent[4:5]
c_index_mean <- mean(boot_results$t, na.rm = TRUE)
cat("C-index for all populations:", c_index_mean, "\n")
cat("95% CI for C-index:", ci, "\n")

# Display results in a data frame
results <- data.frame(
  Group = "All",
  C_index = c_index_mean,
  Lower_CI = ci[1],
  Upper_CI = ci[2]
)
print(results)

# Clear datadist option
options(datadist = NULL)

```

### Test

##### AIC, C statistics

```{r}
# Load necessary libraries
library(survival)
library(rms)
library(Hmisc)
library(boot)
library(parallel)
library(ggplot2)

# Step 1: Define the truncation year and threshold
stop_year <- 2  # Truncation at year 3
threshold <- 60  # Include observations with eGFR_ckd_epi_2021 < threshold

# Step 2: Data preparation
stop_time <- stop_year * 365.25
df_complete <- test_data

# Replace NA values in time_to_event with stop_time
df_complete$time_to_event[is.na(df_complete$time_to_event)] <- stop_time

# Create time_to_event_2 and outcome variables
df_complete <- within(df_complete, {
  time_to_event_2 <- ifelse(time_to_event <= stop_time, time_to_event, stop_time)
  outcome <- ifelse(disease_status == 1 & time_to_event <= stop_time, 1, 0)
})

# Step 3: Filter and select relevant columns
required_columns<- c(
"time_to_event_2", "outcome", "race",

"racial_identity",
"ethnicity",

"eGFR_ckd_epi_2021",
"age_precise",
"sex_at_birth",


"Gout",
"Congestive.heart.failure",
"Type.2.diabetes.mellitus",
"Essential.hypertension",
"BMI",
"Anemia",
"Atherosclerosis.of.coronary.artery.without.angina.pectoris",
"Hypothyroidism"
)

missing_columns <- setdiff(required_columns, names(df_complete))
if (length(missing_columns) > 0) {
  warning("The following required columns are missing in df_complete: ", paste(missing_columns, collapse = ", "))
}

existing_columns <- intersect(required_columns, names(df_complete))
df_filtered <- df_complete[, existing_columns]
df_filtered <- df_filtered[df_filtered$eGFR_ckd_epi_2021 < threshold, ]

# Step 4: Split data by race
dat_src <- subset(df_filtered, race %in% c('Black or African American', 'White'))
dat_tar_black <- subset(df_filtered, race == 'Black or African American')
dat_tar_white <- subset(df_filtered, race == 'White')
dat_tar_asian <- subset(df_filtered, race == 'Asian')

# Set up datadist for rms functions
dd <- datadist(dat_src)
options(datadist = "dd")

# Step 5: Define Cox model formula
cox_formula <- as.formula(paste("Surv(time_to_event_2, outcome) ~", 
                                paste(setdiff(names(df_filtered), c("time_to_event_2", "outcome", "race")), collapse = " + ")))
cox_s <- rms::cph(cox_formula, data = dat_src, x = TRUE, y = TRUE, surv = TRUE)

# Step 6: Define function to calculate C-statistic for bootstrapping
c_statistic <- function(data, indices) {
  boot_data <- data[indices, ]
  # Use type = "lp" for linear predictor
  predictions <- predict(cox_s, newdata = boot_data, type = "lp")
  1 - rcorr.cens(predictions, Surv(boot_data$time_to_event_2, boot_data$outcome))['C Index']
}

# Step 7: Set up parallel processing for bootstrapping
n_cores <- detectCores() - 1
cl <- makeCluster(n_cores)
clusterExport(cl, c("cox_s", "c_statistic", "dat_src", "dat_tar_black", "dat_tar_white", "dat_tar_asian"))
clusterEvalQ(cl, { library(survival); library(Hmisc) })

# Step 8: Perform 50 bootstraps for each group
set.seed(123)  # For reproducibility

# Initialize results for each group
c_all_ci <- c_black_ci <- c_white_ci <- c_asian_ci <- c(NA, NA)
c_all_mean <- c_black_mean <- c_white_mean <- c_asian_mean <- NA

# Helper function to run bootstraps and calculate CI
calculate_boot_ci <- function(data) {
  if (nrow(data) > 0) {
    boot_results <- boot(data, c_statistic, R = 50, parallel = "snow", ncpus = n_cores, cl = cl)
    ci <- boot.ci(boot_results, type = "perc")$percent[4:5]
    mean_val <- mean(boot_results$t, na.rm = TRUE)
    return(list(mean = mean_val, ci = ci))
  }
  return(list(mean = NA, ci = c(NA, NA)))
}

# Run bootstraps and calculate CI for each group
all_results <- calculate_boot_ci(dat_src)
c_all_mean <- all_results$mean
c_all_ci <- all_results$ci

black_results <- calculate_boot_ci(dat_tar_black)
c_black_mean <- black_results$mean
c_black_ci <- black_results$ci

white_results <- calculate_boot_ci(dat_tar_white)
c_white_mean <- white_results$mean
c_white_ci <- white_results$ci

asian_results <- calculate_boot_ci(dat_tar_asian)
c_asian_mean <- asian_results$mean
c_asian_ci <- asian_results$ci

# Stop the cluster
stopCluster(cl)

# Step 9: Display results with confidence intervals
results <- data.frame(
  Group = c('All', 'Black or African American', 'White', 'Asian'),
  C_index = c(c_all_mean, c_black_mean, c_white_mean, c_asian_mean),
  Lower_CI = c(c_all_ci[1], c_black_ci[1], c_white_ci[1], c_asian_ci[1]),
  Upper_CI = c(c_all_ci[2], c_black_ci[2], c_white_ci[2], c_asian_ci[2])
)
print(results)

# Clear datadist option after you are done
options(datadist = NULL)

```

##### Observed vs Predicted, **Nam and D’Agostino** **chi square statistic**

```{r}
# Load necessary libraries
library(survival)
library(rms)
library(Hmisc)
library(pec)
library(ggplot2)

# Step 1: Define the truncation year and threshold
stop_year <- 5  # Truncation at year 5
threshold <- 60  # Include observations with eGFR_ckd_epi_2021 < threshold
stop_time <- stop_year * 365.25

# Step 2: Data preparation
df_complete <- test_data
df_complete$time_to_event[is.na(df_complete$time_to_event)] <- stop_time

# Create time_to_event_2 and outcome variables using base R
df_complete$time_to_event_2 <- ifelse(df_complete$time_to_event <= stop_time, df_complete$time_to_event, stop_time)
df_complete$outcome <- ifelse(df_complete$disease_status == 1 & df_complete$time_to_event <= stop_time, 1, 0)

# Step 3: Filter and select relevant columns using base R
required_columns  <- c(
"time_to_event_2", "outcome", "race",

"racial_identity",
"ethnicity",

"eGFR_ckd_epi_2021",
"age_precise",
"sex_at_birth",


"Gout",
"Congestive.heart.failure",
"Type.2.diabetes.mellitus",
"Essential.hypertension",
"BMI",
"Anemia",
"Atherosclerosis.of.coronary.artery.without.angina.pectoris",
"Hypothyroidism"
)


# Select only existing columns from `required_columns` and filter for eGFR threshold
existing_columns <- intersect(required_columns, names(df_complete))
df_filtered <- df_complete[existing_columns]
df_filtered <- df_filtered[df_filtered$eGFR_ckd_epi_2021 < threshold, ]

# Step 4: Split data by race using base R
dat_src <- subset(df_filtered, race %in% c('Black or African American', 'White'))

# Convert necessary variables to appropriate types
dat_src$eGFR_ckd_epi_2021 <- as.numeric(dat_src$eGFR_ckd_epi_2021)
dat_src$age_precise <- as.numeric(dat_src$age_precise)
dat_src$sex_at_birth <- as.factor(dat_src$sex_at_birth)
dat_src$Essential.hypertension <- as.factor(dat_src$Essential.hypertension)
dat_src$Type.2.diabetes.mellitus <- as.factor(dat_src$Type.2.diabetes.mellitus)

# Ensure factor levels in df_filtered match those in dat_src
for (col in names(dat_src)) {
  if (is.factor(dat_src[[col]])) {
    df_filtered[[col]] <- factor(df_filtered[[col]], levels = levels(dat_src[[col]]))
  }
}

# Set up datadist for rms functions
dd <- datadist(dat_src)
options(datadist = "dd")

# Step 5: Define and fit Cox model formula
cox_formula <- as.formula(paste(
  "Surv(time_to_event_2, outcome) ~", 
  paste(setdiff(names(dat_src), c("time_to_event_2", "outcome", "race")), collapse = " + ")
))
cox_s <- rms::cph(cox_formula, data = dat_src, x = TRUE, y = TRUE, surv = TRUE)

# Step 7: Predict risks for calibration
predicted_surv <- predictSurvProb(cox_s, newdata = df_filtered, times = stop_time)
predicted_risk <- 1 - predicted_surv  # Probability of event (1 - survival probability)

# Create predicted risk column
df_filtered$predicted_risk <- predicted_risk

# Step 8: Create quintiles based on predicted risk
quintile_breaks <- quantile(df_filtered$predicted_risk, probs = seq(0, 1, 0.2), na.rm = TRUE)
df_filtered$predicted_risk_quintile <- cut(df_filtered$predicted_risk, breaks = quintile_breaks, include.lowest = TRUE)

# Calculate observed and predicted event rates in each quintile
calibration_data <- aggregate(
  list(Observed = df_filtered$outcome, Predicted = df_filtered$predicted_risk), 
  by = list(Quintile = df_filtered$predicted_risk_quintile), 
  FUN = mean
)
calibration_data$Observed <- calibration_data$Observed * 100  # Convert to percentages
calibration_data$Predicted <- calibration_data$Predicted * 100  # Convert to percentages

# Print calibration table
print(calibration_data)

# Reshape calibration_data to long format for ggplot
library(reshape2)
calibration_long <- melt(calibration_data, id.vars = "Quintile", variable.name = "Type", value.name = "Probability")

# Plot calibration curve with reduced spacing between Observed and Predicted bars
ggplot(calibration_long, aes(x = Quintile, y = Probability, fill = Type)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.5), width = 0.45) +
  scale_fill_manual(values = c("Observed" = "darkgrey", "Predicted" = "black")) +
  labs(title = "Observed vs Predicted Probability of Event by Predicted Risk Quintile",
       x = "Predicted Risk Quintile",
       y = "Probability of Event (%)") +
  theme_minimal()

# Step 10: Calculate Nam and D'Agostino chi-squared statistic for calibration
observed_counts <- aggregate(outcome ~ predicted_risk_quintile, data = df_filtered, sum)
total_counts <- aggregate(outcome ~ predicted_risk_quintile, data = df_filtered, length)
predicted_counts <- aggregate(predicted_risk ~ predicted_risk_quintile, data = df_filtered, sum)

# Convert total and predicted counts to numeric if they are factors
observed_counts$total <- as.numeric(total_counts$outcome)
predicted_counts$expected <- as.numeric(predicted_counts$predicted_risk) * observed_counts$total
chi_square_stat <- sum((observed_counts$outcome - predicted_counts$expected)^2 / predicted_counts$expected, na.rm = TRUE)
cat("Nam and D'Agostino chi-squared statistic:", chi_square_stat, "\n")

# Final display of calibration data
print(calibration_data)

```

##### Shaply

```{r}
#install.packages("iml")
#install.packages("survival")
library(iml)
library(survival)


# Identify model features (exclude time_to_event_2 and outcome as they're not predictors)
model_features <- all.vars(cox_formula)[-c(1, 2)]

# Filter df_filtered to only include the model features
df_filtered_model <- df_filtered[, model_features, drop = FALSE]

# Define a prediction function for the Cox model
predict_survival <- function(model, newdata) {
  # Get survival probabilities for a specific time (e.g., 5 years)
  time_horizon <- 5 * 365.25  # 5 years in days
  predicted_survival <- predictSurvProb(model, newdata = newdata, times = time_horizon)
  risk_score <- 1 - predicted_survival  # Convert survival probability to risk
  return(risk_score)
}

# Wrap the model and filtered data in a Predictor object
predictor <- Predictor$new(model = cox_s, data = df_filtered_model, y = df_filtered$outcome, predict.function = predict_survival)

# Compute Shapley values for an instance (e.g., the first observation)
shapley_values <- Shapley$new(predictor, x.interest = df_filtered_model[1, ])

# Plot Shapley values without numbers in labels
shapley_plot <- shapley_values$plot()

# Adjust the plot to remove exact values from labels and make it cleaner for publication
shapley_plot + 
  coord_flip() +  # Horizontal bars
  theme_minimal() +  # Clean theme
  labs(title = "Shapley Values for Model Features",
       x = "Feature Contribution (phi)",
       y = "Feature") +
  theme(axis.text.y = element_text(size = 10)) +  # Adjust label text size
  theme(plot.title = element_text(hjust = 0.5))  # Center title, Remove Exact Values from Labels

# Extract Shapley values into a data frame
shapley_df <- shapley_values$results

# Remove numbers from feature labels by extracting only the feature name before the "=" symbol
shapley_df$feature <- gsub("=.*", "", shapley_df$feature.value)  # Keep only text before "="

# Plot Shapley values using ggplot2 without numbers in the labels
library(ggplot2)

ggplot(shapley_df, aes(x = reorder(feature, phi), y = phi)) +
  geom_bar(stat = "identity") +
  coord_flip() +  # Horizontal bars
  theme_minimal() +  # Clean theme
  labs(title = "Shapley Values for Model Features",
       x = "Feature Contribution (phi)",
       y = "Feature") +
  theme(axis.text.y = element_text(size = 10),  # Adjust label text size for readability
        plot.title = element_text(hjust = 0.5))  # Center the plot title

```
