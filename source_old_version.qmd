---
title: "source"
format: html
editor: visual
---
# Data

```{r}
library(tidyverse)
library(bigrquery)

# This query represents dataset "followup_cohort_data" for domain "person" and was generated for All of Us Controlled Tier Dataset v7
dataset_62031819_person_sql <- paste("
    SELECT
        person.person_id,
        p_gender_concept.concept_name as gender,
        person.birth_datetime as date_of_birth,
        p_race_concept.concept_name as race,
        p_ethnicity_concept.concept_name as ethnicity,
        p_sex_at_birth_concept.concept_name as sex_at_birth 
    FROM
        `person` person 
    LEFT JOIN
        `concept` p_gender_concept 
            ON person.gender_concept_id = p_gender_concept.concept_id 
    LEFT JOIN
        `concept` p_race_concept 
            ON person.race_concept_id = p_race_concept.concept_id 
    LEFT JOIN
        `concept` p_ethnicity_concept 
            ON person.ethnicity_concept_id = p_ethnicity_concept.concept_id 
    LEFT JOIN
        `concept` p_sex_at_birth_concept 
            ON person.sex_at_birth_concept_id = p_sex_at_birth_concept.concept_id  
    WHERE
        person.PERSON_ID IN (SELECT
            distinct person_id  
        FROM
            `cb_search_person` cb_search_person  
        WHERE
            cb_search_person.person_id IN (SELECT
                person_id 
            FROM
                `person` p 
            WHERE
                race_concept_id IN (8527, 8516, 2100000001, 8515, 903096, 2000000008, 45882607, 1177221, 38003615, 8557) ) 
            AND cb_search_person.person_id NOT IN (SELECT
                criteria.person_id 
            FROM
                (SELECT
                    DISTINCT person_id, entry_date, concept_id 
                FROM
                    `cb_search_all_events` 
                WHERE
                    (concept_id IN(SELECT
                        DISTINCT c.concept_id 
                    FROM
                        `cb_criteria` c 
                    JOIN
                        (SELECT
                            CAST(cr.id as string) AS id       
                        FROM
                            `cb_criteria` cr       
                        WHERE
                            concept_id IN (440508)       
                            AND full_text LIKE '%_rank1]%'      ) a 
                            ON (c.path LIKE CONCAT('%.', a.id, '.%') 
                            OR c.path LIKE CONCAT('%.', a.id) 
                            OR c.path LIKE CONCAT(a.id, '.%') 
                            OR c.path = a.id) 
                    WHERE
                        is_standard = 1 
                        AND is_selectable = 1) 
                    AND is_standard = 1 )) criteria ) 
            AND cb_search_person.person_id NOT IN (SELECT
                person_id 
            FROM
                `person` p 
            WHERE
                race_concept_id IN (2100000001, 8515, 903096, 2000000008, 45882607, 1177221, 38003615, 8557) ) )", sep="")

# Formulate a Cloud Storage destination path for the data exported from BigQuery.
# NOTE: By default data exported multiple times on the same day will overwrite older copies.
#       But data exported on a different days will write to a new location so that historical
#       copies can be kept as the dataset definition is changed.
person_62031819_path <- file.path(
  Sys.getenv("WORKSPACE_BUCKET"),
  "bq_exports",
  Sys.getenv("OWNER_EMAIL"),
  strftime(lubridate::now(), "%Y%m%d"),  # Comment out this line if you want the export to always overwrite.
  "person_62031819",
  "person_62031819_*.csv")
message(str_glue('The data will be written to {person_62031819_path}. Use this path when reading ',
                 'the data into your notebooks in the future.'))

# Perform the query and export the dataset to Cloud Storage as CSV files.
# NOTE: You only need to run `bq_table_save` once. After that, you can
#       just read data from the CSVs in Cloud Storage.
bq_table_save(
  bq_dataset_query(Sys.getenv("WORKSPACE_CDR"), dataset_62031819_person_sql, billing = Sys.getenv("GOOGLE_PROJECT")),
  person_62031819_path,
  destination_format = "CSV")


# Read the data directly from Cloud Storage into memory.
# NOTE: Alternatively you can `gsutil -m cp {person_62031819_path}` to copy these files
#       to the Jupyter disk.
read_bq_export_from_workspace_bucket <- function(export_path) {
  col_types <- cols(gender = col_character(), race = col_character(), ethnicity = col_character(), sex_at_birth = col_character())
  bind_rows(
    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),
        function(csv) {
          message(str_glue('Loading {csv}.'))
          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)
          if (is.null(col_types)) {
            col_types <- spec(chunk)
          }
          chunk
        }))
}
dataset_62031819_person_df <- read_bq_export_from_workspace_bucket(person_62031819_path)

dim(dataset_62031819_person_df)

head(dataset_62031819_person_df, 5)
library(tidyverse)
library(bigrquery)

# This query represents dataset "followup_cohort_data" for domain "condition" and was generated for All of Us Controlled Tier Dataset v7
dataset_62031819_condition_sql <- paste("
    SELECT
        c_occurrence.person_id,
        c_standard_concept.concept_name as standard_concept_name,
        c_standard_concept.concept_code as standard_concept_code,
        c_occurrence.condition_start_datetime,
        c_occurrence.condition_end_datetime,
        c_occurrence.visit_occurrence_id,
        c_occurrence.condition_source_value,
        c_source_concept.concept_code as source_concept_code 
    FROM
        ( SELECT
            * 
        FROM
            `condition_occurrence` c_occurrence 
        WHERE
            (
                condition_concept_id IN (SELECT
                    DISTINCT c.concept_id 
                FROM
                    `cb_criteria` c 
                JOIN
                    (SELECT
                        CAST(cr.id as string) AS id       
                    FROM
                        `cb_criteria` cr       
                    WHERE
                        concept_id IN (132736, 132797, 133729, 133810, 133956, 134057, 134442, 137275, 138384, 138713, 140362, 140673, 140842, 141050, 141934, 192359, 192673, 192686, 193519, 193688, 193782, 195314, 195363, 195556, 195771, 195834, 195861, 196158, 196236, 197320, 197593, 197921, 198185, 199064, 200618, 200687, 201254, 201820, 201826, 201965, 24609, 252365, 255891, 257628, 312358, 313223, 313800, 313928, 315558, 317002, 318712, 319835, 320128, 321052, 321596, 321887, 36717004, 36717279, 37119138, 372892, 372903, 37311303, 37312531, 376112, 4029602, 4030520, 40480859, 40483287, 40493038, 4090272, 4096347, 4111261, 4132314, 4140207, 4174977, 4177206, 4234480, 42538169, 42539502, 4256228, 4263367, 42709887, 4280354, 4295287, 4298809, 43021246, 43021252, 4306451, 432595, 432863, 4335872, 433811, 434004, 434311, 434314, 434610, 434622, 434908, 435308, 435510, 435511, 435517, 435788, 436222, 436659, 437247, 437992, 438297, 438624, 438721, 439770, 439777, 439928, 440302, 440674,
 440704, 440977, 440979, 441553, 443439, 443447, 443597, 443601, 443612, 443614, 443729, 443731, 443733, 443734, 44784621, 45768812, 45770886, 45773180, 46270384, 46271022, 75004, 75650, 764123)       
                        AND full_text LIKE '%_rank1]%'      ) a 
                        ON (c.path LIKE CONCAT('%.', a.id, '.%') 
                        OR c.path LIKE CONCAT('%.', a.id) 
                        OR c.path LIKE CONCAT(a.id, '.%') 
                        OR c.path = a.id) 
                WHERE
                    is_standard = 1 
                    AND is_selectable = 1) 
                OR  condition_source_concept_id IN (SELECT
                    DISTINCT c.concept_id 
                FROM
                    `cb_criteria` c 
                JOIN
                    (SELECT
                        CAST(cr.id as string) AS id       
                    FROM
                        `cb_criteria` cr       
                    WHERE
                        concept_id IN (1326482, 1326483, 1326492, 1326493, 1326494, 1326495, 1326496, 1326511, 1326512, 1326513, 1326514, 1326516, 1326517, 1326522, 1326523, 1326526, 1326527, 1326528, 1326529, 1326531, 1326532, 1326533, 1326534, 1326620, 1326621, 1326622, 1326623, 1326624, 1326625, 1326626, 1326627, 1326628, 1326632, 1326635, 1326637, 1326640, 1326643, 1326644, 1326646, 1326647, 1326648, 1326649, 1326652, 1326653, 1326654, 1326655, 1326657, 1326658, 1326660, 1326661, 1326662, 1326663, 1326664, 1326665, 1326666, 1326667, 1326668, 1326669, 1326670, 1326671, 1326672, 1326673, 1326674, 1326675, 1326676, 1326679, 1326680, 1326681, 1326682, 1326683, 1326684, 1326685, 1326686, 1326688, 1326689, 1326690, 1326691, 1326692, 1326693, 1326694, 1326695, 1326697, 1326700, 1326701, 1326702, 1326703, 1567286, 1567838, 1567852, 1567858, 1567859, 1567861, 1567862, 1567893, 1567922, 1567940, 1567943, 1567944, 1567949, 1567956, 1567958, 1567959, 1567960, 1567964, 1567965, 1567966, 1567969,
 1567971, 1567975, 1567988, 1567990, 1567991, 1568066, 1568073, 1568076, 1568079, 1568420, 1568421, 1568422, 1568430, 1569271, 1569324, 1569325, 1569336, 1569419, 1569420, 1569515, 1569637, 1569638, 1569811, 1569849, 1569878, 1569966, 1569967, 1569997, 1570377, 1570393, 1570611, 1570612, 1570619, 1571469, 1571473, 1571479, 1571485, 1571486, 1571533, 1572108, 1572216, 1572227, 1572241, 1572267, 1572283, 1572285, 1575304, 1575308, 1595536, 1595600, 1595601, 1595691, 1595692, 1595693, 1595694, 1595695, 19400, 19409, 19431, 19454, 19456, 19470, 19472, 19475, 19481, 19483, 19485, 19486, 19487, 19488, 19489, 19694, 19699, 19716, 19726, 19736, 35205420, 35205549, 35205550, 35205551, 35205552, 35205553, 35206692, 35206693, 35206694, 35206695, 35206702, 35206703, 35206704, 35206705, 35206706, 35206707, 35206708, 35206709, 35206710, 35206731, 35206732, 35206733, 35206734, 35206735, 35206736, 35206737, 35206738, 35206739, 35206750, 35206751, 35206752, 35206753, 35206754, 35206755, 35206757,
 35206761, 35206762, 35206763, 35206764, 35206765, 35206766, 35206838, 35206839, 35206840, 35206842, 35206852, 35206853, 35206854, 35206855, 35206856, 35206857, 35206858, 35206859, 35206878, 35206879, 35206881, 35206882, 35206884, 35206885, 35206887, 35206888, 35206889, 35206891, 35206892, 35206893, 35206894, 35206895, 35206896, 35206897, 35206898, 35206899, 35206900, 35206901, 35206902, 35206903, 35207072, 35207073, 35207074, 35207075, 35207076, 35207077, 35207078, 35207079, 35207081, 35207085, 35207086, 35207087, 35207088, 35207089, 35207090, 35207091, 35207092, 35207094, 35207097, 35207099, 35207101, 35207102, 35207107, 35207351, 35207560, 35207664, 35207665, 35207666, 35207667, 35207668, 35207671, 35207672, 35207673, 35207793, 35207794, 35207795, 35207796, 35207797, 35207798, 35207799, 35207800, 35207801, 35207836, 35207841, 35207842, 35207843, 35207859, 35207860, 35207868, 35207869, 35207870, 35207871, 35207872, 35207873, 35207874, 35207876, 35207904, 35207915, 35207916, 35207917,
 35207918, 35208099, 35208100, 35208101, 35208125, 35208126, 35208127, 35208128, 35208130, 35208131, 35208132, 35208133, 35208275, 35208276, 35208277, 35208278, 35208279, 35208280, 35208281, 35208282, 35208284, 35208285, 35208656, 35208657, 35208658, 35208659, 35208660, 35208661, 35208662, 35208663, 35208664, 35208665, 35208715, 35208716, 35208717, 35208718, 35208719, 35208720, 35208721, 35208722, 35208750, 35208756, 35208817, 35208825, 35208826, 35208827, 35208828, 35208829, 35208830, 35208831, 35209165, 35209166, 35209167, 35209170, 35209172, 35209173, 35209174, 35209196, 35209197, 35209198, 35209199, 35209200, 35209201, 35209202, 35209203, 35209204, 35209205, 35209206, 35209207, 35209208, 35209210, 35209211, 35209212, 35209213, 35209214, 35209215, 35209216, 35209217, 35209218, 35209219, 35209220, 35209221, 35209222, 35209223, 35209224, 35209235, 35209236, 35209237, 35209243, 35209244, 35209245, 35209246, 35209264, 35209265, 35209266, 35209267, 35209269, 35209270, 35209271, 35209272,
 35209273, 35209274, 35209275, 35209276, 35209277, 35209278, 35209279, 35209280, 35209291, 35209297, 35209298, 35209299, 35209340, 35209341, 35209342, 35209343, 35209344, 35211012, 35211013, 35211014, 35211015, 35211016, 35211017, 35211018, 35211019, 35211020, 35211021, 35211022, 35211023, 35211024, 35211134, 35211314, 35211315, 35211316, 35211317, 35211318, 35211319, 35211340, 35211392, 35211393, 35211394, 35211395, 35211404, 35211523, 35225373, 35225404, 37200021, 37200022, 37200023, 37200024, 37200025, 37200026, 37200027, 37200028, 37200029, 37200030, 37200031, 37200032, 37200033, 37200034, 37200035, 37200036, 37200037, 37200038, 37200039, 37200040, 37200041, 37200042, 37200043, 37200044, 37200045, 37200046, 37200047, 37200049, 37200050, 37200051, 37200052, 37200053, 37200054, 37200056, 37200057, 37200058, 37200059, 37200063, 37200067, 37200071, 37200072, 37200073, 37200074, 37200075, 37200076, 37200077, 37200078, 37200081, 37200082, 37200083, 37200141, 37200142, 37200143, 37200144,
 37200145, 37200146, 37200147, 37200148, 37200149, 37200150, 37200151, 37200152, 37200153, 37200154, 37200155, 37200156, 37200157, 37200158, 37200159, 37200160, 37200161, 37200162, 37200163, 37200164, 37200165, 37200166, 37200167, 37200168, 37200170, 37200171, 37200172, 37200175, 37200176, 37200177, 37200180, 37200181, 37200182, 37200185, 37200186, 37200187, 37200188, 37200189, 37200190, 37200191, 37200192, 37200194, 37200196, 37200198, 37200199, 37200200, 37200201, 37200202, 37200203, 37200204, 37200205, 37200206, 37200207, 37200208, 37200209, 37200210, 37200211, 37200212, 37200213, 37200214, 37200215, 37200216, 37200217, 37200218, 37200219, 37200220, 37200221, 37200222, 37200223, 37200224, 37200225, 37200227, 37200228, 37200229, 37200230, 37200232, 37200233, 37200234, 37200235, 37200237, 37200238, 37200239, 37200240, 37200242, 37200243, 37200244, 37200245, 37200246, 37200247, 37200248, 37200249, 37200251, 37200252, 37200253, 37200254, 37200255, 37200257, 37200258, 37200260, 37200261,
 37200262, 37200263, 37200265, 37200266, 37200269, 37200270, 37200273, 37200274, 37200275, 37200278, 37200279, 37200280, 37200281, 37200284, 37200291, 37200301, 37200302, 37200303, 37200304, 37200305, 37200306, 37200310, 37200311, 37200492, 37200493, 37200494, 37200495, 37200564, 37200565, 37200566, 37200567, 37200618, 37200619, 37200620, 37200622, 37200623, 37200624, 37200626, 37200627, 37200628, 37200630, 37200632, 37200634, 37200636, 37200640, 37200642, 37200645, 37200686, 37200693, 37200694, 37200695, 37200697, 37200698, 37200699, 37201087, 37201088, 37201937, 37201938, 37201939, 37201941, 37201942, 37202093, 37202097, 37202101, 37202102, 37202105, 37202106, 37202109, 37202113, 37202114, 37202117, 37202118, 37202119, 37202122, 37202123, 37202126, 37202130, 37202131, 37202132, 37202134, 37202135, 37202136, 37202138, 37202139, 37202140, 37202142, 37202146, 37202147, 37202148, 37202154, 37202155, 37202156, 37202162, 37202164, 37202166, 37202170, 37202171, 37202174, 37202175, 37202178,
 37202179, 37202180, 37202183, 37202186, 37202187, 37202188, 37202190, 37202194, 37202195, 37202196, 37202198, 37202202, 37202203, 37202204, 44819357, 44819497, 44819498, 44819499, 44819500, 44819501, 44819502, 44819503, 44819504, 44819520, 44819521, 44819522, 44819523, 44819526, 44819527, 44819530, 44819580, 44819605, 44819606, 44819711, 44819718, 44819719, 44819724, 44819729, 44819738, 44819751, 44819752, 44819753, 44819754, 44819755, 44819756, 44819757, 44819758, 44819801, 44819814, 44819823, 44819824, 44819940, 44819941, 44819989, 44819990, 44820017, 44820018, 44820027, 44820028, 44820029, 44820056, 44820057, 44820255, 44820680, 44820681, 44820682, 44820683, 44820684, 44820685, 44820692, 44820693, 44820694, 44820695, 44820696, 44820700, 44820701, 44820707, 44820780, 44820781, 44820879, 44820899, 44820900, 44820902, 44820903, 44820947, 44820951, 44820952, 44820953, 44820954, 44820965, 44820969, 44820970, 44820971, 44820972, 44820973, 44820974, 44820985, 44821099, 44821101, 44821168,
 44821169, 44821170, 44821171, 44821172, 44821174, 44821175, 44821176, 44821177, 44821178, 44821203, 44821204, 44821205, 44821206, 44821212, 44821238, 44821239, 44821247, 44821564, 44821667, 44821784, 44821785, 44821787, 44821794, 44821799, 44821802, 44821803, 44821807, 44821841, 44821863, 44821864, 44821865, 44821866, 44821870, 44821873, 44821949, 44821958, 44821962, 44821971, 44821972, 44821973, 44821998, 44821999, 44822000, 44822001, 44822002, 44822003, 44822036, 44822037, 44822038, 44822039, 44822046, 44822175, 44822176, 44822238, 44822239, 44822258, 44822265, 44822266, 44822292, 44822293, 44822297, 44822523, 44822524, 44822525, 44822705, 44822813, 44822932, 44822933, 44822934, 44822935, 44822936, 44822937, 44822947, 44822951, 44822954, 44822955, 44822958, 44822959, 44822960, 44823017, 44823034, 44823035, 44823036, 44823040, 44823108, 44823109, 44823120, 44823124, 44823125, 44823135, 44823136, 44823150, 44823151, 44823152, 44823184, 44823191, 44823192, 44823199, 44823329, 44823330,
 44823331, 44823332, 44823333, 44823384, 44823405, 44823412, 44823446, 44823651, 44823652, 44823653, 44823927, 44823969, 44824071, 44824072, 44824073, 44824074, 44824080, 44824088, 44824089, 44824090, 44824091, 44824094, 44824095, 44824098, 44824099, 44824154, 44824172, 44824234, 44824257, 44824266, 44824275, 44824295, 44824296, 44824297, 44824324, 44824332, 44824333, 44824339, 44824340, 44824341, 44824342, 44824344, 44824345, 44824352, 44824486, 44824487, 44824543, 44824544, 44824574, 44824575, 44824582, 44824583, 44824619, 44824620, 44824621, 44824622, 44824630, 44824845, 44824846, 44824852, 44825015, 44825094, 44825095, 44825096, 44825143, 44825144, 44825259, 44825263, 44825264, 44825265, 44825273, 44825274, 44825275, 44825280, 44825281, 44825282, 44825286, 44825349, 44825425, 44825426, 44825427, 44825443, 44825446, 44825447, 44825448, 44825451, 44825456, 44825457, 44825485, 44825486, 44825523, 44825524, 44825539, 44825540, 44825544, 44825551, 44825553, 44825554, 44825658, 44825659,
 44825660, 44825661, 44825736, 44825737, 44825764, 44825765, 44825766, 44825767, 44825805, 44825821, 44826028, 44826029, 44826030, 44826037, 44826288, 44826329, 44826330, 44826456, 44826459, 44826460, 44826461, 44826462, 44826468, 44826472, 44826474, 44826475, 44826476, 44826477, 44826478, 44826479, 44826480, 44826546, 44826547, 44826568, 44826570, 44826572, 44826573, 44826642, 44826643, 44826646, 44826647, 44826651, 44826654, 44826655, 44826657, 44826658, 44826659, 44826688, 44826689, 44826690, 44826725, 44826730, 44826731, 44826733, 44826744, 44826854, 44826916, 44826917, 44826939, 44826940, 44827183, 44827184, 44827368, 44827369, 44827370, 44827371, 44827487, 44827613, 44827615, 44827616, 44827617, 44827631, 44827632, 44827634, 44827636, 44827692, 44827693, 44827711, 44827712, 44827717, 44827797, 44827802, 44827804, 44827805, 44827806, 44827807, 44827808, 44827814, 44827815, 44827835, 44827836, 44827837, 44827874, 44827887, 44827888, 44827889, 44827890, 44827894, 44827899, 44828017,
 44828019, 44828080, 44828081, 44828082, 44828106, 44828107, 44828108, 44828114, 44828115, 44828116, 44828117, 44828118, 44828147, 44828148, 44828150, 44828164, 44828357, 44828358, 44828554, 44828669, 44828670, 44828671, 44828672, 44828786, 44828790, 44828792, 44828793, 44828794, 44828795, 44828796, 44828803, 44828805, 44828806, 44828807, 44828808, 44828810, 44828811, 44828812, 44828814, 44828815, 44828816, 44828820, 44828891, 44828892, 44828893, 44828894, 44828971, 44828984, 44828985, 44828991, 44828992, 44828994, 44828996, 44829021, 44829022, 44829024, 44829025, 44829026, 44829027, 44829028, 44829048, 44829051, 44829052, 44829053, 44829054, 44829058, 44829060, 44829062, 44829063, 44829064, 44829073, 44829187, 44829188, 44829235, 44829294, 44829295, 44829308, 44829479, 44829480, 44829481, 44829482, 44829639, 44829733, 44829874, 44829876, 44829877, 44829878, 44829879, 44829880, 44829881, 44829882, 44829884, 44829895, 44829897, 44829898, 44829899, 44829900, 44829902, 44829904, 44829905,
 44829906, 44829907, 44829990, 44829991, 44829992, 44829994, 44829995, 44830077, 44830087, 44830091, 44830094, 44830123, 44830159, 44830161, 44830162, 44830166, 44830171, 44830172, 44830173, 44830176, 44830307, 44830308, 44830310, 44830369, 44830370, 44830402, 44830410, 44830411, 44830412, 44830442, 44830443, 44830444, 44830633, 44830634, 44830794, 44830803, 44830804, 44830883, 44830917, 44831045, 44831046, 44831047, 44831048, 44831049, 44831055, 44831057, 44831058, 44831060, 44831061, 44831062, 44831063, 44831068, 44831069, 44831127, 44831143, 44831144, 44831145, 44831148, 44831231, 44831251, 44831256, 44831257, 44831258, 44831262, 44831263, 44831270, 44831318, 44831323, 44831325, 44831326, 44831327, 44831328, 44831331, 44831336, 44831471, 44831472, 44831487, 44831555, 44831561, 44831583, 44831585, 44831586, 44832069, 44832187, 44832188, 44832190, 44832191, 44832192, 44832193, 44832194, 44832203, 44832206, 44832207, 44832208, 44832214, 44832215, 44832218, 44832269, 44832299, 44832300,
 44832306, 44832368, 44832383, 44832388, 44832397, 44832399, 44832400, 44832409, 44832434, 44832435, 44832474, 44832475, 44832487, 44832496, 44832611, 44832612, 44832613, 44832669, 44832670, 44832671, 44832672, 44832687, 44832688, 44832689, 44832709, 44832710, 44832711, 44832721, 44832931, 44833111, 44833364, 44833365, 44833366, 44833367, 44833368, 44833379, 44833383, 44833384, 44833385, 44833386, 44833388, 44833389, 44833390, 44833391, 44833392, 44833393, 44833394, 44833437, 44833464, 44833465, 44833555, 44833556, 44833558, 44833575, 44833578, 44833579, 44833583, 44833593, 44833594, 44833595, 44833649, 44833650, 44833651, 44833652, 44833657, 44833658, 44833659, 44833660, 44833661, 44833665, 44833676, 44833800, 44833801, 44833880, 44833892, 44833893, 44833894, 44834387, 44834388, 44834548, 44834549, 44834559, 44834560, 44834561, 44834566, 44834568, 44834570, 44834571, 44834573, 44834574, 44834579, 44834580, 44834628, 44834629, 44834642, 44834643, 44834644, 44834645, 44834646, 44834647,
 44834715, 44834733, 44834734, 44834739, 44834749, 44834750, 44834760, 44834761, 44834762, 44834763, 44834776, 44834778, 44834779, 44834813, 44834816, 44834817, 44834822, 44834830, 44834832, 44834833, 44834834, 44834835, 44834959, 44834960, 44834961, 44834962, 44835026, 44835027, 44835056, 44835062, 44835064, 44835065, 44835083, 44835294, 44835295, 44835296, 44835474, 44835483, 44835568, 44835607, 44835748, 44835749, 44835750, 44835751, 44835752, 44835753, 44835754, 44835761, 44835763, 44835764, 44835765, 44835767, 44835769, 44835770, 44835831, 44835847, 44835848, 44835858, 44835859, 44835921, 44835922, 44835923, 44835945, 44835953, 44835957, 44835958, 44835959, 44835967, 44835994, 44835995, 44836021, 44836026, 44836027, 44836028, 44836034, 44836035, 44836036, 44836037, 44836047, 44836158, 44836159, 44836228, 44836230, 44836250, 44836251, 44836259, 44836260, 44836294, 44836295, 44836300, 44836487, 44836913, 44836914, 44836915, 44836916, 44836917, 44836918, 44836931, 44836933, 44836934,
 44836935, 44836936, 44836937, 44836938, 44836939, 44836940, 44836944, 44836945, 44836946, 44836947, 44836953, 44837006, 44837023, 44837031, 44837108, 44837109, 44837112, 44837116, 44837120, 44837126, 44837145, 44837146, 44837174, 44837186, 44837187, 44837188, 44837189, 44837190, 44837191, 44837192, 44837193, 44837316, 44837317, 44837377, 44837378, 44837401, 44837402, 44837403, 44837430, 44837445, 44837833, 45532833, 45533009, 45533010, 45533011, 45533017, 45533018, 45533019, 45533020, 45533021, 45533022, 45533023, 45533024, 45533047, 45533048, 45533332, 45533333, 45533336, 45533468, 45533483, 45533484, 45533488, 45533490, 45533491, 45533492, 45533503, 45533563, 45533663, 45533664, 45533665, 45533666, 45533668, 45533669, 45533689, 45533693, 45533696, 45533697, 45533698, 45533699, 45533700, 45533712, 45533713, 45533780, 45533781, 45533782, 45533783, 45534125, 45534439, 45534453, 45534454, 45534546, 45534547, 45537025, 45537027, 45537030, 45537031, 45537033, 45537034, 45537035, 45537036,
 45537037, 45537038, 45537039, 45537040, 45537041, 45537042, 45537070, 45537072, 45537073, 45537074, 45537076, 45537077, 45537078, 45537080, 45537081, 45537082, 45537083, 45537087, 45537090, 45537692, 45537694, 45537695, 45537722, 45537723, 45537927, 45537943, 45537944, 45537953, 45537954, 45537958, 45537960, 45537961, 45537962, 45537963, 45538143, 45538147, 45538291, 45538292, 45538295, 45538299, 45538368, 45538387, 45538403, 45538422, 45538425, 45538426, 45538427, 45538431, 45538433, 45538435, 45538437, 45538438, 45538443, 45538444, 45538448, 45538489, 45538537, 45538586, 45538605, 45538606, 45538607, 45538608, 45538609, 45538610, 45538623, 45538624, 45538626, 45538628, 45538629, 45538631, 45538633, 45538647, 45538648, 45538649, 45538701, 45538702, 45538703, 45538704, 45538706, 45538707, 45538708, 45538709, 45539344, 45541805, 45541820, 45541821, 45541823, 45541825, 45541826, 45541827, 45541833, 45541834, 45541835, 45541836, 45541837, 45541838, 45541839, 45541840, 45541876, 45541877,
 45541879, 45541880, 45541881, 45541884, 45542480, 45542481, 45542482, 45542483, 45542484, 45542728, 45542730, 45542731, 45542736, 45542737, 45542738, 45542741, 45542773, 45542774, 45542775, 45542778, 45542780, 45542781, 45542911, 45542912, 45543055, 45543057, 45543060, 45543061, 45543164, 45543209, 45543210, 45543218, 45543283, 45543401, 45543403, 45543404, 45543405, 45543406, 45543407, 45543409, 45543410, 45543434, 45543435, 45543436, 45543449, 45543515, 45543517, 45543518, 45543519, 45543520, 45543521, 45543522, 45543877, 45544154, 45546703, 45546705, 45546708, 45546709, 45546710, 45546711, 45546749, 45546750, 45546751, 45546752, 45546753, 45546754, 45546757, 45546758, 45546759, 45546762, 45546763, 45547364, 45547365, 45547366, 45547367, 45547369, 45547617, 45547618, 45547621, 45547622, 45547623, 45547624, 45547625, 45547626, 45547627, 45547632, 45547633, 45547635, 45547659, 45547661, 45547662, 45547663, 45547667, 45547763, 45547877, 45547903, 45547904, 45547906, 45547908, 45548056,
 45548057, 45548058, 45548059, 45548060, 45548061, 45548069, 45548070, 45548076, 45548097, 45548131, 45548223, 45548224, 45548225, 45548226, 45548227, 45548228, 45548252, 45548253, 45548254, 45548257, 45548259, 45548260, 45548261, 45548262, 45548263, 45548265, 45548266, 45548287, 45548288, 45548289, 45548346, 45548351, 45548353, 45548354, 45548653, 45548655, 45548968, 45548977, 45549060, 45551473, 45551474, 45551487, 45551488, 45551489, 45551490, 45551491, 45551493, 45551495, 45551496, 45551497, 45551498, 45551499, 45551500, 45551502, 45551503, 45551504, 45551534, 45551535, 45551537, 45551540, 45551541, 45551542, 45551543, 45551544, 45551547, 45552133, 45552134, 45552135, 45552137, 45552138, 45552179, 45552180, 45552359, 45552361, 45552372, 45552373, 45552374, 45552375, 45552379, 45552381, 45552382, 45552383, 45552385, 45552386, 45552388, 45552419, 45552420, 45552551, 45552702, 45552703, 45552704, 45552705, 45552706, 45552707, 45552709, 45552823, 45552824, 45552825, 45552826, 45552827,
 45552830, 45552831, 45552832, 45552833, 45552835, 45552839, 45552871, 45552874, 45553011, 45553012, 45553013, 45553014, 45553015, 45553037, 45553040, 45553043, 45553044, 45553045, 45553046, 45553047, 45553048, 45553049, 45553066, 45553067, 45553114, 45553116, 45553117, 45553118, 45553119, 45553120, 45553171, 45553439, 45553686, 45553727, 45553736, 45553737, 45556244, 45556246, 45556252, 45556254, 45556257, 45556260, 45556292, 45556294, 45556295, 45556296, 45556297, 45556298, 45556872, 45556873, 45556876, 45556877, 45557089, 45557090, 45557106, 45557107, 45557110, 45557112, 45557113, 45557116, 45557142, 45557434, 45557437, 45557441, 45557568, 45557569, 45557570, 45557571, 45557572, 45557575, 45557576, 45557578, 45557579, 45557581, 45557584, 45557585, 45557588, 45557592, 45557635, 45557728, 45557730, 45557731, 45557732, 45557733, 45557734, 45557735, 45557736, 45557737, 45557738, 45557754, 45557759, 45557760, 45557761, 45557781, 45557821, 45557822, 45557823, 45557824, 45557825, 45557826,
 45557827, 45557828, 45558143, 45558145, 45558146, 45558484, 45558577, 45561018, 45561035, 45561036, 45561037, 45561038, 45561040, 45561041, 45561043, 45561044, 45561045, 45561046, 45561047, 45561048, 45561049, 45561051, 45561052, 45561053, 45561085, 45561087, 45561089, 45561090, 45561091, 45561092, 45561093, 45561098, 45561928, 45561929, 45561938, 45561940, 45561941, 45561942, 45561943, 45561947, 45561948, 45561949, 45561953, 45561954, 45561955, 45561956, 45561957, 45561977, 45561989, 45561990, 45561991, 45561993, 45562109, 45562249, 45562250, 45562251, 45562257, 45562371, 45562385, 45562386, 45562387, 45562388, 45562391, 45562393, 45562395, 45562396, 45562399, 45562498, 45562561, 45562562, 45562563, 45562564, 45562565, 45562590, 45562591, 45562592, 45562610, 45562665, 45562666, 45562667, 45562668, 45562669, 45562670, 45562671, 45562672, 45562709, 45563006, 45563255, 45563298, 45563324, 45563325, 45565829, 45565830, 45565831, 45565833, 45565834, 45565835, 45565836, 45565838, 45565843,
 45565844, 45565845, 45565886, 45565887, 45565888, 45565889, 45565890, 45565891, 45565893, 45565895, 45565898, 45565899, 45566472, 45566504, 45566723, 45566724, 45566729, 45566731, 45566733, 45566734, 45566735, 45566736, 45566737, 45566762, 45566906, 45566907, 45566910, 45567050, 45567070, 45567071, 45567073, 45567075, 45567079, 45567183, 45567192, 45567207, 45567208, 45567209, 45567210, 45567211, 45567212, 45567214, 45567216, 45567223, 45567228, 45567283, 45567284, 45567313, 45567377, 45567378, 45567379, 45567380, 45567381, 45567382, 45567406, 45567409, 45567413, 45567415, 45567416, 45567417, 45567418, 45567419, 45567440, 45567489, 45567490, 45567491, 45567492, 45567493, 45567494, 45567496, 45567497, 45567498, 45567545, 45568131, 45568222, 45570697, 45570716, 45570719, 45570720, 45570721, 45570722, 45570723, 45570724, 45570765, 45570767, 45570768, 45570769, 45570771, 45570772, 45570773, 45570775, 45571428, 45571638, 45571639, 45571649, 45571650, 45571651, 45571652, 45571654, 45571658,
 45571659, 45571661, 45571662, 45571692, 45571693, 45571694, 45571836, 45571990, 45572116, 45572117, 45572118, 45572119, 45572121, 45572124, 45572126, 45572134, 45572177, 45572296, 45572297, 45572298, 45572299, 45572300, 45572301, 45572302, 45572303, 45572304, 45572327, 45572329, 45572331, 45572332, 45572336, 45572337, 45572338, 45572339, 45572392, 45572394, 45572395, 45572448, 45572699, 45572717, 45573034, 45573035, 45573039, 45573041, 45573115, 45573116, 45575555, 45575556, 45575557, 45575559, 45575560, 45575562, 45575567, 45575568, 45575571, 45575575, 45575576, 45575579, 45575611, 45575612, 45575613, 45575615, 45575616, 45575617, 45575620, 45575621, 45575622, 45575623, 45575624, 45575625, 45576203, 45576204, 45576205, 45576206, 45576207, 45576208, 45576209, 45576410, 45576437, 45576438, 45576439, 45576440, 45576443, 45576446, 45576447, 45576476, 45576480, 45576481, 45576757, 45576758, 45576761, 45576762, 45576767, 45576768, 45576769, 45576770, 45576773, 45576890, 45576903, 45576904,
 45576905, 45576906, 45576907, 45576908, 45576909, 45577068, 45577069, 45577070, 45577072, 45577073, 45577074, 45577075, 45577092, 45577093, 45577094, 45577095, 45577098, 45577100, 45577102, 45577189, 45577190, 45577192, 45577193, 45577195, 45577196, 45577516, 45577738, 45577803, 45577889, 45580405, 45580407, 45580409, 45580410, 45580413, 45580414, 45580419, 45580420, 45580421, 45580422, 45580456, 45580457, 45580458, 45580460, 45580461, 45580463, 45580464, 45580465, 45580467, 45580469, 45581085, 45581086, 45581112, 45581342, 45581343, 45581344, 45581349, 45581350, 45581352, 45581353, 45581354, 45581355, 45581358, 45581359, 45581376, 45581388, 45581519, 45581678, 45581787, 45581798, 45581799, 45581800, 45581801, 45581802, 45581807, 45581808, 45581809, 45581810, 45581814, 45581868, 45581874, 45581973, 45581974, 45581976, 45581977, 45581978, 45581979, 45581980, 45581981, 45581982, 45581983, 45581984, 45582007, 45582010, 45582014, 45582015, 45582016, 45582021, 45582022, 45582023, 45582024,
 45582082, 45582083, 45582084, 45582085, 45582086, 45582088, 45582089, 45582126, 45582130, 45582399, 45582407, 45582408, 45582409, 45582709, 45582710, 45585223, 45585239, 45585240, 45585241, 45585243, 45585247, 45585248, 45585250, 45585251, 45585255, 45585257, 45585259, 45585260, 45585311, 45585312, 45585313, 45585314, 45585317, 45585318, 45585322, 45585323, 45585324, 45585871, 45586119, 45586132, 45586138, 45586139, 45586140, 45586142, 45586143, 45586144, 45586145, 45586476, 45586477, 45586479, 45586480, 45586481, 45586483, 45586614, 45586615, 45586617, 45586619, 45586623, 45586629, 45586630, 45586802, 45586804, 45586805, 45586806, 45586807, 45586808, 45586809, 45586833, 45586834, 45586836, 45586838, 45586839, 45586859, 45586913, 45586914, 45586916, 45586917, 45586918, 45587018, 45587246, 45587247, 45587467, 45587506, 45587526, 45587528, 45587600, 45590048, 45590049, 45590063, 45590065, 45590068, 45590075, 45590076, 45590077, 45590112, 45590113, 45590115, 45590116, 45590120, 45590122,
 45590124, 45590125, 45590127, 45590128, 45591019, 45591023, 45591026, 45591027, 45591029, 45591030, 45591031, 45591033, 45591034, 45591065, 45591066, 45591068, 45591201, 45591330, 45591363, 45591364, 45591366, 45591370, 45591373, 45591374, 45591494, 45591495, 45591496, 45591497, 45591498, 45591499, 45591500, 45591501, 45591503, 45591504, 45591509, 45591537, 45591660, 45591661, 45591662, 45591663, 45591664, 45591686, 45591688, 45591691, 45591698, 45591699, 45591700, 45591716, 45591769, 45591770, 45591772, 45591773, 45591819, 45591820, 45592142, 45592435, 45594885, 45594886, 45594897, 45594898, 45594899, 45594904, 45594905, 45594907, 45594908, 45594909, 45594913, 45594914, 45594915, 45594916, 45594917, 45594918, 45594920, 45594955, 45594956, 45594959, 45594960, 45594961, 45594962, 45594963, 45594964, 45595554, 45595555, 45595557, 45595558, 45595586, 45595772, 45595782, 45595783, 45595789, 45595790, 45595791, 45595792, 45595793, 45595794, 45595795, 45595797, 45595798, 45595799, 45595802,
 45595803, 45595804, 45595805, 45595826, 45595833, 45595834, 45595836, 45595839, 45596104, 45596106, 45596107, 45596108, 45596109, 45596111, 45596113, 45596188, 45596232, 45596233, 45596234, 45596235, 45596236, 45596239, 45596240, 45596244, 45596247, 45596289, 45596290, 45596334, 45596388, 45596397, 45596398, 45596399, 45596400, 45596428, 45596432, 45596435, 45596436, 45596437, 45596438, 45596439, 45596440, 45596452, 45596515, 45596516, 45596517, 45596518, 45596519, 45596520, 45596521, 45596523, 45596524, 45596525, 45596565, 45596873, 45596874, 45597196, 45599743, 45599756, 45599757, 45599758, 45599760, 45599765, 45599769, 45599770, 45599771, 45599772, 45599773, 45599811, 45599813, 45599814, 45599815, 45599816, 45599817, 45599818, 45599819, 45599823, 45599824, 45599827, 45599829, 45600368, 45600369, 45600370, 45600371, 45600609, 45600633, 45600634, 45600636, 45600637, 45600638, 45600639, 45600640, 45600641, 45600642, 45600644, 45600678, 45600680, 45600930, 45600932, 45600934, 45600938,
 45600941, 45601065, 45601066, 45601069, 45601070, 45601071, 45601072, 45601073, 45601080, 45601084, 45601180, 45601252, 45601253, 45601254, 45601255, 45601256, 45601257, 45601259, 45601260, 45601276, 45601279, 45601280, 45601281, 45601282, 45601283, 45601305, 45601306, 45601307, 45601380, 45601381, 45601382, 45601383, 45601384, 45601385, 45601386, 45601433, 45601434, 45601721, 45601722, 45601736, 45602035, 45604535, 45604541, 45604542, 45604544, 45604545, 45604574, 45604576, 45604579, 45604580, 45604581, 45604584, 45604585, 45604587, 45605168, 45605169, 45605170, 45605379, 45605392, 45605393, 45605394, 45605395, 45605397, 45605398, 45605401, 45605402, 45605403, 45605404, 45605405, 45605434, 45605696, 45605777, 45605828, 45605829, 45605830, 45605831, 45605832, 45605833, 45605834, 45605838, 45605840, 45605842, 45605844, 45605873, 45605906, 45605907, 45605942, 45606018, 45606019, 45606020, 45606021, 45606022, 45606023, 45606024, 45606025, 45606027, 45606047, 45606048, 45606050, 45606051,
 45606155, 45606156, 45606159, 45606160, 45606161, 45606162, 45606164, 45606165, 45606166, 45606167, 45606168, 45606213, 45606214, 45606486, 45606496, 45606750, 45606800, 45606823, 45606824, 45609323, 45609324, 45609329, 45609334, 45609335, 45609337, 45609338, 45609339, 45609342, 45609345, 45609381, 45609382, 45609383, 45609384, 45609386, 45609388, 45609389, 45609392, 45609393, 45609989, 45609990, 45609991, 713855, 725238, 725239, 725240, 725241, 725242, 725258, 725259, 725260, 725263, 725370, 725437, 725440, 725441, 725442, 766347, 766391, 766392, 766393, 920128, 920129)       
                        AND full_text LIKE '%_rank1]%'      ) a 
                        ON (c.path LIKE CONCAT('%.', a.id, '.%') 
                        OR c.path LIKE CONCAT('%.', a.id) 
                        OR c.path LIKE CONCAT(a.id, '.%') 
                        OR c.path = a.id) 
                WHERE
                    is_standard = 0 
                    AND is_selectable = 1)
            )  
            AND (
                c_occurrence.PERSON_ID IN (SELECT
                    distinct person_id  
                FROM
                    `cb_search_person` cb_search_person  
                WHERE
                    cb_search_person.person_id IN (SELECT
                        person_id 
                    FROM
                        `person` p 
                    WHERE
                        race_concept_id IN (8527, 8516, 2100000001, 8515, 903096, 2000000008, 45882607, 1177221, 38003615, 8557) ) 
                    AND cb_search_person.person_id NOT IN (SELECT
                        criteria.person_id 
                    FROM
                        (SELECT
                            DISTINCT person_id, entry_date, concept_id 
                        FROM
                            `cb_search_all_events` 
                        WHERE
                            (concept_id IN(SELECT
                                DISTINCT c.concept_id 
                            FROM
                                `cb_criteria` c 
                            JOIN
                                (SELECT
                                    CAST(cr.id as string) AS id       
                                FROM
                                    `cb_criteria` cr       
                                WHERE
                                    concept_id IN (440508)       
                                    AND full_text LIKE '%_rank1]%'      ) a 
                                    ON (c.path LIKE CONCAT('%.', a.id, '.%') 
                                    OR c.path LIKE CONCAT('%.', a.id) 
                                    OR c.path LIKE CONCAT(a.id, '.%') 
                                    OR c.path = a.id) 
                            WHERE
                                is_standard = 1 
                                AND is_selectable = 1) 
                            AND is_standard = 1 )) criteria ) 
                    AND cb_search_person.person_id NOT IN (SELECT
                        person_id 
                    FROM
                        `person` p 
                    WHERE
                        race_concept_id IN (2100000001, 8515, 903096, 2000000008, 45882607, 1177221, 38003615, 8557) ) )
            )) c_occurrence 
    LEFT JOIN
        `concept` c_standard_concept 
            ON c_occurrence.condition_concept_id = c_standard_concept.concept_id 
    LEFT JOIN
        `concept` c_source_concept 
            ON c_occurrence.condition_source_concept_id = c_source_concept.concept_id", sep="")

# Formulate a Cloud Storage destination path for the data exported from BigQuery.
# NOTE: By default data exported multiple times on the same day will overwrite older copies.
#       But data exported on a different days will write to a new location so that historical
#       copies can be kept as the dataset definition is changed.
condition_62031819_path <- file.path(
  Sys.getenv("WORKSPACE_BUCKET"),
  "bq_exports",
  Sys.getenv("OWNER_EMAIL"),
  strftime(lubridate::now(), "%Y%m%d"),  # Comment out this line if you want the export to always overwrite.
  "condition_62031819",
  "condition_62031819_*.csv")
message(str_glue('The data will be written to {condition_62031819_path}. Use this path when reading ',
                 'the data into your notebooks in the future.'))

# Perform the query and export the dataset to Cloud Storage as CSV files.
# NOTE: You only need to run `bq_table_save` once. After that, you can
#       just read data from the CSVs in Cloud Storage.
bq_table_save(
  bq_dataset_query(Sys.getenv("WORKSPACE_CDR"), dataset_62031819_condition_sql, billing = Sys.getenv("GOOGLE_PROJECT")),
  condition_62031819_path,
  destination_format = "CSV")


# Read the data directly from Cloud Storage into memory.
# NOTE: Alternatively you can `gsutil -m cp {condition_62031819_path}` to copy these files
#       to the Jupyter disk.
read_bq_export_from_workspace_bucket <- function(export_path) {
  col_types <- cols(standard_concept_name = col_character(), standard_concept_code = col_character(), condition_source_value = col_character(), source_concept_code = col_character())
  bind_rows(
    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),
        function(csv) {
          message(str_glue('Loading {csv}.'))
          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)
          if (is.null(col_types)) {
            col_types <- spec(chunk)
          }
          chunk
        }))
}
dataset_62031819_condition_df <- read_bq_export_from_workspace_bucket(condition_62031819_path)

dim(dataset_62031819_condition_df)

head(dataset_62031819_condition_df, 5)
library(tidyverse)
library(bigrquery)

# This query represents dataset "followup_cohort_data" for domain "device" and was generated for All of Us Controlled Tier Dataset v7
dataset_62031819_device_sql <- paste("
    SELECT
        device.person_id,
        d_standard_concept.concept_name as standard_concept_name,
        d_standard_concept.concept_code as standard_concept_code,
        device.device_exposure_start_datetime,
        device.device_exposure_end_datetime,
        device.visit_occurrence_id,
        device.device_source_value,
        d_source_concept.concept_code as source_concept_code 
    FROM
        ( SELECT
            * 
        FROM
            `device_exposure` device 
        WHERE
            (
                device_concept_id IN (2616955)
            )  
            AND (
                PERSON_ID IN (SELECT
                    distinct person_id  
                FROM
                    `cb_search_person` cb_search_person  
                WHERE
                    cb_search_person.person_id IN (SELECT
                        person_id 
                    FROM
                        `person` p 
                    WHERE
                        race_concept_id IN (8527, 8516, 2100000001, 8515, 903096, 2000000008, 45882607, 1177221, 38003615, 8557) ) 
                    AND cb_search_person.person_id NOT IN (SELECT
                        criteria.person_id 
                    FROM
                        (SELECT
                            DISTINCT person_id, entry_date, concept_id 
                        FROM
                            `cb_search_all_events` 
                        WHERE
                            (concept_id IN(SELECT
                                DISTINCT c.concept_id 
                            FROM
                                `cb_criteria` c 
                            JOIN
                                (SELECT
                                    CAST(cr.id as string) AS id       
                                FROM
                                    `cb_criteria` cr       
                                WHERE
                                    concept_id IN (440508)       
                                    AND full_text LIKE '%_rank1]%'      ) a 
                                    ON (c.path LIKE CONCAT('%.', a.id, '.%') 
                                    OR c.path LIKE CONCAT('%.', a.id) 
                                    OR c.path LIKE CONCAT(a.id, '.%') 
                                    OR c.path = a.id) 
                            WHERE
                                is_standard = 1 
                                AND is_selectable = 1) 
                            AND is_standard = 1 )) criteria ) 
                    AND cb_search_person.person_id NOT IN (SELECT
                        person_id 
                    FROM
                        `person` p 
                    WHERE
                        race_concept_id IN (2100000001, 8515, 903096, 2000000008, 45882607, 1177221, 38003615, 8557) ) )
            )) device 
    LEFT JOIN
        `concept` d_standard_concept 
            ON device.device_concept_id = d_standard_concept.concept_id 
    LEFT JOIN
        `concept` d_source_concept 
            ON device.device_source_concept_id = d_source_concept.concept_id", sep="")

# Formulate a Cloud Storage destination path for the data exported from BigQuery.
# NOTE: By default data exported multiple times on the same day will overwrite older copies.
#       But data exported on a different days will write to a new location so that historical
#       copies can be kept as the dataset definition is changed.
device_62031819_path <- file.path(
  Sys.getenv("WORKSPACE_BUCKET"),
  "bq_exports",
  Sys.getenv("OWNER_EMAIL"),
  strftime(lubridate::now(), "%Y%m%d"),  # Comment out this line if you want the export to always overwrite.
  "device_62031819",
  "device_62031819_*.csv")
message(str_glue('The data will be written to {device_62031819_path}. Use this path when reading ',
                 'the data into your notebooks in the future.'))

# Perform the query and export the dataset to Cloud Storage as CSV files.
# NOTE: You only need to run `bq_table_save` once. After that, you can
#       just read data from the CSVs in Cloud Storage.
bq_table_save(
  bq_dataset_query(Sys.getenv("WORKSPACE_CDR"), dataset_62031819_device_sql, billing = Sys.getenv("GOOGLE_PROJECT")),
  device_62031819_path,
  destination_format = "CSV")


# Read the data directly from Cloud Storage into memory.
# NOTE: Alternatively you can `gsutil -m cp {device_62031819_path}` to copy these files
#       to the Jupyter disk.
read_bq_export_from_workspace_bucket <- function(export_path) {
  col_types <- cols(standard_concept_name = col_character(), standard_concept_code = col_character(), device_source_value = col_character(), source_concept_code = col_character())
  bind_rows(
    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),
        function(csv) {
          message(str_glue('Loading {csv}.'))
          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)
          if (is.null(col_types)) {
            col_types <- spec(chunk)
          }
          chunk
        }))
}
dataset_62031819_device_df <- read_bq_export_from_workspace_bucket(device_62031819_path)

dim(dataset_62031819_device_df)

head(dataset_62031819_device_df, 5)
library(tidyverse)
library(bigrquery)

# This query represents dataset "followup_cohort_data" for domain "drug" and was generated for All of Us Controlled Tier Dataset v7
dataset_62031819_drug_sql <- paste("
    SELECT
        d_exposure.person_id,
        d_standard_concept.concept_name as standard_concept_name,
        d_standard_concept.concept_code as standard_concept_code,
        d_exposure.drug_exposure_start_datetime,
        d_exposure.drug_exposure_end_datetime,
        d_exposure.verbatim_end_date,
        d_exposure.visit_occurrence_id,
        d_visit.concept_name as visit_occurrence_concept_name,
        d_exposure.drug_source_value,
        d_source_concept.concept_name as source_concept_name,
        d_source_concept.concept_code as source_concept_code 
    FROM
        ( SELECT
            * 
        FROM
            `drug_exposure` d_exposure 
        WHERE
            (
                drug_concept_id IN (SELECT
                    DISTINCT ca.descendant_id 
                FROM
                    `cb_criteria_ancestor` ca 
                JOIN
                    (SELECT
                        DISTINCT c.concept_id       
                    FROM
                        `cb_criteria` c       
                    JOIN
                        (SELECT
                            CAST(cr.id as string) AS id             
                        FROM
                            `cb_criteria` cr             
                        WHERE
                            concept_id IN (1301125, 1304643, 1308368, 1309068, 1317967, 1318853, 1319156, 1332418, 1346823, 1347450, 1352890, 1367571, 1373928, 1381661, 1395773, 1399177, 1502905, 1512446, 1513876, 1516976, 1517740, 1531601, 1544838, 1548111, 1550023, 1567198, 1593331, 1596977, 1703069, 1707687, 1746244, 1758536, 1769389, 1776684, 1786617, 19010309, 19015768, 19019193, 19035631, 19078126, 35606465, 35606467, 40165636, 42899476, 46221581, 501343, 528323, 754270, 900017, 939506, 950316, 950435, 951469, 952004, 977968, 985247, 994058)             
                            AND full_text LIKE '%_rank1]%'       ) a 
                            ON (c.path LIKE CONCAT('%.', a.id, '.%') 
                            OR c.path LIKE CONCAT('%.', a.id) 
                            OR c.path LIKE CONCAT(a.id, '.%') 
                            OR c.path = a.id) 
                    WHERE
                        is_standard = 1 
                        AND is_selectable = 1) b 
                        ON (ca.ancestor_id = b.concept_id)))  
                    AND (d_exposure.PERSON_ID IN (SELECT
                        distinct person_id  
                FROM
                    `cb_search_person` cb_search_person  
                WHERE
                    cb_search_person.person_id IN (SELECT
                        person_id 
                    FROM
                        `person` p 
                    WHERE
                        race_concept_id IN (8527, 8516, 2100000001, 8515, 903096, 2000000008, 45882607, 1177221, 38003615, 8557) ) 
                    AND cb_search_person.person_id NOT IN (SELECT
                        criteria.person_id 
                    FROM
                        (SELECT
                            DISTINCT person_id, entry_date, concept_id 
                        FROM
                            `cb_search_all_events` 
                        WHERE
                            (concept_id IN(SELECT
                                DISTINCT c.concept_id 
                            FROM
                                `cb_criteria` c 
                            JOIN
                                (SELECT
                                    CAST(cr.id as string) AS id       
                                FROM
                                    `cb_criteria` cr       
                                WHERE
                                    concept_id IN (440508)       
                                    AND full_text LIKE '%_rank1]%'      ) a 
                                    ON (c.path LIKE CONCAT('%.', a.id, '.%') 
                                    OR c.path LIKE CONCAT('%.', a.id) 
                                    OR c.path LIKE CONCAT(a.id, '.%') 
                                    OR c.path = a.id) 
                            WHERE
                                is_standard = 1 
                                AND is_selectable = 1) 
                            AND is_standard = 1 )) criteria ) 
                    AND cb_search_person.person_id NOT IN (SELECT
                        person_id 
                    FROM
                        `person` p 
                    WHERE
                        race_concept_id IN (2100000001, 8515, 903096, 2000000008, 45882607, 1177221, 38003615, 8557) ) )
            )) d_exposure 
    LEFT JOIN
        `concept` d_standard_concept 
            ON d_exposure.drug_concept_id = d_standard_concept.concept_id 
    LEFT JOIN
        `visit_occurrence` v 
            ON d_exposure.visit_occurrence_id = v.visit_occurrence_id 
    LEFT JOIN
        `concept` d_visit 
            ON v.visit_concept_id = d_visit.concept_id 
    LEFT JOIN
        `concept` d_source_concept 
            ON d_exposure.drug_source_concept_id = d_source_concept.concept_id", sep="")

# Formulate a Cloud Storage destination path for the data exported from BigQuery.
# NOTE: By default data exported multiple times on the same day will overwrite older copies.
#       But data exported on a different days will write to a new location so that historical
#       copies can be kept as the dataset definition is changed.
drug_62031819_path <- file.path(
  Sys.getenv("WORKSPACE_BUCKET"),
  "bq_exports",
  Sys.getenv("OWNER_EMAIL"),
  strftime(lubridate::now(), "%Y%m%d"),  # Comment out this line if you want the export to always overwrite.
  "drug_62031819",
  "drug_62031819_*.csv")
message(str_glue('The data will be written to {drug_62031819_path}. Use this path when reading ',
                 'the data into your notebooks in the future.'))

# Perform the query and export the dataset to Cloud Storage as CSV files.
# NOTE: You only need to run `bq_table_save` once. After that, you can
#       just read data from the CSVs in Cloud Storage.
bq_table_save(
  bq_dataset_query(Sys.getenv("WORKSPACE_CDR"), dataset_62031819_drug_sql, billing = Sys.getenv("GOOGLE_PROJECT")),
  drug_62031819_path,
  destination_format = "CSV")


# Read the data directly from Cloud Storage into memory.
# NOTE: Alternatively you can `gsutil -m cp {drug_62031819_path}` to copy these files
#       to the Jupyter disk.
read_bq_export_from_workspace_bucket <- function(export_path) {
  col_types <- cols(standard_concept_name = col_character(), standard_concept_code = col_character(), visit_occurrence_concept_name = col_character(), drug_source_value = col_character(), source_concept_name = col_character(), source_concept_code = col_character())
  bind_rows(
    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),
        function(csv) {
          message(str_glue('Loading {csv}.'))
          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)
          if (is.null(col_types)) {
            col_types <- spec(chunk)
          }
          chunk
        }))
}
dataset_62031819_drug_df <- read_bq_export_from_workspace_bucket(drug_62031819_path)

dim(dataset_62031819_drug_df)

head(dataset_62031819_drug_df, 5)
library(tidyverse)
library(bigrquery)

# This query represents dataset "followup_cohort_data" for domain "measurement" and was generated for All of Us Controlled Tier Dataset v7
dataset_62031819_measurement_sql <- paste("
    SELECT
        measurement.person_id,
        m_standard_concept.concept_name as standard_concept_name,
        m_standard_concept.concept_code as standard_concept_code,
        measurement.measurement_datetime,
        m_type.concept_name as measurement_type_concept_name,
        measurement.value_as_number,
        m_value.concept_name as value_as_concept_name,
        measurement.visit_occurrence_id,
        measurement.measurement_source_value,
        m_source_concept.concept_name as source_concept_name,
        m_source_concept.concept_code as source_concept_code 
    FROM
        ( SELECT
            * 
        FROM
            `measurement` measurement 
        WHERE
            (
                measurement_concept_id IN (SELECT
                    DISTINCT c.concept_id 
                FROM
                    `cb_criteria` c 
                JOIN
                    (SELECT
                        CAST(cr.id as string) AS id       
                    FROM
                        `cb_criteria` cr       
                    WHERE
                        concept_id IN (1619025, 3000067, 3000185, 3000819, 3001122, 3001802, 3002302, 3002400, 3002812, 3004249, 3004410, 3004789, 3005715, 3006032, 3006453, 3007359, 3013801, 3016662, 3016723, 3017797, 3019510, 3019574, 3019839, 3019897, 3021044, 3021347, 3022192, 3022844, 3023421, 3023520, 3027945, 3030366, 3034485, 3037121, 3037556, 3042733, 37022114, 37040063, 40764999, 4235028, 46236952)       
                        AND full_text LIKE '%_rank1]%'      ) a 
                        ON (c.path LIKE CONCAT('%.', a.id, '.%') 
                        OR c.path LIKE CONCAT('%.', a.id) 
                        OR c.path LIKE CONCAT(a.id, '.%') 
                        OR c.path = a.id) 
                WHERE
                    is_standard = 1 
                    AND is_selectable = 1) 
                OR  measurement_source_concept_id IN (SELECT
                    DISTINCT c.concept_id 
                FROM
                    `cb_criteria` c 
                JOIN
                    (SELECT
                        CAST(cr.id as string) AS id       
                    FROM
                        `cb_criteria` cr       
                    WHERE
                        concept_id IN (44825059, 44826256, 44828604, 44828606, 44834348, 45568152, 903121, 903133)       
                        AND full_text LIKE '%_rank1]%'      ) a 
                        ON (c.path LIKE CONCAT('%.', a.id, '.%') 
                        OR c.path LIKE CONCAT('%.', a.id) 
                        OR c.path LIKE CONCAT(a.id, '.%') 
                        OR c.path = a.id) 
                WHERE
                    is_standard = 0 
                    AND is_selectable = 1)
            )  
            AND (
                measurement.PERSON_ID IN (SELECT
                    distinct person_id  
                FROM
                    `cb_search_person` cb_search_person  
                WHERE
                    cb_search_person.person_id IN (SELECT
                        person_id 
                    FROM
                        `person` p 
                    WHERE
                        race_concept_id IN (8527, 8516, 2100000001, 8515, 903096, 2000000008, 45882607, 1177221, 38003615, 8557) ) 
                    AND cb_search_person.person_id NOT IN (SELECT
                        criteria.person_id 
                    FROM
                        (SELECT
                            DISTINCT person_id, entry_date, concept_id 
                        FROM
                            `cb_search_all_events` 
                        WHERE
                            (concept_id IN(SELECT
                                DISTINCT c.concept_id 
                            FROM
                                `cb_criteria` c 
                            JOIN
                                (SELECT
                                    CAST(cr.id as string) AS id       
                                FROM
                                    `cb_criteria` cr       
                                WHERE
                                    concept_id IN (440508)       
                                    AND full_text LIKE '%_rank1]%'      ) a 
                                    ON (c.path LIKE CONCAT('%.', a.id, '.%') 
                                    OR c.path LIKE CONCAT('%.', a.id) 
                                    OR c.path LIKE CONCAT(a.id, '.%') 
                                    OR c.path = a.id) 
                            WHERE
                                is_standard = 1 
                                AND is_selectable = 1) 
                            AND is_standard = 1 )) criteria ) 
                    AND cb_search_person.person_id NOT IN (SELECT
                        person_id 
                    FROM
                        `person` p 
                    WHERE
                        race_concept_id IN (2100000001, 8515, 903096, 2000000008, 45882607, 1177221, 38003615, 8557) ) )
            )) measurement 
    LEFT JOIN
        `concept` m_standard_concept 
            ON measurement.measurement_concept_id = m_standard_concept.concept_id 
    LEFT JOIN
        `concept` m_type 
            ON measurement.measurement_type_concept_id = m_type.concept_id 
    LEFT JOIN
        `concept` m_value 
            ON measurement.value_as_concept_id = m_value.concept_id 
    LEFT JOIN
        `concept` m_source_concept 
            ON measurement.measurement_source_concept_id = m_source_concept.concept_id", sep="")

# Formulate a Cloud Storage destination path for the data exported from BigQuery.
# NOTE: By default data exported multiple times on the same day will overwrite older copies.
#       But data exported on a different days will write to a new location so that historical
#       copies can be kept as the dataset definition is changed.
measurement_62031819_path <- file.path(
  Sys.getenv("WORKSPACE_BUCKET"),
  "bq_exports",
  Sys.getenv("OWNER_EMAIL"),
  strftime(lubridate::now(), "%Y%m%d"),  # Comment out this line if you want the export to always overwrite.
  "measurement_62031819",
  "measurement_62031819_*.csv")
message(str_glue('The data will be written to {measurement_62031819_path}. Use this path when reading ',
                 'the data into your notebooks in the future.'))

# Perform the query and export the dataset to Cloud Storage as CSV files.
# NOTE: You only need to run `bq_table_save` once. After that, you can
#       just read data from the CSVs in Cloud Storage.
bq_table_save(
  bq_dataset_query(Sys.getenv("WORKSPACE_CDR"), dataset_62031819_measurement_sql, billing = Sys.getenv("GOOGLE_PROJECT")),
  measurement_62031819_path,
  destination_format = "CSV")


# Read the data directly from Cloud Storage into memory.
# NOTE: Alternatively you can `gsutil -m cp {measurement_62031819_path}` to copy these files
#       to the Jupyter disk.
read_bq_export_from_workspace_bucket <- function(export_path) {
  col_types <- cols(standard_concept_name = col_character(), standard_concept_code = col_character(), measurement_type_concept_name = col_character(), value_as_concept_name = col_character(), measurement_source_value = col_character(), source_concept_name = col_character(), source_concept_code = col_character())
  bind_rows(
    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),
        function(csv) {
          message(str_glue('Loading {csv}.'))
          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)
          if (is.null(col_types)) {
            col_types <- spec(chunk)
          }
          chunk
        }))
}
dataset_62031819_measurement_df <- read_bq_export_from_workspace_bucket(measurement_62031819_path)

dim(dataset_62031819_measurement_df)

head(dataset_62031819_measurement_df, 5)
library(tidyverse)
library(bigrquery)

# This query represents dataset "followup_cohort_data" for domain "observation" and was generated for All of Us Controlled Tier Dataset v7
dataset_62031819_observation_sql <- paste("
    SELECT
        observation.person_id,
        o_standard_concept.concept_name as standard_concept_name,
        o_standard_concept.concept_code as standard_concept_code,
        observation.observation_datetime,
        observation.value_as_number,
        o_value.concept_name as value_as_concept_name,
        observation.visit_occurrence_id,
        o_visit.concept_name as visit_occurrence_concept_name,
        observation.observation_source_value,
        o_source_concept.concept_name as source_concept_name,
        o_source_concept.concept_code as source_concept_code,
        observation.value_source_value 
    FROM
        ( SELECT
            * 
        FROM
            `observation` observation 
        WHERE
            (
                observation_concept_id IN (43054909) 
                OR  observation_source_concept_id IN (1554061, 35225061, 35225062, 35225063, 35225064, 35225065, 35225066, 35225067, 35225068, 35225373, 35225404, 35225436, 44820385, 44821564, 44822663, 44822705, 44823447, 44823833, 44823834, 44824957, 44827360, 44827367, 44827368, 44827369, 44827370, 44828554, 44829595, 44829639, 44830102, 44830794, 44830803, 44831947, 44831957, 44833056, 44834280, 44834282, 44835426, 44835472, 44835482, 44835497, 44836656, 44836657, 44837448, 44837833, 45537692, 45537694, 45542476, 45542482, 45547367, 45552133, 45552870, 45556876, 45557821, 45561673, 45566436, 45566472, 45576204, 45576209, 45577822, 45581085, 45585801, 45585835, 45590766, 45595484, 45595553, 45595557, 45600368, 45600370, 45605169, 45605170, 45609945, 45609986, 45609991, 45610000, 713855)
            )  
            AND (
                observation.PERSON_ID IN (SELECT
                    distinct person_id  
                FROM
                    `cb_search_person` cb_search_person  
                WHERE
                    cb_search_person.person_id IN (SELECT
                        person_id 
                    FROM
                        `person` p 
                    WHERE
                        race_concept_id IN (8527, 8516, 2100000001, 8515, 903096, 2000000008, 45882607, 1177221, 38003615, 8557) ) 
                    AND cb_search_person.person_id NOT IN (SELECT
                        criteria.person_id 
                    FROM
                        (SELECT
                            DISTINCT person_id, entry_date, concept_id 
                        FROM
                            `cb_search_all_events` 
                        WHERE
                            (concept_id IN(SELECT
                                DISTINCT c.concept_id 
                            FROM
                                `cb_criteria` c 
                            JOIN
                                (SELECT
                                    CAST(cr.id as string) AS id       
                                FROM
                                    `cb_criteria` cr       
                                WHERE
                                    concept_id IN (440508)       
                                    AND full_text LIKE '%_rank1]%'      ) a 
                                    ON (c.path LIKE CONCAT('%.', a.id, '.%') 
                                    OR c.path LIKE CONCAT('%.', a.id) 
                                    OR c.path LIKE CONCAT(a.id, '.%') 
                                    OR c.path = a.id) 
                            WHERE
                                is_standard = 1 
                                AND is_selectable = 1) 
                            AND is_standard = 1 )) criteria ) 
                    AND cb_search_person.person_id NOT IN (SELECT
                        person_id 
                    FROM
                        `person` p 
                    WHERE
                        race_concept_id IN (2100000001, 8515, 903096, 2000000008, 45882607, 1177221, 38003615, 8557) ) )
            )) observation 
    LEFT JOIN
        `concept` o_standard_concept 
            ON observation.observation_concept_id = o_standard_concept.concept_id 
    LEFT JOIN
        `concept` o_value 
            ON observation.value_as_concept_id = o_value.concept_id 
    LEFT JOIN
        `visit_occurrence` v 
            ON observation.visit_occurrence_id = v.visit_occurrence_id 
    LEFT JOIN
        `concept` o_visit 
            ON v.visit_concept_id = o_visit.concept_id 
    LEFT JOIN
        `concept` o_source_concept 
            ON observation.observation_source_concept_id = o_source_concept.concept_id", sep="")

# Formulate a Cloud Storage destination path for the data exported from BigQuery.
# NOTE: By default data exported multiple times on the same day will overwrite older copies.
#       But data exported on a different days will write to a new location so that historical
#       copies can be kept as the dataset definition is changed.
observation_62031819_path <- file.path(
  Sys.getenv("WORKSPACE_BUCKET"),
  "bq_exports",
  Sys.getenv("OWNER_EMAIL"),
  strftime(lubridate::now(), "%Y%m%d"),  # Comment out this line if you want the export to always overwrite.
  "observation_62031819",
  "observation_62031819_*.csv")
message(str_glue('The data will be written to {observation_62031819_path}. Use this path when reading ',
                 'the data into your notebooks in the future.'))

# Perform the query and export the dataset to Cloud Storage as CSV files.
# NOTE: You only need to run `bq_table_save` once. After that, you can
#       just read data from the CSVs in Cloud Storage.
bq_table_save(
  bq_dataset_query(Sys.getenv("WORKSPACE_CDR"), dataset_62031819_observation_sql, billing = Sys.getenv("GOOGLE_PROJECT")),
  observation_62031819_path,
  destination_format = "CSV")


# Read the data directly from Cloud Storage into memory.
# NOTE: Alternatively you can `gsutil -m cp {observation_62031819_path}` to copy these files
#       to the Jupyter disk.
read_bq_export_from_workspace_bucket <- function(export_path) {
  col_types <- cols(standard_concept_name = col_character(), standard_concept_code = col_character(), value_as_concept_name = col_character(), visit_occurrence_concept_name = col_character(), observation_source_value = col_character(), source_concept_name = col_character(), source_concept_code = col_character(), value_source_value = col_character())
  bind_rows(
    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),
        function(csv) {
          message(str_glue('Loading {csv}.'))
          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)
          if (is.null(col_types)) {
            col_types <- spec(chunk)
          }
          chunk
        }))
}
dataset_62031819_observation_df <- read_bq_export_from_workspace_bucket(observation_62031819_path)

dim(dataset_62031819_observation_df)

head(dataset_62031819_observation_df, 5)
library(tidyverse)
library(bigrquery)

# This query represents dataset "followup_cohort_data" for domain "procedure" and was generated for All of Us Controlled Tier Dataset v7
dataset_62031819_procedure_sql <- paste("
    SELECT
        procedure.person_id,
        p_standard_concept.concept_name as standard_concept_name,
        p_standard_concept.concept_code as standard_concept_code,
        procedure.procedure_datetime,
        procedure.visit_occurrence_id,
        p_visit.concept_name as visit_occurrence_concept_name,
        procedure.procedure_source_value,
        p_source_concept.concept_name as source_concept_name,
        p_source_concept.concept_code as source_concept_code 
    FROM
        ( SELECT
            * 
        FROM
            `procedure_occurrence` procedure 
        WHERE
            (
                procedure_concept_id IN (SELECT
                    DISTINCT c.concept_id 
                FROM
                    `cb_criteria` c 
                JOIN
                    (SELECT
                        CAST(cr.id as string) AS id       
                    FROM
                        `cb_criteria` cr       
                    WHERE
                        concept_id IN (2617378, 4021108, 4088217, 4120120, 4172515, 4197217, 4304536, 4324124)       
                        AND full_text LIKE '%_rank1]%'      ) a 
                        ON (c.path LIKE CONCAT('%.', a.id, '.%') 
                        OR c.path LIKE CONCAT('%.', a.id) 
                        OR c.path LIKE CONCAT(a.id, '.%') 
                        OR c.path = a.id) 
                WHERE
                    is_standard = 1 
                    AND is_selectable = 1) 
                OR  procedure_source_concept_id IN (SELECT
                    DISTINCT c.concept_id 
                FROM
                    `cb_criteria` c 
                JOIN
                    (SELECT
                        CAST(cr.id as string) AS id       
                    FROM
                        `cb_criteria` cr       
                    WHERE
                        concept_id IN (2002208, 2002282, 2003564, 2003608, 2003610, 2003622, 2003623, 2108292, 2109569, 2109574, 2213572, 2213576, 2314206, 2773680, 2776753, 2786488, 2788041, 44820493, 44821578, 44825821, 44832721, 44833130, 45585835, 45609945)       
                        AND full_text LIKE '%_rank1]%'      ) a 
                        ON (c.path LIKE CONCAT('%.', a.id, '.%') 
                        OR c.path LIKE CONCAT('%.', a.id) 
                        OR c.path LIKE CONCAT(a.id, '.%') 
                        OR c.path = a.id) 
                WHERE
                    is_standard = 0 
                    AND is_selectable = 1)
            )  
            AND (
                procedure.PERSON_ID IN (SELECT
                    distinct person_id  
                FROM
                    `cb_search_person` cb_search_person  
                WHERE
                    cb_search_person.person_id IN (SELECT
                        person_id 
                    FROM
                        `person` p 
                    WHERE
                        race_concept_id IN (8527, 8516, 2100000001, 8515, 903096, 2000000008, 45882607, 1177221, 38003615, 8557) ) 
                    AND cb_search_person.person_id NOT IN (SELECT
                        criteria.person_id 
                    FROM
                        (SELECT
                            DISTINCT person_id, entry_date, concept_id 
                        FROM
                            `cb_search_all_events` 
                        WHERE
                            (concept_id IN(SELECT
                                DISTINCT c.concept_id 
                            FROM
                                `cb_criteria` c 
                            JOIN
                                (SELECT
                                    CAST(cr.id as string) AS id       
                                FROM
                                    `cb_criteria` cr       
                                WHERE
                                    concept_id IN (440508)       
                                    AND full_text LIKE '%_rank1]%'      ) a 
                                    ON (c.path LIKE CONCAT('%.', a.id, '.%') 
                                    OR c.path LIKE CONCAT('%.', a.id) 
                                    OR c.path LIKE CONCAT(a.id, '.%') 
                                    OR c.path = a.id) 
                            WHERE
                                is_standard = 1 
                                AND is_selectable = 1) 
                            AND is_standard = 1 )) criteria ) 
                    AND cb_search_person.person_id NOT IN (SELECT
                        person_id 
                    FROM
                        `person` p 
                    WHERE
                        race_concept_id IN (2100000001, 8515, 903096, 2000000008, 45882607, 1177221, 38003615, 8557) ) )
            )) procedure 
    LEFT JOIN
        `concept` p_standard_concept 
            ON procedure.procedure_concept_id = p_standard_concept.concept_id 
    LEFT JOIN
        `visit_occurrence` v 
            ON procedure.visit_occurrence_id = v.visit_occurrence_id 
    LEFT JOIN
        `concept` p_visit 
            ON v.visit_concept_id = p_visit.concept_id 
    LEFT JOIN
        `concept` p_source_concept 
            ON procedure.procedure_source_concept_id = p_source_concept.concept_id", sep="")

# Formulate a Cloud Storage destination path for the data exported from BigQuery.
# NOTE: By default data exported multiple times on the same day will overwrite older copies.
#       But data exported on a different days will write to a new location so that historical
#       copies can be kept as the dataset definition is changed.
procedure_62031819_path <- file.path(
  Sys.getenv("WORKSPACE_BUCKET"),
  "bq_exports",
  Sys.getenv("OWNER_EMAIL"),
  strftime(lubridate::now(), "%Y%m%d"),  # Comment out this line if you want the export to always overwrite.
  "procedure_62031819",
  "procedure_62031819_*.csv")
message(str_glue('The data will be written to {procedure_62031819_path}. Use this path when reading ',
                 'the data into your notebooks in the future.'))

# Perform the query and export the dataset to Cloud Storage as CSV files.
# NOTE: You only need to run `bq_table_save` once. After that, you can
#       just read data from the CSVs in Cloud Storage.
bq_table_save(
  bq_dataset_query(Sys.getenv("WORKSPACE_CDR"), dataset_62031819_procedure_sql, billing = Sys.getenv("GOOGLE_PROJECT")),
  procedure_62031819_path,
  destination_format = "CSV")


# Read the data directly from Cloud Storage into memory.
# NOTE: Alternatively you can `gsutil -m cp {procedure_62031819_path}` to copy these files
#       to the Jupyter disk.
read_bq_export_from_workspace_bucket <- function(export_path) {
  col_types <- cols(standard_concept_name = col_character(), standard_concept_code = col_character(), visit_occurrence_concept_name = col_character(), procedure_source_value = col_character(), source_concept_name = col_character(), source_concept_code = col_character())
  bind_rows(
    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),
        function(csv) {
          message(str_glue('Loading {csv}.'))
          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)
          if (is.null(col_types)) {
            col_types <- spec(chunk)
          }
          chunk
        }))
}
dataset_62031819_procedure_df <- read_bq_export_from_workspace_bucket(procedure_62031819_path)

dim(dataset_62031819_procedure_df)

head(dataset_62031819_procedure_df, 5)
```

# Libraries

```{r}
library(knitr)
library(dplyr)
library(tidyr)
library(lubridate)
library(purrr)
library(stringr)

#install.packages("survival")
#install.packages("survminer")

# Load the necessary libraries
library(survival)

# install.packages("RColorBrewer")
library(RColorBrewer)

# Install and load necessary packages
#install.packages("caret")
#install.packages("ggplot2")
library(caret)
library(ggplot2)

# Load necessary libraries
#library(pROC)

```

# Data Wrangling

```{r}
# rename datasets
condition_df <- dataset_62031819_condition_df
device_df <- dataset_62031819_device_df
drug_df <- dataset_62031819_drug_df
measurement_df <- dataset_62031819_measurement_df
observation_df <- dataset_62031819_observation_df
person_df <- dataset_62031819_person_df
procedure_df <- dataset_62031819_procedure_df 

```


## Condition

```{r}
##########################################################################
# condition
# subsetting data: 119 variables
# Define the list of conditions and corresponding codes

conditions <- c(
    "Acidosis", #1
    "Acquired coagulation factor deficiency", #2
    "Acquired absence of limb", #3
    "Acquired hypothyroidism", #4
    "Acute glomerulonephritis", #5
    "Acute renal failure syndrome", #6
    "Altered mental status", #7
    "Amyloidosis", #8
    "Other anemias", #9.1
    "Anemia in chronic kidney disease", #9.2
    "Anemia of chronic disease", #9.3
    "Oliguria and Anuria", #10
    "Arteriovenous fistula", #11
    "Atherosclerosis of the extremities", #12.1
    "Atherosclerosis of renal artery", #12.2
    "Atherosclerosis of coronary artery without angina pectoris", #12.3
    "Bacteremia", #13
    "Changes in skin texture", #14
    "Chronic glomerulonephritis", #15.1
    "Glomerulonephritis", #15.2
    "Chronic graft-versus-host disease", #16
    "Chronic kidney disease stage 1", #17
    "Chronic kidney disease stage 2", #18
    "Chronic kidney disease stage 3", #19
    "Chronic kidney disease stage 4", #20
    "Chronic pain syndrome",#21
    "Chronic renal failure", #22
    "Chronic ulcer of lower extremity", #23
    "Chronic vascular insufficiency of intestine", #24
    "Clostridioides difficile infection", #25
    "Complication associated with insulin pump", #26 #rename insulin pump user
    "Mechanical complication of peritoneal dialysis catheter", #27 #rename peritoneal dialysis
    "Complication of renal dialysis", #28.1
    #"Hemodialysis", #28.2
    #"Congenital anomaly of the kidney", #29
    #"Congenital osteodystrophy", #30
    "Congestive heart failure", #31
    "Other deficiency anemia", #32.1
    "deficiency anemias", #32.2
    "folate-deficiency anemia", #32.3
    "Degenerative skin disorder", #33
    "Diabetes mellitus", #34
    "Disorder of artery", #35
    "Disorder of cardiovascular system", #36
    "Disorder of eye due to type 1 diabetes mellitus", #37
    "Disorder of eye due to type 2 diabetes mellitus", #38
    "Disorder of hard tissues of teeth", #39
    "Disorder of kidney and/or ureter", #40
    "Disorder of mineral metabolism", #41
    "Disorder of muscle", #42
    "Disorder of parathyroid gland", #43
    "Disorder of penis", #44
    "Disorder of phosphate, calcium and vitamin D metabolism", #45
    "Disorder of phosphorus metabolism", #46
    "Disorder of plasma protein metabolism", #47
    "Disorder of porphyrin metabolism", #48
    "Disorder of the urea cycle metabolism", #49
    "End-stage renal disease", #50
    "Essential hypertension", #51
    "Failure to thrive in neonate", #52
    "Frank hematuria", #53
    "Gangrene", #54.1
    "Atherosclerosis of native arteries of the extremities with ulceration or gangrene", #54.2
    "Gout", #55.1
    "Gouty arthropathy", #55.2
    "Granulomatosis with polyangiitis", #56
    "Hydronephrosis", #57
    "Hypercalcemia", #58
    "Hypercoagulability state", #59
    "Hyperkalemia", #60
    "Hyperparathyroidism", #61
    "Hyperparathyroidism due to renal insufficiency", #62
    "Hypertensive complication", #63
    "Hypertensive heart and chronic kidney disease", #64
    "Hypertensive heart AND renal disease", #65
    "Hypertrophy of kidney", #66
    "Hypervolemia", #67
    "Hypocalcemia", #68
    "Hypoglycemia", #69
    "Hypoparathyroidism", #70
    "Hypothyroidism", #71
    "Iatrogenic hypotension", #72
    "Impaction of intestine", #73
    "Impaired renal function disorder", #74
    "Injury of globe of eye", #75
    "Iron deficiency anemias", #76.1
    "Iron deficiency anemias, unspecified or not due to blood loss", #76.2
    "Ketoacidosis due to type 1 diabetes mellitus", #77
    "Ketoacidosis due to type 2 diabetes mellitus", #78
    "Low blood pressure", #79
    "Lupus erythematosus", #80
    "Mechanical complication of cardiac device, implant AND/OR graft", #81
    "Megaloblastic anemia due to folate deficiency", #82
    "Membranous glomerulonephritis", #83
    "Metabolic encephalopathy", #84
    #"Multiple congenital cysts of kidney", #85
    "Myoclonus", #86
    "Nephritic syndrome", #87
    "Nephropathy co-occurrent and due to systemic lupus erythematosus", #88
    "Nephrosclerosis", #89
    "Nephrotic syndrome", #90
    "Non-autoimmune hemolytic anemia", #91
    "Peripheral circulatory disorder due to type 1 diabetes mellitus", #92
    "Peripheral circulatory disorder due to type 2 diabetes mellitus", #93
    "Peripheral vascular complication", #94
    "Peripheral vascular disease", #95
    "Peripheral venous insufficiency", #96
    "Polyneuropathy due to diabetes mellitus", #97
    "Postoperative shock", #98
    "Proteinuria", #99
    "Proliferative glomerulonephritis", #100
    "Renal disorder due to type 1 diabetes mellitus", #101
    "Renal disorder due to type 2 diabetes mellitus", #102
    "Acute renal failure syndrome	", #103.1
    "Renal failure syndrome", #103.2
    "Renal function tests abnormal", #104
    "Renal osteodystrophy", #105
    "Respiratory failure", #106
    "Retinal edema", #107
    "Retinopathy due to diabetes mellitus", #108
    "Screening finding", #109
    "Secondary diabetes mellitus", #110
    "Sepsis", #111.1
    "Sepsis due to Gram negative bacteria", #111.2
    "Gram positive sepsis", #111.3
    "Septic shock", #112
    "Septicemia due to enterococcus", #113
    "Shock",#114
    "Small kidney", #115
    "Systemic lupus erythematosus", #116
    "Systemic sclerosis", #117
    "Thrombotic microangiopathy", #118
    "Transplanted kidney present", #119
    "Tubulointerstitial nephritis", #120
    "Type 1 diabetes mellitus", #121
    "Type 2 diabetes mellitus", #122
    "Vascular insufficiency of intestine" #123
)

phecode <- c(
  "PheCode:276.41", #1
    "PheCode:286.4", #2
    "PheCode:1089", #3
    "PheCode:244.2", #4
    "PheCode:580.13", #5
    "PheCode:585.1", #6
    "PheCode:292.4", #7
    "PheCode:270.33", #8
    "PheCode:285", #9.1
    "PheCode:285.21", #9.2
    "PheCode:285.2", #9.3
    "PheCode:599.6", #10
    "CCS:57", #11
    "PheCode:440.2", #12.1
    "PheCode:440.1", #12.2
    "PheCode:440.21", #12.3 
    "PheCode:038.3", #13
    "PheCode:687.3", #14
    "PheCode:580.14", #15.1
    "PheCode:580.1", #15.2
    "PheCode:081.12", #16
    "PheCode:585.4", #17
    "PheCode:585.4", #18
    "PheCode:585.33", #19
    "PheCode:585.34", #20
    "PheCode:355.1",#21
    "PheCode:585.3", #22
    "PheCode:707.3", #23
    "PheCode:441.2", #24
    "PheCode:008.52", #25
    "PheCode:250.3", #26 
    "CCS:91", #27 #Peritoneal dialysis
    "PheCode:585.31", #28.1 #Renal dialysis #check data values
    #"CCS:58", #28.2 #Hemodialysis
    #"Congenital anomaly of the kidney", #29
    #"Congenital osteodystrophy", #30
    "PheCode:428", #31
    "PheCode:281", #32.1
    "PheCode:281.9", #32.2
    "PheCode:281.13", #32.3
    "PheCode:702.4", #33
    "PheCode:250", #34
    "PheCode:447", #35
    "PheCode:459", #36
    "PheCode:250.13", #37
    "PheCode:250.23", #38
    "PheCode:521", #39
    "PheCode:586.1", #40
    "PheCode:275", #41
    "PheCode:359.2", #42
    "PheCode:252", #43
    "PheCode:604", #44
    "PheCode:275.5", #45
    "PheCode:275.53", #46
    "PheCode:270.38", #47
    "PheCode:277.1", #48
    "PheCode:270.21", #49
    "PheCode:585.32", #50
    "PheCode:401.1", #51
    "PheCode:264.2", #52
    "PheCode:593.1", #53
    "PheCode:791", #54.1
    "PheCode:440.21", #54.2
    "PheCode:274.1", #55.1
    "PheCode: 274.11", #55.2
    "PheCode:446.4", #56
    "PheCode:595", #57
    "PheCode:275.51", #58
    "PheCode:286.8", #59
    "PheCode:276.13", #60
    "PheCode:252.1", #61
    "PheCode:588.2", #62
    "PheCode:401.3", #63
    "PheCode:401.22", #64
    "PheCode:401.2", #65
    "PheCode:586.3", #66
    "PheCode:276.6", #67
    "PheCode:275.51", #68
    "PheCode:251.1", #69
    "PheCode:252.2", #70
    "PheCode:244", #71
    "PheCode:458.2", #72
    "PheCode:560.2", #73
    "PheCode:588", #74
    "PheCode:360", #75
    "PheCode: 280", #76.1
    "PheCode: 280.1", #76.2
    "PheCode:250.11", #77
    "PheCode:250.21", #78
    "PheCode:458.9", #79
    "PheCode:695.4", #80
    "PheCode:854", #81
    "PheCode:281.13", #82
    "PheCode:580.12", #83
    "PheCode:348.8", #84
    #"Multiple congenital cysts of kidney", #85
    "PheCode:333.2", #86
    "PheCode:580.32", #87
    "PheCode:580.31", #88
    "PheCode:580.4", #89
    "PheCode:580.2", #90
    "PheCode:283.2", #91
    "PheCode:250.15", #92
    "PheCode:250.25", #93
    "PheCode:443.8", #94
    "PheCode:443.9", #95
    "PheCode:456", #96
    "PheCode:250.6", #97
    "PheCode:958.1", #98
    "PheCode:269", #99
    "PheCode:580.11", #100
    "PheCode:250.12", #101
    "PheCode:250.22", #102
    "PheCode:585", #103.1
    "PheCode:585.2", #103.2
    "PheCode:589", #104
    "PheCode:588.1", #105
    "PheCode:509.1", #106
    "PheCode:362.9", #107
    "PheCode:250.7", #108
    "PheCode:1010.1", #109
    "PheCode:249", #110
    "PheCode:994.2", #111.1
    "PheCode:038.1", #111.2
    "PheCode:038.2", #111.3
    "PheCode:994.21", #112
    "PheCode:038", #113
    "PheCode:797",#114
    "PheCode:586.11", #115
    "PheCode:695.42", #116
    "PheCode:709.3", #117
    "PheCode:446.8", #118
    "PheCode:587", #119
    "PheCode:580.3", #120
    "PheCode:250.1", #121
    "PheCode:250.2", #122
    "PheCode:441" #123
)



# Corresponding ICD codes for each condition
snomed_codes <- list(
  51387008, #1
  25904003, #2
  816117000, #3
  40930008, #4
  19351000, #5
  14669001, #6
  419284004, #7
  17602002, #8
  271737000, #9.1
  707323002, #9.2
  234347009, #9.3
  c(83128009,2472002), #10
  439470001, #11
  51274000, #12.1
  45281005, #12.2
  451041000124103, #12.3
  5758002, #13
  274672009, #14
  20917003, #15.1
  36171008, #15.2
  402356004, #16
  431855005, #17
  431856006, #18
  433144002, #19
  431857002, #20
  373621006, #21
  90688005, #22
  26649005, #23
  111354009, #24
  186431008, #25
  473033004, #26
  431028002, #27 
  33603003, #28.1
  # 302497006, #28.2
  # 110182004, #29
  # 348350001, #30
  42343007, #31
  66612000, #32.1
  267513007, #32.2
  85649008, #32.3
  396325007, #33
  73211009, #34
  359557001, #35
  49601007, #36
  739681000, #37
  422099009, #38
  46557008, #39
  443820000, #40
  45744005, #41
  129565002, #42
  73132005, #43
  33958003, #44
  237879001, #45
  87049008, #46
  147211000119101, #47
  29094004, #48
  36444000, #49
  46177005, #50
  59621000, #51
  134251000119105, #52
  197941005, #53
  372070002, #54.1
  15649901000119104, #54.2
  90560007, #55.1
  190828008, #55.2
  195353004, #56
  43064006, #57
  66931009, #58
  76612001, #59
  14140009, #60
  66999008, #61
  19034001, #62
  449759005, #63
  8501000119104, #64
  86234004, #65
  88531004, #66
  21639008, #67
  5291005, #68
  302866003, #69
  36976004, #70
  40930008, #71
  408668005, #72
  62851005, #73
  197663003, #74
  231794000, #75
  87522002, #76.1
  724556004, #76.2
  420270002, #77
  421750000, #78
  45007003, #79
  200936003, #80
  473041004, #81
  85649008, #82
  77182004, #83
  50122000, #84
  # 82525005, #85
  17450006, #86
  7724006, #87
  295101000119105, #88
  32916005, #89
  52254009, #90
  4854004, #91
  421365002, #92
  422166005, #93
  10596002, #94
  400047006, #95
  20696009, #96
  49455004, #97
  58581001, #98
  29738008, #99
  441815006, #100
  421893009, #101
  420279001, #102
  14669001, #103.1
  42399005, #103.2
  167180005, #104
  16726004, #105
  409622000, #106
  6141006, #107
  4855003, #108
  365856005, #109
  8801005, #110
  91302008, #111.1
  449082003, #111.2
  194394004, #111.3
  76571007, #112
  310669007, #113
  27942005, #114
  236448000, #115
  55464009, #116
  89155008, #117
  126729006, #118
  737295003, #119
  428255004, #120
  46635009, #121
  44054006, #122
  82196007 #123
)
```

### Proteinuria

```{r}
# Filter the rows in condition_df based on the given criteria
filtered_rows <- condition_df %>%
  filter(
    source_concept_code %in% c("29738008") |
    standard_concept_name == "Proteinuria"
  )

# Print the filtered rows
print(filtered_rows)

# Count the number of unique person_id
num_unique_persons <- filtered_rows %>%
  summarise(num_unique_ids = n_distinct(person_id))

# Print the number of unique person_id
print(paste("Number of unique person_id:", num_unique_persons$num_unique_ids))

# Count the number of times each unique person_id appears
person_id_counts <- filtered_rows %>%
  group_by(person_id) %>%
  summarise(count = n()) %>%
  arrange(desc(count))

# Print the count for each person_id
print(person_id_counts)

```

### Without Roll-up Count, select on both SNOMED and ICD:

The code snippet will only match variables that exactly match the codes in `snomed_code` and `icd_codes_list`.

```{r}
# Create a dataframe to map conditions, SNOMED, ICD codes, and Phecodes
conditions_df <- data.frame(
  condition = I(conditions),
  phecode = phecode,
  snomed_code = I(sapply(snomed_codes, function(x) toString(x)))
)

# Initialize an empty dataframe to store the results
filtered_conditions <- data.frame()

# Loop through each condition and filter the dataset
for (i in 1:nrow(conditions_df)) {
  phecode_name <- conditions_df$phecode[i]
  snomed_code <- unlist(strsplit(as.character(conditions_df$snomed_code[i]), ", "))

  temp_df <- condition_df %>%
    filter(standard_concept_code %in% snomed_code) %>%
    mutate(new_name = phecode_name)

  filtered_conditions <- rbind(filtered_conditions, temp_df)
}

# Count the number of unique person_id for each variable in new_name
unique_counts <- filtered_conditions %>%
  group_by(new_name) %>%
  summarize(unique_person_count = n_distinct(person_id))

# View the result
print(unique_counts)

```


### ESRD

```{r}
# Identify person_id with End-stage renal disease and get the earliest condition_start_datetime for each person_id
condition_filtered_ESRD <- filtered_conditions %>%
  filter(new_name == "PheCode:585.32") %>%
  group_by(person_id) %>%
  summarize(disease_date = min(condition_start_datetime)) %>%
  ungroup()

# Add columns disease_status and disease_date to the person dataset
person <- person_df %>%
  left_join(condition_filtered_ESRD, by = "person_id") %>%
  mutate(
    disease_status = if_else(!is.na(disease_date), 1, 0)
  )

# Count the number of unique person_id where disease_status is 1
num_ESRD <- person %>%
  filter(disease_status == 1) %>%
  summarise(count = n_distinct(person_id)) %>%
  pull(count)

# Print the number of unique person_id with disease_status == 1
print(paste("Number of unique person_id with disease_status == 1:", num_ESRD))

# Print the resulting person dataset
print(person)
```
### ESRD by race
```{r}
# Identify person_id with End-stage renal disease and get the earliest condition_start_datetime for each person_id
condition_filtered_ESRD <- filtered_conditions %>%
  filter(new_name == "PheCode:585.32") %>%
  group_by(person_id) %>%
  summarize(disease_date = min(condition_start_datetime)) %>%
  ungroup()

# Add columns disease_status and disease_date to the person dataset
person <- person_df %>%
  left_join(condition_filtered_ESRD, by = "person_id") %>%
  mutate(
    disease_status = if_else(!is.na(disease_date), 1, 0)
  )

# Count the number of cases (disease_status == 1) and controls (disease_status == 0) by race
case_control_counts <- person %>%
  group_by(race, disease_status) %>%
  summarise(count = n_distinct(person_id), .groups = 'drop')

# Separate cases and controls for printing
cases <- case_control_counts %>%
  filter(disease_status == 1)

controls <- case_control_counts %>%
  filter(disease_status == 0)

# Print the number of cases and controls by race
print("Number of cases (disease_status == 1) by race:")
print(cases)

print("Number of controls (disease_status == 0) by race:")
print(controls)

```

## Device

```{r}
##########################################################################
# Define HCPCS code and the corresponding device name
HCPCS_code <- list("E1399")
device <- list("Durable medical equipment")

# Create a dataframe to map HCPCS code and device name
devices_df <- data.frame(
  HCPCS_code = unlist(HCPCS_code),
  device_name = unlist(device)
)

# Filter the dataset based on HCPCS codes and rename the selected variables
device_filtered <- device_df %>%
  filter(standard_concept_code %in% devices_df$HCPCS_code) %>%
  mutate(new_name = devices_df$device_name)

# View the filtered dataset with renamed variables
print(device_filtered)

# Summarize the number of unique person_id for each new_name
unique_person_id_count <- device_filtered %>%
  group_by(new_name) %>%
  summarize(unique_person_count = n_distinct(person_id))

# Print the summary table
print(unique_person_id_count)

```

## Drug

```{r}
##########################################################################
# drug

# Group by standard_concept_name and summarize the count of each
drug_summary <- drug_df %>%
  group_by(standard_concept_name) %>%
  summarize(count = n()) %>%
  arrange(desc(count))

# Create a vector containing all unique variables from the provided images
medication <- c(
  "aliskiren", #1
  "alteplase", #2
  "aluminum hydroxide", #3
  "amlodipine", #4
  "calcitriol", #5
  "calcium acetate", #6
  "carvedilol", #7
  "cascara sagrada", #8
  "ceftazidime", #9
  "cinacalcet", #10
  "citric acid", #11
  "daptomycin", #12
  "darbepoetin alfa", #13
  "dextran", #14
  "dextran 70", #15 #unavailable
  "dextran 75", #16
  "doxercalciferol", #17
  "elbasvir", #18
  "emtricitabine", #19
  "epoetin alfa", #20
  "etelcalcetide", #21
  "ethyl chloride", #22
  "etravirine", #23
  "grazoprevir", #24
  "heparin", #25
  "hepatitis B immune globulin", #26
  "hepatitis B surface antigen vaccine", #27
  "hydralazine", #28
  "insulin aspart protamine, human", #29
  "insulin aspart, human", #30
  "insulin detemir", #31
  "insulin glargine", #32
  "insulin glulisine, human", #33
  "insulin isophane", #34
  "insulin lispro", #35
  "insulin lispro protamine, human", #36
  "insulin, regular, human", #37
  "iron sucrose", #38
  "iron-dextran complex", #39
  "lanthanum", #40
  "mannitol", #41
  "methoxy polyethylene glycol-epoetin beta", #42 #unavailable
  "midodrine", #43
  "minoxidil", #44
  "nevirapine", #45
  "nifedipine", #46
  "paricalcitol", #47
  "pitavastatin", #48
  "protriptyline", #49
  "saquinavir", #50
  "sevelamer", #51
  "sodium bicarbonate", #52
  "sodium citrate", #53
  "sodium ferric gluconate complex", #54
  "sodium polystyrene sulfonate", #55
  "vancomycin", #56
  "water" #57
)

RXNORM_codes <- c(
  325646, #1
  8410, #2
  612, #3
  17767, #4
  1894, #5
  214342, #6
  20352, #7
  66869, #8
  2191, #9
  407990, #10
  21183, #11
  22299, #12
  283838, #13
  42635, #14
  3274, #15
  3275, #16
  11516, #17
  1734628, #18
  276237, #19
  105694, #20
  1876119, #21
  4141, #22
  475969, #23
  1734630, #24
  5224, #25
  26744, #26
  797752, #27
  5470, #28
  352385, #29
  51428, #30
  139825, #31
  274783, #32
  400008, #33
  1605101, #34
  86009, #35
  314684, #36
  253182, #37
  24909, #38
  5992, #39
  1311070, #40
  6628, #41
  729596, #42
  6963, #43
  6984, #44
  53654, #45
  7417, #46
  73710, #47
  861634, #48
  8886, #49
  83395, #50
  214824, #51
  36676, #52
  56466, #53
  261435, #54
  56512, #55
  11124, #56
  11295 #56
)
# Create a dataframe to map medication names and RXNORM codes
medication_df <- data.frame(
  medication = medication,
  RXNORM_code = as.character(RXNORM_codes)
)

# Ensure standard_concept_code in drug_df is character type
drug_df <- drug_df %>%
  mutate(standard_concept_code = as.character(standard_concept_code))

# Filter the dataset based on RXNORM codes and rename the selected variables
drug_filtered <- drug_df %>%
  filter(standard_concept_code %in% medication_df$RXNORM_code) %>%
  left_join(medication_df, by = c("standard_concept_code" = "RXNORM_code")) %>%
  mutate(new_name = medication)

# View the filtered dataset with renamed variables
print(drug_filtered)

# Find variables in medication list that are not in the drug_filtered$standard_concept_name column
not_in_drug_filtered <- setdiff(medication, drug_filtered$new_name)

# Print the result
print(not_in_drug_filtered)


# Summarize the data to get the unique count of person_id and total count for each variable
summary_table <- drug_filtered %>%
  group_by(new_name) %>%
  summarise(
    unique_person_id_count = n_distinct(person_id),
    count = n()
  )

# Print the summary table
print(summary_table)

```

## Measurement

```{r}
##########################################################################
# measurement

# Create a vector containing all unique variables from the provided images
measurements <- list(
  "Administration of medication", #1
  #"Arteriovenous anastomosis for renal dialysis", #2
  "Bilirubin.indirect [Mass/volume] in Serum or Plasma", #3
  "Calcium.ionized [Moles/volume] in Serum or Plasma", #4
  "Creatinine [Mass/volume] in Body fluid", #5.1
  "Cystatin C [Mass/volume] in Serum or Plasma", #5.2  
  "Erythrocyte distribution width [Ratio] by Automated count", #6
  "Ferritin [Mass/volume] in Serum or Plasma", #7
  "Fructosamine [Moles/volume] in Serum or Plasma", #8
  #"Hemodialysis", #9
  "Hepatitis B virus core IgG Ab [Units/volume] in Serum by Immunoassay", #10
  "Hepatitis B virus surface Ab [Units/volume] in Serum", #11
  "Hepatitis B virus surface Ab [Presence] in Serum by Immunoassay", #12
  "Hepatitis B virus surface Ab [Units/volume] in Serum", #13
  "Hepatitis B virus surface Ab [Units/volume] in Serum by Radioimmunoassay (RIA)", #14
  "Hepatitis B virus surface Ag [Presence] in Serum or Plasma by Immunoassay", #15
  "Hepatitis C virus Ab [Presence] in Serum or Plasma by Immunoassay", #16
  "Hepatitis C virus Ab [Presence] in Serum by Immunoblot", #17
  "HLA Ab [Presence] in Serum", #18
  "Iron binding capacity [Mass/volume] in Serum or Plasma", #19
  "Iron [Mass/volume] in Serum or Plasma", #20
  "Iron saturation [Mass Fraction] in Serum or Plasma", #21
  "Nucleated erythrocytes [#/volume] in Body fluid by Manual count", #22
  "Parathyrin.intact [Mass/volume] in Serum or Plasma", #23
  "Parathyroid Hormones", #24
  #"Partial nephrectomy", #25
  #"Peritoneal dialysis", #26
  "Protein [Mass/volume] in Urine", #27
  "Reticulocytes [#/volume] in Blood", #28
  "Reticulocytes/100 erythrocytes in Blood", #29
  "Therapeutic procedure", #30
  #"Total nephrectomy", #31
  "Iron binding capacity [Mass/volume] in Serum or Plasma", #32
  "Transferrin [Mass/volume] in Serum or Plasma", #33
  "Urate [Mass/volume] in Serum or Plasma", #34
  "Vancomycin [Mass/volume] in Serum or Plasma", #35


  "Glomerular filtration rate/1.73 sq M.predicted [Volume Rate/Area] in Serum, Plasma or Blood by Creatinine-based formula (MDRD)", #36
  "Hemoglobin A1c/Hemoglobin.total in Blood", #37
  "Systolic blood pressure", #38
  "Triglyceride [Mass/volume] in Serum or Plasma", #39
  
  "Albumin/Creatinine [Mass Ratio] in Urine", #40
  "Microalbumin/Creatinine [Mass Ratio] in Urine", #41.1
  "Albumin/Creatinine [Mass Ratio] in 24 hour Urine", #41.2
  "Albumin/Creatinine [Molar ratio] in Urine" #41.3
)

measurement_code <- list(
  "18629005", #1
  #"79827002", #2
  "1971-1", #3
  "1995-0", #4
  "2160-0", #5.1
  "33863-2", #5.2
  "788-0", #6
  "2276-4", #7
  "15069-8", #8
  #"302497006", #9
  "13919-6", #10
  "22322-2", #11
  "10900-9", #12
  "16935-9", #13
  "5194-6", #14
  "5196-1", #15
  "13955-0", #16
  "5199-5", #17
  "44534-6", #18
  "2500-7", #19
  "2498-4", #20
  "2502-3", #21
  "13530-1", #22
  "2731-8", #23
  "LP31659-3", #24
  #"81516001", #25
  #"71192002", #26
  "2888-6", #27
  "14196-0", #28
  "4679-7", #29
  "277132007", #30
  #"175905003", #31
  "359979000", #32
  "3034-6", #33
  "3084-1", #34
  "20578-1", #35
  
  
  "77147-7", #36
  "4548-4", #37
  "8480-6", #38
  "2571-8", #39
  
  "9318-7", #40
  "14959-1", #40.1
  "13705-9", #40.2
  "14585-4" #40.3
)


# Create a dataframe to map measurements and their codes
measurements_df <- data.frame(
  measurement = unlist(measurements),
  measurement_code = unlist(measurement_code)
)

# Filter the measurement_df based on measurement_code and create a new column with the corresponding variable names
filtered_measurements <- measurement_df %>%
  filter(standard_concept_code %in% measurements_df$measurement_code | source_concept_code %in% measurements_df$measurement_code) %>%
  left_join(measurements_df, by = c("standard_concept_code" = "measurement_code")) %>%
  mutate(new_name = measurement)


# View the filtered dataset with renamed variables
print(head(filtered_measurements))


# Find variables in diagnostics list that are not in the measurement_filtered_1$standard_concept_name column
not_in_measurement_filtered <- setdiff(measurements, filtered_measurements$new_name)

# Print the result
print(not_in_measurement_filtered)


# Summarize the data to get the unique person_id and count for each variable
summary_table <- filtered_measurements %>%
  group_by(standard_concept_name) %>%
  summarise(
    unique_person_id_count = n_distinct(person_id),
    count = n()
  )

# Print the summary table
print(summary_table)

```

#### BMI

```{r}
# Step 1: Filter the measurement data for height and weight
body_measurements <- measurement_df %>%
  filter(standard_concept_name %in% c("Body height", "Body weight"))

# Step 2: Reshape the data to have height and weight in separate columns
body_measurements_wide <- body_measurements %>%
  select(person_id, standard_concept_name, value_as_number) %>%
  spread(key = standard_concept_name, value = value_as_number)

# Step 3: Replace non-positive values with NA for height and weight and rename columns
body_measurements_wide <- body_measurements_wide %>%
  mutate(
    height = ifelse(`Body height` > 0, `Body height`, NA),   # Keep only positive height values and rename
    weight = ifelse(`Body weight` > 0, `Body weight`, NA)    # Keep only positive weight values and rename
  ) %>%
  select(person_id, height, weight)  # Keep only renamed columns

# Step 4: Calculate BMI for each person_id (BMI = weight(kg) / height(m)^2)
bmi_table <- body_measurements_wide %>%
  mutate(
    BMI = ifelse(!is.na(height) & !is.na(weight), 
                 weight / ((height / 100) ^ 2), NA)  # Convert height to meters for BMI calculation
  ) %>%
  select(person_id, height, weight, BMI)

# Step 5: Join the height, weight, and BMI data back to the person dataset
# Ensure all columns from person_df are retained
person <- person %>%
  left_join(bmi_table, by = "person_id")  # Merge height, weight, and BMI

# Print the resulting dataset to verify
print(person)

```
```{r}
library(dplyr)
library(tidyr)

# Step 1: Define the BMI categories
bmi_table <- bmi_table %>%
  mutate(
    BMI_category = case_when(
      BMI < 20 ~ "< 20",
      BMI >= 20 & BMI < 25 ~ "20 to < 25",
      BMI >= 25 & BMI < 30 ~ "25 to < 30",
      BMI >= 30 ~ "≥ 30",
      TRUE ~ NA_character_
    )
  )

# Step 2: Join BMI categories with the person dataset (which should include race)
person_with_bmi <- person %>%
  left_join(bmi_table %>% select(person_id, BMI_category), by = "person_id")

# Step 3: Count the number of people in each BMI category, by race
bmi_summary <- person_with_bmi %>%
  group_by(race, BMI_category) %>%
  summarise(
    count = n(),
    .groups = 'drop'
  )

# Step 4: Calculate the percentage for each group
bmi_summary <- bmi_summary %>%
  group_by(race) %>%
  mutate(percentage = (count / sum(count)) * 100)

# Step 5: Reshape the data for easier viewing in table format
bmi_summary_table <- bmi_summary %>%
  pivot_wider(
    names_from = BMI_category,
    values_from = c(count, percentage),
    names_glue = "{BMI_category}_{.value}"  # To distinguish between count and percentage columns
  )

# Step 6: Calculate the overall summary (for "All" race category)
bmi_summary_overall <- person_with_bmi %>%
  group_by(BMI_category) %>%
  summarise(
    count = n(),
    .groups = 'drop'
  ) %>%
  mutate(percentage = (count / sum(count)) * 100) %>%
  mutate(race = "All")

# Step 7: Combine the overall summary with the race-specific summary
bmi_summary_final <- bind_rows(
  bmi_summary_overall %>%
    pivot_wider(names_from = BMI_category, values_from = c(count, percentage), names_glue = "{BMI_category}_{.value}"),
  bmi_summary_table
)

# Step 8: Print the final summary
print(bmi_summary_final)


```


## Observation

```{r}
##########################################################################
# observation

## note: may not be used in the future due to missing proportion. Code missing as unknown.

# Define the concept name and LOINC code to filter
concept_name <- "Tobacco smoking status"
LOINC_code <- "72166-2"

# Filter the observation data frame to keep only rows where standard_concept_code matches the LOINC code
observation_filtered <- observation_df %>%
  filter(standard_concept_code == LOINC_code)

# Print the filtered data
print(observation_filtered)

# Summarize the data to get the unique count of person_id and total count for the concept name
summary_table <- observation_filtered %>%
  group_by(standard_concept_name) %>%
  summarise(
    unique_person_id_count = n_distinct(person_id),
    count = n()
  )

# Print the summary table
print(summary_table)

```

#### Smoking levels

```{r}
# Assuming observation_filtered is your dataframe, get the unique values
unique_values <- observation_filtered %>%
  select(value_as_concept_name, value_as_number) %>%
  distinct()

# View the result
print(unique_values)

```

#### Ranking smoking status

```{r}
# Define the ranks for each smoking status
smoking_status_rank <- data.frame(
  value_as_concept_name = c(
    "Never smoker",
    "Never smoked tobacco",
    "Unknown if ever smoked",
    "Former smoker",
    "Ex-smoker",
    "Current some day smoker",
    "Occasional tobacco smoker",
    "Light tobacco smoker",
    "Current every day smoker",
    "Smokes tobacco daily",
    "Smoker",
    "Smoker, current status unknown",
    "Heavy tobacco smoker"
  ),
  smoking_status_rank = c(
    1, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, 11
  )
)

# Merge the rank information with the original dataframe
observation_filtered <- observation_filtered %>%
  left_join(smoking_status_rank, by = "value_as_concept_name")

# View the result
print(observation_filtered)

```

## Procedure

```{r}
##########################################################################
# procedure

# Define the list of diagnostic procedures
procedure_list <- c(
  "Arteriovenous anastomosis for renal dialysis", #1
  "Hemodialysis", #2
  "Nephrectomy", #3
  "Peritoneal dialysis", #4
  "Direct skilled nursing services of a registered nurse (rn) in the home health or hospice setting, each 15 minutes" #5
  
)

procedure_codes <- list(
  79827002, #1
  302497006, #2
  c(81516001, 175905003), #3
  71192002, #4

  79827002 #5
)


# Determine the maximum length among all lists
max_length <- max(length(procedure_list), max(lengths(procedure_codes)))

# Extend lists to match the maximum length
procedure_list <- rep(procedure_list, length.out = max_length)
procedure_codes <- unlist(lapply(procedure_codes, function(x) paste(x, collapse = ", ")))  # Properly join multiple codes

# Create the data frame
procedures_df <- data.frame(
  procedure = procedure_list,
  procedure_code = procedure_codes,  # Already character due to collapsing
  stringsAsFactors = FALSE
)

# Convert standard_concept_code to character for matching
procedure_df <- procedure_df %>%
  mutate(
    standard_concept_code = as.character(standard_concept_code)
  )

# Initialize an empty dataframe to store the results
filtered_procedures <- data.frame()

# Loop through each procedure and filter the dataset
for (i in 1:nrow(procedures_df)) {
  procedure_name <- procedures_df$procedure[i]
  procedure_code <- unlist(strsplit(as.character(procedures_df$procedure_code[i]), ", "))

  temp_df <- procedure_df %>%
    filter(standard_concept_code %in% procedure_code) %>%
    mutate(new_name = procedure_name)

  filtered_procedures <- rbind(filtered_procedures, temp_df)
}

# Print the resulting data frame
print(filtered_procedures)

# Check the variables in filtered_procedures that are not in procedure_list
not_in_diagnostic_list <- setdiff(filtered_procedures$new_name, procedure_list)

# Print the variables that are not in diagnostic_list
print("Variables in filtered_procedures that are not in diagnostic_list:")
print(not_in_diagnostic_list)

# Summarize the data to get the unique person_id and count for each variable
summary_table <- filtered_procedures %>%
  group_by(new_name) %>%
  summarise(
    unique_person_id_count = n_distinct(person_id),
    count = n()
  )

# Print the summary table
print(summary_table)

```



# Creatnine_distribution
```{r}
# Target Population: eGFR
# Define the specific standard concept name to filter
concept_code <- "2160-0"

# Filter the measurement_filtered data frame
measurement_filtered_specific <- filtered_measurements %>%
  filter(standard_concept_code == concept_code)

# Filter to get only the first measurement for each person_id
first_measurements <- measurement_filtered_specific %>%
  group_by(person_id) %>%
  arrange(measurement_datetime) %>%
  slice(1) %>%
  ungroup()

# Print the resulting data frame
print(first_measurements)

# Attach individual specific race
# Function to add race column to a data frame based on person_id
add_race_column <- function(df, person_df) {
  df <- df %>%
    left_join(person_df %>% select(person_id, race), by = "person_id")
  return(df)
}

first_measurements <- add_race_column(first_measurements, person_df)

# Calculate the count and percentage of each group
group_summary <- first_measurements %>%
  group_by(race) %>%
  summarise(
    count = n()
  ) %>%
  ungroup() %>%
  mutate(
    percentage = (count / sum(count)) * 100
  )

# Print the summary table
print("Group summary with count and percentage by race:")
print(group_summary)


```

### Index date

```{r}
# Identify the first measurement for each person_id
first_measurements <- first_measurements %>%
  group_by(person_id) %>%
  arrange(measurement_datetime) %>%
  slice(1) %>%
  ungroup() %>%
  select(person_id, measurement_datetime, value_as_number) %>%
  rename(index_date = measurement_datetime, serum_creatinine = value_as_number)

# Add the index_date and serum_creatinine to the person dataset
person <- person %>%
  left_join(first_measurements, by = "person_id")

person <- person %>%
  mutate(index_date = as.Date(index_date))
```

### Index date summary statistics
```{r}
library(dplyr)
library(lubridate)

# Step 3: Calculate summary statistics for the entire dataset
summary_statistics <- person %>%
  summarise(
    earliest_index_date = min(index_date, na.rm = TRUE),
    latest_index_date = max(index_date, na.rm = TRUE),
    total_participants = n(),
    median_index_date = median(index_date, na.rm = TRUE),
    q1_index_date = quantile(as.numeric(index_date), probs = 0.25, na.rm = TRUE) %>% as.Date(origin = "1970-01-01"),
    q3_index_date = quantile(as.numeric(index_date), probs = 0.75, na.rm = TRUE) %>% as.Date(origin = "1970-01-01"),
    min_creatinine = min(serum_creatinine, na.rm = TRUE),
    max_creatinine = max(serum_creatinine, na.rm = TRUE),
    mean_creatinine = mean(serum_creatinine, na.rm = TRUE),
    q1_creatinine = quantile(serum_creatinine, probs = 0.25, na.rm = TRUE),
    q3_creatinine = quantile(serum_creatinine, probs = 0.75, na.rm = TRUE),
    count_normal_egfr = sum(serum_creatinine >= 90, na.rm = TRUE),
    count_early_stage_ckd = sum(serum_creatinine >= 60 & serum_creatinine < 90, na.rm = TRUE),
    count_ckd = sum(serum_creatinine >= 15 & serum_creatinine < 60, na.rm = TRUE),
    count_esrd = sum(serum_creatinine < 15, na.rm = TRUE)
  )

# Print the summary statistics
print(summary_statistics)

# Step 4: Breakdown by race
summary_by_race <- person %>%
  group_by(race) %>%
  summarise(
    earliest_index_date = min(index_date, na.rm = TRUE),
    latest_index_date = max(index_date, na.rm = TRUE),
    total_participants = n(),
    median_index_date = median(index_date, na.rm = TRUE),
    q1_index_date = quantile(as.numeric(index_date), probs = 0.25, na.rm = TRUE) %>% as.Date(origin = "1970-01-01"),
    q3_index_date = quantile(as.numeric(index_date), probs = 0.75, na.rm = TRUE) %>% as.Date(origin = "1970-01-01"),
    min_creatinine = min(serum_creatinine, na.rm = TRUE),
    max_creatinine = max(serum_creatinine, na.rm = TRUE),
    mean_creatinine = mean(serum_creatinine, na.rm = TRUE),
    q1_creatinine = quantile(serum_creatinine, probs = 0.25, na.rm = TRUE),
    q3_creatinine = quantile(serum_creatinine, probs = 0.75, na.rm = TRUE),
    count_normal_egfr = sum(serum_creatinine >= 90, na.rm = TRUE),
    count_early_stage_ckd = sum(serum_creatinine >= 60 & serum_creatinine < 90, na.rm = TRUE),
    count_ckd = sum(serum_creatinine >= 15 & serum_creatinine < 60, na.rm = TRUE),
    count_esrd = sum(serum_creatinine < 15, na.rm = TRUE)
  )

# Print the breakdown by race
print(summary_by_race)

```
#### Table for Outliers by Race (Serum Creatinine = 1e+07)
```{r}
# Table 1: Count of people with outlier serum creatinine value of 1e+07 by race
creatinine_outlier_count <- person %>%
  filter(serum_creatinine == 1e+07) %>%
  group_by(race) %>%
  summarise(count = n()) 

# Print the result
print("Table 1: Count of people with outlier serum creatinine (1e+07) by race")
print(creatinine_outlier_count)

```
#### Table for Overall Summary Statistics
```{r}
# Table 2: Summary statistics for the entire dataset (after removing outliers)
# Filter out rows where serum_creatinine is equal to 1e+07
person_filtered <- person %>%
  filter(serum_creatinine != 1e+07)

summary_statistics <- person_filtered %>%
  summarise(
    earliest_index_date = min(index_date, na.rm = TRUE),
    latest_index_date = max(index_date, na.rm = TRUE),
    total_participants = n(),
    median_index_date = median(index_date, na.rm = TRUE),
    q1_index_date = quantile(as.numeric(index_date), probs = 0.25, na.rm = TRUE) %>% as.Date(origin = "1970-01-01"),
    q3_index_date = quantile(as.numeric(index_date), probs = 0.75, na.rm = TRUE) %>% as.Date(origin = "1970-01-01"),
    min_creatinine = min(serum_creatinine, na.rm = TRUE),
    max_creatinine = max(serum_creatinine, na.rm = TRUE),
    mean_creatinine = mean(serum_creatinine, na.rm = TRUE),
    q1_creatinine = quantile(serum_creatinine, probs = 0.25, na.rm = TRUE),
    q3_creatinine = quantile(serum_creatinine, probs = 0.75, na.rm = TRUE),

  )

# Print the overall summary statistics
print("Table 2: Overall Summary Statistics after Removing Outliers")
print(summary_statistics)

```
#### Table for Breakdown of Statistics by Race
```{r}
# Table 3: Breakdown of summary statistics by race (after removing outliers)
summary_by_race <- person_filtered %>%
  group_by(race) %>%
  summarise(
    earliest_index_date = min(index_date, na.rm = TRUE),
    latest_index_date = max(index_date, na.rm = TRUE),
    total_participants = n(),
    median_index_date = median(index_date, na.rm = TRUE),
    q1_index_date = quantile(as.numeric(index_date), probs = 0.25, na.rm = TRUE) %>% as.Date(origin = "1970-01-01"), # Unix epoch as the reference point for converting numbers back to dates.
    q3_index_date = quantile(as.numeric(index_date), probs = 0.75, na.rm = TRUE) %>% as.Date(origin = "1970-01-01"),
    min_creatinine = min(serum_creatinine, na.rm = TRUE),
    max_creatinine = max(serum_creatinine, na.rm = TRUE),
    mean_creatinine = mean(serum_creatinine, na.rm = TRUE),
    q1_creatinine = quantile(serum_creatinine, probs = 0.25, na.rm = TRUE),
    q3_creatinine = quantile(serum_creatinine, probs = 0.75, na.rm = TRUE),

  )

# Print the breakdown by race
print("Table 3: Summary Statistics Breakdown by Race after Removing Outliers")
print(summary_by_race)

```

```{r}
# Load necessary libraries
library(ggplot2)
library(dplyr)

# Filter out non-finite values (zeros or negative values) for serum_creatinine
filtered_person <- person_filtered %>%
  filter(serum_creatinine > 0,  # Remove non-positive values
         race %in% c("Black or African American", "White"))  # Filter relevant races

# Check if the filtering worked
summary(filtered_person$serum_creatinine)  # Should not show zeros or negative values

# Create the boxplot
ggplot(filtered_person, aes(x = race, y = serum_creatinine, fill = race)) +
  geom_boxplot(outlier.shape = 16, outlier.size = 2, notch = TRUE) +
  scale_y_log10() +  # Log transformation (only if your data is skewed)
  labs(
    title = "Comparison of Serum Creatinine Levels Across Racial Groups",
    x = "Race",
    y = "Serum Creatinine (mg/dL)",
    fill = "Race"
  ) +
  theme_minimal(base_size = 15) +  # Lancet-style minimalist theme
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    axis.title.x = element_text(face = "bold"),
    axis.title.y = element_text(face = "bold")
  ) +
  stat_summary(fun.data = "mean_cl_boot", geom = "errorbar", width = 0.2, color = "black") +
  theme(legend.position = "none")  # Remove legend if not needed

```

## Cohort

### Disease_date before index_date

```{r}
# Remove rows where disease_date is before index_date for each person_id with disease_status == 1
person_filtered <- person %>%
  filter(disease_status != 1 | !(disease_date < index_date)) %>%
  ungroup()


# Count the number of people with disease_status == 1 and disease_status == 0
status_counts <- person_filtered %>%
  group_by(disease_status) %>%
  summarise(count = n())

# Print the counts
print(status_counts)
```
#### case and control distribution by race
```{r}
# Remove rows where disease_date is before index_date for each person_id with disease_status == 1
person_filtered <- person %>%
  filter(disease_status != 1 | !(disease_date < index_date)) %>%
  ungroup()

# Count the number of people with disease_status == 1 and disease_status == 0, grouped by race
status_counts_by_race <- person_filtered %>%
  group_by(race, disease_status) %>%
  summarise(count = n()) %>%
  ungroup()

# Print the counts by race and disease status
print(status_counts_by_race)

```

### Age

```{r}
# Calculate age as index_date - date_of_birth
# Calculate precise age
person_filtered <- person_filtered %>%
  mutate(age_precise = as.numeric(difftime(index_date, date_of_birth, units = "days")) / 365.25)

# Round the age to the nearest whole number for reporting
person_filtered <- person_filtered %>%
  mutate(age_reported = round(age_precise))

# View the result
print(person_filtered)
```


# Feature Window

## Feature Window comparison

## Checking for perfect separation

```{r}
# Join 'index_date' and 'disease_status' from 'person' to 'filtered_conditions'

filtered_conditions0 <- filtered_conditions %>%
  left_join(person %>% select(person_id, index_date, disease_status), by = "person_id")

# Function to filter and summarize data based on feature window
summarize_conditions <- function(data, window_days) {
  data %>%
    filter(difftime(condition_start_datetime, index_date, units = "days") <= window_days) %>%
    group_by(new_name, disease_status) %>%
    summarize(unique_person_count = n_distinct(person_id), .groups = 'drop')
}

# Calculate the metrics for each feature window
summary_1year <- summarize_conditions(filtered_conditions0, 365)
summary_1_5years <- summarize_conditions(filtered_conditions0, 1.5 * 365.25)
summary_2years <- summarize_conditions(filtered_conditions0, 2 * 365.25)

# Add a column to indicate the feature window
summary_1year <- summary_1year %>% mutate(feature_window = "1 year")
summary_1_5years <- summary_1_5years %>% mutate(feature_window = "1.5 years")
summary_2years <- summary_2years %>% mutate(feature_window = "2 years")

# Combine the summaries into one table
summary_combined <- bind_rows(summary_1year, summary_1_5years, summary_2years)

# Pivot the table to have feature windows as columns
summary_table <- summary_combined %>%
  pivot_wider(names_from = feature_window, values_from = unique_person_count, values_fill = list(unique_person_count = 0))

# Print the resulting summary table
print(summary_table)

```

## 1 year from Index date

```{r}
feature_window <- 365

```
Filtering out conditions with perfect separation

```{r}
# # Step 1: Summarize conditions within the 1-year window (already done in your provided code)
# summary_1year <- summarize_conditions(filtered_conditions0, 365)
# 
# # Step 2: Identify features with perfect separation
# # For perfect separation, we are looking for features where either all participants have disease_status == 1 or disease_status == 0.
# perfect_separation_features <- summary_1year %>%
#   group_by(new_name) %>%
#   summarise(
#     count_status_0 = sum(disease_status == 0),
#     count_status_1 = sum(disease_status == 1)
#   ) %>%
#   filter(count_status_0 == 0 | count_status_1 == 0) %>%
#   pull(new_name)  # Get the names of the features with perfect separation
# 
# # Step 3: Remove features with perfect separation from the dataset
# filtered_conditions_cleaned <- filtered_conditions0 %>%
#   filter(!new_name %in% perfect_separation_features)
# 
# # Step 4: Recalculate the summary table without the features that show perfect separation
# summary_cleaned_1year <- summarize_conditions(filtered_conditions_cleaned, 365)
# summary_cleaned_1_5years <- summarize_conditions(filtered_conditions_cleaned, 1.5 * 365.25)
# summary_cleaned_2years <- summarize_conditions(filtered_conditions_cleaned, 2 * 365.25)
# 
# # Add feature window column to each summary
# summary_cleaned_1year <- summary_cleaned_1year %>% mutate(feature_window = "1 year")
# summary_cleaned_1_5years <- summary_cleaned_1_5years %>% mutate(feature_window = "1.5 years")
# summary_cleaned_2years <- summary_cleaned_2years %>% mutate(feature_window = "2 years")
# 
# # Step 5: Combine the cleaned summaries into one table
# summary_cleaned_combined <- bind_rows(summary_cleaned_1year, summary_cleaned_1_5years, summary_cleaned_2years)
# 
# # Step 6: Pivot the cleaned summary table to have feature windows as columns
# summary_cleaned_table <- summary_cleaned_combined %>%
#   pivot_wider(names_from = feature_window, values_from = unique_person_count, values_fill = list(unique_person_count = 0))
# 
# # Print the resulting cleaned summary table
# print(summary_cleaned_table)
# 
# # Step 7: Print the features with perfect separation (if needed)
# print("Features with perfect separation:")
# print(perfect_separation_features)

```


#### Condition count

```{r}
# Filter rows where 'condition_start_datetime' is 365 days or less from 'index_date'
filtered_conditions <- filtered_conditions_cleaned %>%
  filter(difftime(condition_start_datetime, index_date, units = "days") <= feature_window)

# Calculate the metrics for each unique person_id and new_name
summary_table1 <- filtered_conditions %>%
  group_by(new_name, person_id) %>%
  summarize(
    unique_count = n_distinct(condition_start_datetime, condition_end_datetime, visit_occurrence_id),
    last_end_date_condition = {
      if (all(is.na(condition_end_datetime))) {
        any_dates <- na.omit(c(condition_start_datetime, condition_end_datetime))
        if (length(any_dates) > 0) max(any_dates) else NA
      } else {
        max(condition_end_datetime, na.rm = TRUE)
      }
    }
  )

# Print the resulting summary table
print(summary_table1)

# Count the number of unique person_id in the filtered_conditions dataframe
unique_person_count <- filtered_conditions %>%
  summarize(unique_person_count = n_distinct(person_id))

# Print the count
print(unique_person_count)

```

##### Appending to person dataset and addressing missing data. by deletion

```{r}
# Ensure person_filtered includes only person_id values in filtered_conditions
person_filtered <- person_filtered %>%
  filter(person_id %in% filtered_conditions$person_id)

# Step 2: Create the wide format summary table
summary_wide <- summary_table1 %>%
  pivot_wider(
    id_cols = person_id,  # Include person_id as an identifier
    names_from = new_name,
    values_from = unique_count,
    values_fill = list(unique_count = 0)  # Specify values_fill as a list
  )

# Step 3: Join with person_filtered
person_with_summary <- person_filtered %>%
  left_join(summary_wide, by = "person_id")

# Optional: Check for non-zero counts in each column
non_zero_counts <- person_with_summary %>%
  summarise(across(everything(), ~ sum(. != 0, na.rm = TRUE)))

# Print in vertical format
for (col in names(non_zero_counts)) {
  cat(paste0(col, ": ", non_zero_counts[[col]], "\n"))
}

```

```{r}
# Count the number of people with disease_status == 0 and disease_status == 1
status_counts <- person_with_summary %>%
  group_by(disease_status) %>%
  summarise(count = n())

# Print the counts
print(status_counts)

```
```{r}
# Count the number of people with disease_status == 0 and disease_status == 1, grouped by race
status_counts_by_race <- person_with_summary %>%
  group_by(race, disease_status) %>%
  summarise(count = n(), .groups = 'drop')  # .groups = 'drop' prevents summarise from returning grouped data

# Print the counts by race and disease status
print(status_counts_by_race)

```

#### Device count

```{r}
# Join 'index_date' from 'person' to 'device_filtered'
device_filtered <- device_filtered %>%
  left_join(person %>% select(person_id, index_date), by = "person_id")

# Filter rows where 'device_exposure_end_datetime' is 365 days or less from 'index_date'
device_filtered <- device_filtered %>%
  filter(difftime(device_exposure_start_datetime, index_date, units = "days") <= feature_window)

# Calculate the metrics for each unique device_type and person_id
summary_table2 <- device_filtered %>%
  group_by(new_name, person_id) %>%
  summarize(
    unique_count = n_distinct(device_exposure_start_datetime, device_exposure_end_datetime, visit_occurrence_id),
    last_end_date_device = {
      if (all(is.na(device_exposure_end_datetime))) {
        any_dates <- na.omit(c(device_exposure_start_datetime, device_exposure_end_datetime))
        if (length(any_dates) > 0) max(any_dates) else NA
      } else {
        max(device_exposure_end_datetime, na.rm = TRUE)
      }
    }
  )

# Print the resulting summary table
print(summary_table2)

```

##### Appending to person dataset

```{r}
# Step 1: Create the wide format summary table
summary_wide <- summary_table2 %>%
  pivot_wider(
    id_cols = person_id,  # Include person_id as an identifier
    names_from = new_name,
    values_from = unique_count,
    values_fill = 0  # simplified values_fill since it's a scalar
  )

# Step 2: Join with person_filtered
person_with_summary <- person_with_summary %>%
  left_join(summary_wide, by = "person_id")


# Optional: Check for non-zero counts in each column
non_zero_counts <- person_with_summary %>%
  summarise(across(everything(), ~ sum(. != 0, na.rm = TRUE)))

print(non_zero_counts)
```

#### Drug count

```{r}
# Join 'index_date' from 'person' to 'drug_filtered'
drug_filtered <- drug_filtered %>%
  left_join(person %>% select(person_id, index_date), by = "person_id")

# Filter rows where 'drug_exposure_end_datetime' is 365 days or less from 'index_date'
drug_filtered <- drug_filtered %>%
  filter(difftime(drug_exposure_start_datetime, index_date, units = "days") <= feature_window)

# Calculate the metrics for each unique drug_concept_id and person_id
summary_table3 <- drug_filtered %>%
  group_by(new_name, person_id) %>%
  summarize(
    unique_count = n_distinct(drug_exposure_start_datetime, drug_exposure_end_datetime, visit_occurrence_id),
    last_end_date_drug = {
      if (all(is.na(drug_exposure_end_datetime))) {
        any_dates <- na.omit(c(drug_exposure_start_datetime, drug_exposure_end_datetime))
        if (length(any_dates) > 0) max(any_dates) else NA
      } else {
        max(drug_exposure_end_datetime, na.rm = TRUE)
      }
    }
  )

# Print the resulting summary table
print(summary_table3)

```

##### Appending to person dataset

```{r}
# Step 1: Create the wide format summary table
summary_wide <- summary_table3 %>%
  pivot_wider(
    id_cols = person_id,  # Include person_id as an identifier
    names_from = new_name,
    values_from = unique_count,
    values_fill = 0  # simplified values_fill since it's a scalar
  )

# Step 2: Join with person_filtered
person_with_summary <- person_with_summary %>%
  left_join(summary_wide, by = "person_id")


# Optional: Check for non-zero counts in each column
non_zero_counts <- person_with_summary %>%
  summarise(across(everything(), ~ sum(. != 0, na.rm = TRUE)))

print(non_zero_counts)
```

#### Measurement count

```{r}
# Join 'index_date' from 'person' to 'filtered_measurements'
filtered_measurements <- filtered_measurements %>%
  left_join(person %>% select(person_id, index_date), by = "person_id")

# Filter rows where 'measurement_datetime' is 365 days or less from 'index_date'
filtered_measurements <- filtered_measurements %>%
  filter(difftime(measurement_datetime, index_date, units = "days") <= feature_window)

# Calculate the metrics for each unique measurement_concept_id and person_id
summary_table4 <- filtered_measurements %>%
  group_by(new_name, person_id) %>%
  summarize(
    unique_count = n_distinct(measurement_datetime, visit_occurrence_id),
    last_measurement_date = max(measurement_datetime, na.rm = TRUE)
  )

# Print the resulting summary table
print(summary_table4)
```

### Albumin/Creatinine \[Mass Ratio\] in Urine first measurement

```{r}
# Filter and rename specific standard concept names to "Albumin/Creatinine"
measurement_filtered_specific <- filtered_measurements %>%
  filter(
    standard_concept_name %in% c(
      "Albumin/Creatinine [Mass Ratio] in Urine", 
      "Microalbumin/Creatinine [Mass Ratio] in Urine", 
      "Albumin/Creatinine [Mass Ratio] in 24 hour Urine", 
      "Albumin/Creatinine [Molar ratio] in Urine"
    )
  ) %>%
  rename(`Albumin/Creatinine` = value_as_number)

# Filter to get only the first measurement for each person_id
albumin_first_measurement <- measurement_filtered_specific %>%
  group_by(person_id) %>%
  arrange(measurement_datetime) %>%
  slice(1) %>%
  ungroup()

# Print the resulting data frame
print(albumin_first_measurement)

# Attach individual specific race
# Function to add race column to a data frame based on person_id
add_race_column <- function(df, person_df) {
  df <- df %>%
    left_join(person_df %>% select(person_id, race), by = "person_id")
  return(df)
}

albumin_first_measurement <- add_race_column(albumin_first_measurement, person_df)

# Define the groups based on the ranges provided
albumin_first_measurement <- albumin_first_measurement %>%
  mutate(group = case_when(
    `Albumin/Creatinine` < 30 ~ "< 30",
    `Albumin/Creatinine` >= 30 & `Albumin/Creatinine` < 300 ~ "30-299",
    `Albumin/Creatinine` >= 300 & `Albumin/Creatinine` < 2200 ~ "300-2199",
    `Albumin/Creatinine` >= 2200 ~ ">= 2200"
  ))

print(albumin_first_measurement)

# Calculate the count and percentage of each group
group_summary <- albumin_first_measurement %>%
  group_by(group) %>%
  summarise(
    count = n(),
    white_count = sum(race == "White"),
    black_count = sum(race == "Black or African American")
  ) %>%
  mutate(
    percentage = (count / sum(count)) * 100,
    white_percentage = (white_count / count) * 100,
    black_percentage = (black_count / count) * 100
  )

# Print the summary table
print("Group summary with count and percentage, including race breakdown:")
print(group_summary)

# Append `Albumin/Creatinine` to the `person_with_summary` dataset by `person_id`
person_with_summary <- person_with_summary %>%
  left_join(albumin_first_measurement %>% select(person_id, `Albumin/Creatinine`), by = "person_id")

# Print the updated person dataset
print(person_with_summary)

# Display column names of the updated dataset
colnames(person_with_summary)

```

### Attach all Albumin/creatinine measurments

```{r}
# Load necessary library
library(dplyr)

# Define the list of concept names you want to process
concept_names <- c(
  "Albumin/Creatinine [Mass Ratio] in Urine",
  "Microalbumin/Creatinine [Mass Ratio] in Urine",
  "Albumin/Creatinine [Mass Ratio] in 24 hour Urine",
  "Albumin/Creatinine [Molar ratio] in Urine"
)

# Function to filter and process each concept name
process_concept_name <- function(concept_name, filtered_measurements, person_with_summary) {
  # Filter the measurement_filtered data frame for the specific concept name
  measurement_filtered_specific <- filtered_measurements %>%
    filter(standard_concept_name == concept_name)
  
  # Filter to get only the first measurement for each person_id
  albumin_first_measurement <- measurement_filtered_specific %>%
    group_by(person_id) %>%
    arrange(measurement_datetime) %>%
    slice(1) %>%
    ungroup()
  
  # Rename the column to include the specific concept name
  col_name <- gsub("\\[.*\\]", "", concept_name)  # Remove the part within brackets
  col_name <- gsub(" ", "_", col_name)  # Replace spaces with underscores
  col_name <- paste0(col_name, "_value")  # Add a suffix for clarity
  
  albumin_first_measurement <- albumin_first_measurement %>%
    rename(!!col_name := value_as_number)
  
  # Append the filtered measurement to the person_with_summary dataset
  person_with_summary <- person_with_summary %>%
    left_join(albumin_first_measurement %>% select(person_id, !!sym(col_name)), by = "person_id")
  
  return(person_with_summary)
}

# Loop over each concept name and process it
for (concept_name in concept_names) {
  person_with_summary <- process_concept_name(concept_name, filtered_measurements, person_with_summary)
}

# Print the updated person_with_summary dataset
print(person_with_summary)
colnames(person_with_summary)

```

##### Appending to person dataset

```{r}
# Step 1: Create the wide format summary table
summary_wide <- summary_table4 %>%
  pivot_wider(
    id_cols = person_id,  # Include person_id as an identifier
    names_from = new_name,
    values_from = unique_count,
    values_fill = 0  # simplified values_fill since it's a scalar
  )

# Step 2: Join with person_filtered
person_with_summary <- person_with_summary %>%
  left_join(summary_wide, by = "person_id")


# Optional: Check for non-zero counts in each column
non_zero_counts <- person_with_summary %>%
  summarise(across(everything(), ~ sum(. != 0, na.rm = TRUE)))

print(non_zero_counts)
colnames(person_with_summary) 
```

#### Observation count

```{r}
# Join 'index_date' from 'person' to 'observation_filtered'
observation_filtered <- observation_filtered %>%
  left_join(person %>% select(person_id, index_date), by = "person_id")

# Filter rows where 'observation_datetime' is 365 days or less from 'index_date'
observation_filtered <- observation_filtered %>%
  filter(difftime(observation_datetime, index_date, units = "days") <= feature_window)

# Calculate the metrics for each unique standard_concept_name and person_id
summary_table5 <- observation_filtered %>%
  group_by(standard_concept_name, person_id) %>%
  reframe(
    unique_count = value_as_concept_name,
    last_observation_date = max(observation_datetime, na.rm = TRUE)
  )

# Print the resulting summary table
print(summary_table5)

```

##### Appending to person dataset

```{r}
# Custom function to handle NA values in max
max_na <- function(x) {
  if (all(is.na(x))) {
    return(NA)
  } else {
    return(max(x, na.rm = TRUE))
  }
}

# Step 1: Select the highest smoking status rank for each person
highest_smoking_status <- observation_filtered %>%
  group_by(person_id) %>%
  summarize(highest_smoking_status_rank = max_na(smoking_status_rank), .groups = 'drop')

# Step 2: Pivot the summary_table5 to wide format
summary_wide <- summary_table5 %>%
  pivot_wider(
    id_cols = person_id,  # Include person_id as an identifier
    names_from = standard_concept_name,
    values_from = unique_count,
    values_fill = list(unique_count = NA),  # Fill missing values with NA
    values_fn = list(unique_count = first)  # Use the first value in case of duplicates
  )

# Step 3: Join the highest smoking status rank with person_with_summary
person_with_summary <- person_with_summary %>%
  left_join(highest_smoking_status, by = "person_id") %>%
  left_join(summary_wide, by = "person_id")

# Optional: Check for non-zero counts in each column
non_zero_counts <- person_with_summary %>%
  summarise(across(everything(), ~ sum(. != 0, na.rm = TRUE)))

# Print the non-zero counts
print(non_zero_counts)

# Optional: Print the person_with_summary to verify
print(person_with_summary)

```

#### Procedure count

```{r}
# Join 'index_date' from 'person' to 'filtered_procedures'
filtered_procedures <- filtered_procedures %>%
  left_join(person %>% select(person_id, index_date), by = "person_id")

# Filter rows where 'procedure_datetime' is 365 days or less from 'index_date'
filtered_procedures <- filtered_procedures %>%
  filter(difftime(procedure_datetime, index_date, units = "days") <= feature_window)

# Calculate the metrics for each unique procedure_code and person_id
summary_table6 <- filtered_procedures %>%
  group_by(new_name, person_id) %>%
  summarize(
    unique_count = n_distinct(procedure_datetime, visit_occurrence_id),
    last_procedure_date = max(procedure_datetime, na.rm = TRUE)
  )

# Print the resulting summary table
print(summary_table6)

```

##### Appending to person dataset

```{r}
# Step 1: Create the wide format summary table
summary_wide <- summary_table6 %>%
  pivot_wider(
    id_cols = person_id,  # Include person_id as an identifier
    names_from = new_name,
    values_from = unique_count,
    values_fill = 0  # simplified values_fill since it's a scalar
  )

# Step 2: Join with person_filtered
person_with_summary <- person_with_summary %>%
  left_join(summary_wide, by = "person_id")


# Optional: Check for non-zero counts in each column
# Function to count non-zero values
count_non_zero <- function(df) {
  df %>%
    summarise(across(-contains("Tobacco smoking status"), ~ sum(. != 0, na.rm = TRUE)))
}

# Calculate non-zero counts for each column
non_zero_counts <- count_non_zero(person_with_summary)

# Print the resulting counts
# Transpose the non_zero_counts dataframe for vertical printing
non_zero_counts_vertical <- t(non_zero_counts)

# Print the transposed dataframe vertically
kable(non_zero_counts_vertical)
```

```{r}
# Load necessary library
library(dplyr)

# Select the relevant columns from person_with_summary
selected_columns <- person_with_summary %>%
  select(
    person_id,
    `Albumin/Creatinine`,
    `Albumin/Creatinine [Mass Ratio] in 24 hour Urine`,
    `Albumin/Creatinine [Mass Ratio] in Urine`,
    `Microalbumin/Creatinine [Mass Ratio] in Urine`
  )

# Print the resulting dataset
print(selected_columns)

```

## Checking for perfect separation

```{r}
# Identify numeric columns
numeric_columns <- sapply(person_with_summary, is.numeric)
numeric_columns <- names(numeric_columns[numeric_columns])

# Filter out non-numeric columns and pivot longer
person_long <- person_with_summary %>%
  select(person_id, disease_status, all_of(numeric_columns)) %>%
  pivot_longer(
    cols = -c(person_id, disease_status),
    names_to = "variable",
    values_to = "value"
  ) %>%
  filter(!is.na(value) & value != 0)  # Remove rows where the value is NA or zero

# Count occurrences of disease_status for each variable
status_counts <- person_long %>%
  group_by(variable, disease_status) %>%
  summarise(count = n_distinct(person_id), .groups = 'drop')

# Pivot the summary table to a wide format
summary_wide <- status_counts %>%
  pivot_wider(
    names_from = disease_status,
    values_from = count,
    names_prefix = "disease_status_",
    values_fill = list(count = 0)
  )

# Print the resulting summary table
print(summary_wide)
```

Remove Features with count less than 10

```{r}
# # Filter out variables that have less than a count of 10 cases or controls
# filtered_summary <- summary_wide %>%
#   filter(
#     disease_status_0 >= 10 & disease_status_1 >= 10
#   )
# 
# # Print the filtered summary table
# print("Filtered Summary Table:")
# print(filtered_summary)
# 
# # Recount the occurrences of disease_status for each variable after filtering and display them in a column format
# status_counts_filtered <- filtered_summary %>%
#   select(variable, disease_status_0, disease_status_1) %>%
#   pivot_longer(
#     cols = c(disease_status_0, disease_status_1),
#     names_to = "disease_status",
#     values_to = "count"
#   )
# 
# # Arrange the data to have the disease status counts printed next to each other
# status_counts_formatted <- status_counts_filtered %>%
#   arrange(variable, disease_status) %>%
#   pivot_wider(
#     names_from = disease_status,
#     values_from = count,
#     names_prefix = "disease_status_"
#   )
# 
# # Print the recount of disease_status occurrences after filtering in a column format
# print("Recount of disease_status occurrences after filtering (in column format):")
# print(status_counts_formatted)

```

### Albumin/creatinine check

```{r}
# # Select the specific columns and print them next to each other
# selected_columns <- person_with_summary %>%
#   select(
#     `Albumin/Creatinine`, 
#     `Albumin/Creatinine [Mass Ratio] in Urine`, 
#     `Albumin/Creatinine__in_24_hour_Urine_value`,
#     `Albumin/Creatinine__in_Urine_value.x`,
#     `Albumin/Creatinine__in_Urine_value.y`,
#     `Microalbumin/Creatinine [Mass Ratio] in Urine`, 
#     `Microalbumin/Creatinine__in_Urine_value`,
#   )
# 
# 
# # Print the selected columns
# print(selected_columns)
# # Print the selected columns
# print(selected_columns)
# 
# # Remove specified columns and rename one column
# person_with_summary <- person_with_summary %>%
#   select(-`Albumin/Creatinine [Mass Ratio] in Urine`, -`Microalbumin/Creatinine [Mass Ratio] in Urine`, `Albumin/Creatinine__in_Urine_value.y`) %>%
#   rename(`Albumin/Creatinine__in_Urine_value` = `Albumin/Creatinine__in_Urine_value.x`
# )
# 
# # Print the updated dataset to verify the changes
# print(colnames(person_with_summary))

```

Peritoneal dialysis.x and \`Peritoneal dialysis.y

```{r}
# # Filter rows where "Peritoneal dialysis.x" or "Peritoneal dialysis.y" are greater than 0
# filtered_rows <- person_with_summary %>%
#   filter(`Peritoneal dialysis.x` > 0 | `Peritoneal dialysis.y` > 0)
# 
# # Print the filtered rows with the specified columns
# print(filtered_rows[ , c("Peritoneal dialysis.x", "Peritoneal dialysis.y")])
# 
# # Combine the columns and create a new column
# person_with_summary <- person_with_summary %>%
#   mutate(`Peritoneal dialysis` = `Peritoneal dialysis.x` + `Peritoneal dialysis.y`) %>%
#   select(-`Peritoneal dialysis.x`, -`Peritoneal dialysis.y`)
# 
# colnames(person_with_summary)
```

### Censored Time

```{r}
# Step 1: Combine the relevant date columns from all summary tables
combined_dates <- bind_rows(
  summary_table1 %>% select(person_id, date = last_end_date_condition),
  summary_table2 %>% select(person_id, date = last_end_date_device),
  summary_table3 %>% select(person_id, date = last_end_date_drug),
  summary_table4 %>% select(person_id, date = last_measurement_date),
  summary_table5 %>% select(person_id, date = last_observation_date),
  summary_table6 %>% select(person_id, date = last_procedure_date)
)

# Step 2: Group by person_id and find the maximum date
censored_dates <- combined_dates %>%
  group_by(person_id) %>%
  summarise(censored_date = max(date, na.rm = TRUE))

# Step 3: Merge the censored dates back into person_with_summary
person_with_summary <- person_with_summary %>%
  left_join(censored_dates, by = "person_id")

# Print the resulting data frame
colnames(person_with_summary)
```

### Time to event

```{r}
# Calculate the time_to_event
person_with_summary <- person_with_summary %>%
  mutate(
    event_date = pmin(disease_date, censored_date, na.rm = TRUE),
    time_to_event = as.numeric(difftime(event_date, index_date, units = "days"))
  )

```

Time to event summary statistics

```{r}
# Summarize the time_to_event column and count the number of unique person_id who had the event
summary_time_to_event <- person_with_summary %>%
  filter(!is.na(time_to_event)) %>%
  summarize(
    unique_person_count = n_distinct(person_id),  # Count unique person_id
    count = n(),  # Total count of events
    mean_time = mean(time_to_event, na.rm = TRUE),  # Mean time to event
    median_time = median(time_to_event, na.rm = TRUE),  # Median time to event
    min_time = min(time_to_event, na.rm = TRUE),  # Minimum time to event
    max_time = max(time_to_event, na.rm = TRUE),  # Maximum time to event
    sd_time = sd(time_to_event, na.rm = TRUE),  # Standard deviation of time to event
    q1_time = quantile(time_to_event, 0.25, na.rm = TRUE),  # 1st quartile time to event
    q3_time = quantile(time_to_event, 0.75, na.rm = TRUE)  # 3rd quartile time to event
  )

# Reshape the summary data to long format
summary_table <- summary_time_to_event %>%
  pivot_longer(
    cols = -c(unique_person_count, count),
    names_to = "time_statistic",
    values_to = "time_value"
  )

# Create a final table combining person count and time statistics
final_table <- summary_table %>%
  select(time_statistic, time_value) %>%
  bind_rows(
    tibble(time_statistic = "unique_person_count", time_value = summary_time_to_event$unique_person_count),
    tibble(time_statistic = "total_event_count", time_value = summary_time_to_event$count)
  )

# Print the final table
print(final_table)

```

Converting time from days to months

```{r}
# # Convert time_to_event from days to months (approximate, using 30.44 days per month)
# person_with_summary <- person_with_summary %>%
#   mutate(time_to_event_months = time_to_event / 30.44)
# 
# # Create bins for the time_to_event_months column
# person_with_summary <- person_with_summary %>%
#   mutate(time_to_event_bin = cut(time_to_event_months, breaks = seq(0, max(time_to_event_months, na.rm = TRUE), by = 1), right = FALSE))
# 
# # Summarize the number of people in each bin
# summary_time_to_event <- person_with_summary %>%
#   group_by(time_to_event_bin) %>%
#   summarize(
#     unique_person_id_count = n_distinct(person_id),
#     count = n()
#   ) %>%
#   ungroup()
# 
# # Print the summary
# print(summary_time_to_event)

```

```{r}
# Count the number of entries with disease_status == 0
num_disease_status_0 <- person_with_summary %>%
  filter(disease_status == 0) %>%
  nrow()

# Count the number of entries with disease_status == 1
num_disease_status_1 <- person_with_summary %>%
  filter(disease_status == 1) %>%
  nrow()

# Print the counts
print(paste("Number of entries with disease_status == 0:", num_disease_status_0))
print(paste("Number of entries with disease_status == 1:", num_disease_status_1))

```

## 

# Table 1

```{r}
# Load necessary libraries
library(dplyr)
library(tidyr)

# Deduplicate the person_with_summary dataset by person_id
table1_data <- person_with_summary %>%
  distinct(person_id, .keep_all = TRUE)

# Combine the relevant PheCode columns for each condition into a single column
table1_data <- table1_data %>%
  mutate(
    hypertension = ifelse(`PheCode:401.3` == 1 | `PheCode:401.22` == 1 | `PheCode:401.2` == 1, 1, 0),
    `Chronic kidney disease` = ifelse(`PheCode:585.4` == 1 | `PheCode:585.33` == 1 | `PheCode:585.34` == 1 | `PheCode:585.32` == 1, 1, 0),
    `Congestive heart failure` = ifelse(`PheCode:428` == 1, 1, 0),
    `Peripheral vascular disease` = ifelse(`PheCode:443.9` == 1, 1, 0),
    `Acute renal failure syndrome` = ifelse(`PheCode:585.1` == 1, 1, 0),
    `Diabetes mellitus` = ifelse(`PheCode:250` == 1, 1, 0)
  )

# Create the table with case and control labels
create_baseline_table <- function(data) {
  numeric_summary <- data %>%
    group_by(disease_status) %>%
    summarise(
      `Black or African American` = sum(race == "Black or African American"),
      `White` = sum(race == "White"),
      `18-44` = sum(age_precise >= 18 & age_precise <= 44),
      `45-64` = sum(age_precise >= 45 & age_precise <= 64),
      `> 65` = sum(age_precise > 65),
      `≤30` = sum(eGFR_value <= 30, na.rm = TRUE),
      `31–60` = sum(eGFR_value > 30 & eGFR_value <= 60, na.rm = TRUE),
      `61–90` = sum(eGFR_value > 60 & eGFR_value <= 90, na.rm = TRUE),
      `>90` = sum(eGFR_value > 90, na.rm = TRUE),
      Female = sum(sex_at_birth == "Female"),
      Male = sum(sex_at_birth == "Male"),
      `Underweight (< 18.5)` = sum(BMI < 18.5, na.rm = TRUE),
      `Normal (18.5–24.9)` = sum(BMI >= 18.5 & BMI <= 24.9, na.rm = TRUE),
      `Overweight (25–29.9)` = sum(BMI >= 25 & BMI <= 29.9, na.rm = TRUE),
      `Obese (30+)` = sum(BMI >= 30, na.rm = TRUE),
      hypertension = sum(hypertension == 1),
      `Chronic kidney disease` = sum(`Chronic kidney disease` == 1),
      `Congestive heart failure` = sum(`Congestive heart failure` == 1),
      `Peripheral vascular disease` = sum(`Peripheral vascular disease` == 1),
      `Acute renal failure syndrome` = sum(`Acute renal failure syndrome` == 1),
      `Diabetes mellitus` = sum(`Diabetes mellitus` == 1),
      `Participants with Albumin/Creatinine` = sum(!is.na(`Albumin/Creatinine`))
    ) %>%
    pivot_longer(-disease_status, names_to = "Characteristics", values_to = "Counts") %>%
    mutate(Counts = as.character(Counts))  # Convert Counts to character
  
  character_summary <- data %>%
    group_by(disease_status) %>%
    summarise(
      `eGFR, median (IQR)` = round(median(eGFR_value, na.rm = TRUE), 2),
      `BMI, median (IQR)` = round(median(BMI, na.rm = TRUE), 2)
    ) %>%
    pivot_longer(-disease_status, names_to = "Characteristics", values_to = "Counts") %>%
    mutate(Counts = as.character(Counts))  # Convert Counts to character

  summary_table <- bind_rows(numeric_summary, character_summary) %>%
    pivot_wider(names_from = disease_status, values_from = Counts, names_prefix = "ESRD_") %>%
    mutate(
      Total = as.character(as.numeric(coalesce(ESRD_0, "0")) + as.numeric(coalesce(ESRD_1, "0"))),
      Control = paste0(ESRD_0, " (", round(as.numeric(ESRD_0) / as.numeric(Total) * 100, 1), "%)"),
      Case = paste0(ESRD_1, " (", round(as.numeric(ESRD_1) / as.numeric(Total) * 100, 1), "%)")
    ) %>%
    select(Characteristics, Case, Control, Total)
  
  return(summary_table)
}

# Create the table
baseline_table <- create_baseline_table(table1_data)

# Print the table
print(baseline_table)
```

## Spread of eGFR

```{r}
# Define a plausible range for eGFR values
plausible_min <- 0
plausible_max <- 500

# Filter out eGFR values greater than 500
out_of_range_eGFR <- person_with_summary %>%
  filter(eGFR_value > plausible_max) %>%
  select(person_id, eGFR_value, disease_status)

# Summarize the unique person_id count with disease_status == 1 or 0
eGFR_status_summary <- out_of_range_eGFR %>%
  group_by(eGFR_value, disease_status) %>%
  summarise(unique_person_count = n_distinct(person_id)) %>%
  ungroup()

# Get the top 5 maximum unique out-of-range eGFR values with disease_status
top_5_max_out_of_range_eGFR <- eGFR_status_summary %>%
  arrange(desc(eGFR_value)) %>%
  head(5)

# Function to format numbers without scientific notation
format_numbers <- function(df) {
  df %>%
    mutate(eGFR_value = format(eGFR_value, scientific = FALSE, trim = TRUE))
}

# Apply the function to format the numbers
top_5_max_out_of_range_eGFR <- format_numbers(top_5_max_out_of_range_eGFR)
eGFR_status_summary <- format_numbers(eGFR_status_summary)

# Print the top 5 maximum unique out-of-range eGFR values with disease_status
cat("\nTop 5 maximum out-of-range eGFR values:\n")
print(top_5_max_out_of_range_eGFR)

cat("\nSummary of unique person_id counts by disease_status for eGFR > 500:\n")
print(eGFR_status_summary)

```

Analysis Data sets

```{r}
survival_data <- person_with_summary
ml_data <- person_with_summary

data_reserve <- table1_data


# Save the dataframes to CSV files
write.csv(survival_data, "survival_data.csv", row.names = FALSE)
write.csv(ml_data, "ml_data.csv", row.names = FALSE)
write.csv(data_reserve, "data_reserve.csv", row.names = FALSE)

data <- read.csv("survival_data.csv")
colnames(data)
```

# Survival Analysis

Group by eGFR

All values

```{r}
# # Load necessary libraries
# #install.packages("survminer")
# library(survival)
# library(survminer)
# 
# # Define a new eGFR grouping that includes "> 120"
# person_with_summary <- person_with_summary %>%
#   mutate(
#     eGFR_group = case_when(
#       eGFR_value > 120 ~ ">120",
#       eGFR_value >= 90 & eGFR_value <= 120 ~ "90-120",
#       eGFR_value >= 60 & eGFR_value < 90 ~ "60-89",
#       eGFR_value >= 45 & eGFR_value < 60 ~ "45-59",
#       eGFR_value >= 30 & eGFR_value < 45 ~ "30-44",
#       eGFR_value >= 15 & eGFR_value < 30 ~ "15-29",
#       eGFR_value < 15 ~ "<15",
#       TRUE ~ NA_character_
#     )
#   )
# 
# # Step 1: Create the event variable
# person_with_summary <- person_with_summary %>%
#   mutate(
#     event = ifelse(is.na(disease_date), 0, 1)  # Assuming disease_date is NA if censored
#   )
# 
# # Step 1: Create a survival object for each eGFR group and fit the Kaplan-Meier model
# km_fit <- survfit(Surv(time_to_event, event) ~ eGFR_group, data = person_with_summary)
# 
# # Step 2: Plot the Kaplan-Meier curves for each eGFR group
# km_plot <- ggsurvplot(
#   km_fit,
#   data = person_with_summary,
#   pval = TRUE,
#   conf.int = FALSE,
#   risk.table = TRUE,
#   legend.title = "eGFR Group",
#   legend.labs = levels(person_with_summary$eGFR_group),
#   ggtheme = theme_minimal(),
#   xlab = "Time in Days",
#   ylab = "Survival Probability"
# )
# 
# # Print the Kaplan-Meier plot
# print(km_plot)
```

In range

```{r}
# # Filter out implausible eGFR values
# plausible_eGFR_min <- 0
# plausible_eGFR_max <- 120
# 
# person_with_summary_inrange_eGFR <- person_with_summary %>%
#   filter(eGFR_value >= plausible_eGFR_min & eGFR_value <= plausible_eGFR_max)
# 
# # Step 2: Create a survival object for each eGFR group and fit the Kaplan-Meier model
# km_fit <- survfit(Surv(time_to_event, event) ~ eGFR_group, data = person_with_summary_inrange_eGFR)
# 
# # Step 3: Plot the Kaplan-Meier curves for each eGFR group
# km_plot <- ggsurvplot(
#   km_fit,
#   data = person_with_summary_inrange_eGFR,
#   pval = TRUE,
#   conf.int = FALSE,
#   risk.table = TRUE,
#   legend.title = "eGFR Group",
#   legend.labs = levels(person_with_summary_inrange_eGFR$eGFR_group),
#   ggtheme = theme_minimal(),
#   xlab = "Time in Days",
#   ylab = "Survival Probability"
# )
# 
# # Print the Kaplan-Meier plot
# print(km_plot)
```

In range, first 5 years

```{r}

# # Step 1: Create a survival object for each eGFR group and fit the Kaplan-Meier model
# km_fit <- survfit(Surv(time_to_event, event) ~ eGFR_group, data = person_with_summary_inrange_eGFR)
# 
# # Step 2: Plot the Kaplan-Meier curves for each eGFR group without confidence intervals and limit to 1000 days
# km_plot <- ggsurvplot(
#   km_fit,
#   data = person_with_summary_inrange_eGFR,
#   pval = TRUE,
#   conf.int = FALSE,  # Remove confidence intervals
#   risk.table = TRUE,
#   legend.title = "eGFR Group",
#   legend.labs = levels(person_with_summary_inrange_eGFR$eGFR_group),
#   ggtheme = theme_minimal(),
#   xlab = "Time in Days",
#   ylab = "Survival Probability",
#   xlim = c(0, 1827),  # Limit the plot to the first 1000 days
#   break.time.by = 100  # Label time and number at risk every 100 days
# )
# 
# print(km_plot)
```

## Group by race

441 ESRD cases and 29,215 controls

```{r}
# Load necessary libraries
library(survival)
library(survminer)

# Step 1: Create a survival object for each race and fit the Kaplan-Meier model
km_fit <- survfit(Surv(time_to_event, disease_status) ~ race, data = table1_data)

# Step 2: Plot the Kaplan-Meier curves for each race without confidence intervals
km_plot <- ggsurvplot(
  km_fit,
  data = table1_data,
  pval = TRUE,
  conf.int = FALSE,  # Remove confidence intervals
  risk.table = TRUE,  # Include risk table
  legend.title = "Race",
  legend.labs = levels(person_with_summary$race),
  ggtheme = theme_minimal(),
  xlab = "Time in Days",
  ylab = "Survival Probability",
  size = 2,  # Increase line thickness for better visibility
  font.main = c(16, "bold"),  # Increase title font size and make it bold
  font.x = c(14),  # Increase x-axis label font size
  font.y = c(14),  # Increase y-axis label font size
  font.tickslab = c(12),  # Increase axis ticks label size
  risk.table.fontsize = 4  # Increase font size of the risk table
)

# Step 3: Save the Kaplan-Meier plot to a file with larger dimensions
ggsave("km_plot_race.png", plot = km_plot$plot, width = 14, height = 10)

# Step 4: Print the Kaplan-Meier plot to the R console
print(km_plot)

# Count the number of cases (disease_status == 1) and controls (disease_status == 0)
case_control_counts <- table1_data %>%
  group_by(disease_status) %>%
  summarise(count = n())

# Print the counts
print(case_control_counts)
```

## Group by race and eGFR:60

441 ESRD cases and 29,215 controls

```{r}
# Load necessary libraries
library(dplyr)
library(survival)
library(survminer)
library(RColorBrewer)

# Create eGFR groups
table1_data <- table1_data %>%
  mutate(
    eGFR_group = case_when(
      eGFR_value <= 60 ~ "eGFR < 60",
      eGFR_value > 60 ~ "eGFR >= 60"
    )
  )

# Ensure race and eGFR_group levels are in increasing order
person_with_summary_inrange_eGFR <- table1_data %>%
  mutate(
    race = factor(race, levels = sort(unique(race))),
    eGFR_group = factor(eGFR_group, levels = c("eGFR < 60", "eGFR >= 60")),
    group = interaction(race, eGFR_group)  # Create a combined group label for color and line type
  )

# Create a survival object for each combination of race and eGFR_group and fit the Kaplan-Meier model
km_fit <- survfit(Surv(time_to_event, disease_status) ~ group, data = person_with_summary_inrange_eGFR)

# Use a strong color palette for distinct, bold colors
num_groups <- length(levels(person_with_summary_inrange_eGFR$group))
custom_colors <- brewer.pal(n = min(8, num_groups), name = "Dark2")

# Ensure we have enough colors in the palette
if (num_groups > 8) {
  custom_colors <- colorRampPalette(brewer.pal(8, "Dark2"))(num_groups)
}

# Plot the Kaplan-Meier curves for each combination of race and eGFR_group without confidence intervals
km_plot <- ggsurvplot(
  km_fit,
  data = person_with_summary_inrange_eGFR,
  pval = TRUE,
  conf.int = FALSE,  # Remove confidence intervals
  risk.table = TRUE,  # Include risk table
  legend.title = "Race + eGFR Group",
  ggtheme = theme_minimal(),
  xlab = "Time in Days",
  ylab = "Survival Probability",
  palette = custom_colors,  # Apply the custom color palette
  size = 1.5  # Increase line thickness for bolder lines
)

# Adjust the plot to ensure only one legend is displayed
km_plot$plot <- km_plot$plot + theme(legend.position = "bottom") + guides(
  color = guide_legend(ncol = 1, byrow = TRUE),
  linetype = guide_legend(ncol = 1, byrow = TRUE)
)

# Set all lines to solid
km_plot$plot <- km_plot$plot + scale_linetype_manual(values = rep("solid", num_groups))

# Print the Kaplan-Meier plot
print(km_plot)

# Save the Kaplan-Meier plot to a file with increased size
ggsave("km_plot_race_eGFR60_group.png", plot = km_plot$plot, width = 14, height = 10)

# Count the number of cases (disease_status == 1) and controls (disease_status == 0)
case_control_counts <- person_with_summary_inrange_eGFR %>%
  group_by(disease_status) %>%
  summarise(count = n())

# Print the counts
print(case_control_counts)
```

## Group by race and eGFR:30

441 ESRD cases and 29,215 controls

```{r}
# Load necessary libraries
library(dplyr)
library(survival)
library(survminer)
library(RColorBrewer)

# Create eGFR groups based on the specified categories
person_with_summary <- person_with_summary %>%
  mutate(
    eGFR_group = case_when(
      eGFR_value < 30 ~ "<30",
      eGFR_value >= 30 & eGFR_value < 60 ~ "30-<60",
      eGFR_value >= 60 ~ ">=60"
    )
  )

# Ensure race and eGFR_group levels are in increasing order
person_with_summary_inrange_eGFR <- person_with_summary %>%
  mutate(
    race = factor(race, levels = sort(unique(race))),
    eGFR_group = factor(eGFR_group, levels = c("<30", "30-<60", ">=60")),
    group = interaction(race, eGFR_group)  # Create a combined group label for color and line type
  )

# Define custom colors manually for each group
custom_colors <- brewer.pal(n = 8, name = "Dark2")

# Create a survival object for each combination of race and eGFR_group and fit the Kaplan-Meier model
km_fit <- survfit(Surv(time_to_event, disease_status) ~ group, data = person_with_summary_inrange_eGFR)

# Plot the Kaplan-Meier curves for each combination of race and eGFR_group
km_plot <- ggsurvplot(
  km_fit,
  data = person_with_summary_inrange_eGFR,
  pval = TRUE,
  conf.int = FALSE,  # Remove confidence intervals
  risk.table = TRUE,  # Include risk table
  legend.title = "Race + eGFR Group",
  ggtheme = theme_minimal(),
  xlab = "Time in Days",
  ylab = "Survival Probability",
  palette = custom_colors,  # Apply the custom color palette
  size = 2  # Further increase line thickness for bolder lines
)

# Adjust the plot to ensure only one legend is displayed
km_plot$plot <- km_plot$plot + theme(legend.position = "bottom") + guides(
  color = guide_legend(ncol = 1, byrow = TRUE),
  linetype = guide_legend(ncol = 1, byrow = TRUE)
)

# Set all lines to solid
km_plot$plot <- km_plot$plot + scale_linetype_manual(values = rep("solid", length(levels(person_with_summary_inrange_eGFR$group))))

# Print the Kaplan-Meier plot
print(km_plot)

# Save the Kaplan-Meier plot to a file with larger dimensions
ggsave("km_plot_race_eGFR30_group.png", plot = km_plot$plot, width = 14, height = 8)

# Count the number of cases (disease_status == 1) and controls (disease_status == 0)
case_control_counts <- person_with_summary_inrange_eGFR %>%
  group_by(disease_status) %>%
  summarise(count = n())

# Print the counts
print(case_control_counts)
```

Count the number of eGFR values greater than 500

```{r}
# # Count the number of eGFR values greater than 500
# count_high_eGFR <- person_with_summary_inrange_eGFR %>%
#   filter(eGFR_value > 500) %>%
#   summarise(count = n())
# 
# # Print the result
# print(count_high_eGFR)

```

### by all groups

```{r}
# Load necessary libraries
library(dplyr)
library(survival)
library(survminer)
library(RColorBrewer)

# Group eGFR values as specified
person_with_summary <- person_with_summary %>%
  mutate(group = case_when(
    eGFR_value > 120 ~ ">120",
    eGFR_value >= 90 & eGFR_value <= 120 ~ "90-120",
    eGFR_value >= 60 & eGFR_value < 90 ~ "60-89",
    eGFR_value >= 45 & eGFR_value < 60 ~ "45-59",
    eGFR_value >= 30 & eGFR_value < 45 ~ "30-44",
    eGFR_value >= 15 & eGFR_value < 30 ~ "15-29",
    eGFR_value < 15 ~ "<15",
    TRUE ~ NA_character_  # Handle any unexpected values
  ))

# Ensure race and eGFR group levels are in increasing order
person_with_summary <- person_with_summary %>%
  mutate(
    race = factor(race, levels = sort(unique(race))),
    group = factor(group, levels = c("<15", "15-29", "30-44", "45-59", "60-89", "90-120", ">120")),
    combined_group = interaction(race, group)  # Create a combined group label
  )

# Create a survival object for each combination of race and eGFR group and fit the Kaplan-Meier model
km_fit <- survfit(Surv(time_to_event, disease_status) ~ combined_group, data = person_with_summary)

# Dynamically create a color palette based on the number of groups
num_groups <- length(levels(person_with_summary$combined_group))
custom_colors <- colorRampPalette(brewer.pal(8, "Dark2"))(num_groups)

# Plot the Kaplan-Meier curves for each combination of race and eGFR group
km_plot <- ggsurvplot(
  km_fit,
  data = person_with_summary,
  pval = TRUE,
  conf.int = FALSE,  # Remove confidence intervals
  risk.table = TRUE,  # Include risk table
  legend.title = "Race + eGFR Group",
  legend.labs = levels(person_with_summary$combined_group),  # Wrap labels if needed
  ggtheme = theme_minimal(),
  xlab = "Time in Days",
  ylab = "Survival Probability",
  palette = custom_colors,  # Apply the custom color palette
  size = 1.5  # Increase line thickness for better visibility
)

# Adjust the legend to avoid cluttering the plot
km_plot$plot <- km_plot$plot +
  theme(
    legend.position = "right",  # Position legend to the right
    legend.text = element_text(size = 10),
    legend.title = element_text(size = 12),
    legend.key.size = unit(1, "lines"),  # Increase the size of the legend keys
    plot.margin = margin(20, 20, 20, 20)  # Adjust plot margins to provide more space
  ) +
  guides(
    color = guide_legend(ncol = 1, byrow = TRUE),  # Arrange legend items vertically
    linetype = guide_legend(ncol = 1, byrow = TRUE)
  )

# Print the Kaplan-Meier plot
print(km_plot)

# Save the Kaplan-Meier plot to a file with increased dimensions
ggsave("km_plot_race_eGFRall_group.png", plot = km_plot$plot, width = 14, height = 10)

# Count the number of cases (disease_status == 1) and controls (disease_status == 0)
case_control_counts <- person_with_summary %>%
  group_by(disease_status) %>%
  summarise(count = n())

# Print the counts
print(case_control_counts)
```

First 5 years

```{r}
# # # Load necessary libraries
# # library(survival)
# # library(survminer)
# # library(RColorBrewer)
# 
# # Ensure race and eGFR_group levels are in increasing order
# person_with_summary_inrange_eGFR <- person_with_summary_inrange_eGFR %>%
#   mutate(
#     race = factor(race, levels = sort(unique(race))),
#     eGFR_group = factor(eGFR_group),
#     group = interaction(race, eGFR_group)  # Create a combined group label for color and line type
#   )
# 
# # Create a survival object for each combination of race and eGFR_group and fit the Kaplan-Meier model
# km_fit <- survfit(Surv(time_to_event, event) ~ group, data = person_with_summary_inrange_eGFR)
# 
# # Define custom colors using RColorBrewer
# num_groups <- length(levels(person_with_summary_inrange_eGFR$group))
# custom_colors <- brewer.pal(n = min(12, num_groups), name = "Set3")
# 
# # Plot the Kaplan-Meier curves for each combination of race and eGFR_group without confidence intervals
# km_plot <- ggsurvplot(
#   km_fit,
#   data = person_with_summary_inrange_eGFR,
#   pval = TRUE,
#   conf.int = FALSE,  # Remove confidence intervals
#   risk.table = TRUE,  # Include risk table
#   legend.title = "Race + eGFR Group",
#   ggtheme = theme_minimal(),
#   xlab = "Time in Days",
#   ylab = "Survival Probability",
#   palette = custom_colors  # Apply the custom color palette
# )
# 
# # Adjust the plot to ensure only one legend is displayed
# km_plot$plot <- km_plot$plot + theme(legend.position = "bottom") + guides(
#   color = guide_legend(ncol = 1, byrow = TRUE),
#   linetype = guide_legend(ncol = 1, byrow = TRUE)
# )
# 
# # Set all lines to solid
# km_plot$plot <- km_plot$plot + scale_linetype_manual(values = rep("solid", num_groups))
# 
# # Limit the x-axis to the first 1900 days
# km_plot$plot <- km_plot$plot + coord_cartesian(xlim = c(0, 1900))
# 
# # Print the Kaplan-Meier plot
# print(km_plot)

```

No grouping

```{r}
# # Step 1: Ensure time_to_event is in months and create the event variable
# person_with_summary <- person_with_summary %>%
#   mutate(
#     time_to_event_months = time_to_event / 30.44,
#     event = ifelse(is.na(disease_date), 0, 1)  # Assuming disease_date is NA if censored
#   )
# 
# # Step 2: Create a survival object
# surv_object <- Surv(time = person_with_summary$time_to_event_months, event = person_with_summary$event)
# 
# # Step 3: Fit the Kaplan-Meier model
# km_fit <- survfit(surv_object ~ 1)
# 
# # Step 4: Plot the Kaplan-Meier curve
# ggsurvplot(
#   km_fit,
#   data = person_with_summary,
#   xlab = "Time in Months",
#   ylab = "Survival Probability",
#   ggtheme = theme_minimal(),
#   conf.int = TRUE,  # Add confidence intervals
#   risk.table = TRUE  # Add risk table
# )

```

### KP Table

```{r}
# # Step 1: Create the event variable
# person_with_summary_inrange_eGFR <- person_with_summary_inrange_eGFR %>%
#   mutate(
#     event = ifelse(is.na(disease_date), 0, 1)  # Assuming disease_date is NA if censored
#   )
# 
# # Step 2: Create a survival object
# surv_object <- Surv(time = person_with_summary_inrange_eGFR$time_to_event, event = person_with_summary_inrange_eGFR$event)
# 
# # Step 3: Fit the Kaplan-Meier model
# km_fit <- survfit(surv_object ~ 1)
# 
# # Extract the Kaplan-Meier table
# km_table <- summary(km_fit)
# 
# # Create a data frame from the Kaplan-Meier table with time formatted to 4 decimal places
# km_data <- data.frame(
#   time = format(round(km_table$time, 4), nsmall = 4),
#   n_risk = km_table$n.risk,
#   n_event = km_table$n.event,
#   n_censor = km_table$n.censor,
#   survival = round(km_table$surv, 1),
#   std_error = round(km_table$std.err, 1),
#   lower_ci = round(km_table$lower, 1),
#   upper_ci = round(km_table$upper, 1)
# )
# 
# # Print the Kaplan-Meier table
# print(km_data)

```

Data Check

Renaming

```{r}
# # Step 1: Rename event_date to event_or_censored_date and disease_date to event_date
# person_with_summary <- person_with_summary %>%
#   rename(event_or_censored_date = event_date, event_date = disease_date, event= disease_status)
# 
# # Step 2: Create the event variable based on event_date
# person_with_summary <- person_with_summary %>%
#   mutate(
#     event = ifelse(is.na(event_date), 0, 1)  # Assuming event_date is NA if censored
#   )
# 
# # Step 3: Ensure race and eGFR_group levels are in increasing order
# person_with_summary <- person_with_summary %>%
#   mutate(race = factor(race, levels = sort(unique(race))),
#          eGFR_group = factor(eGFR_group, levels = sort(unique(eGFR_group))),
#          group = interaction(race, eGFR_group))  # Create a combined group label for color and line type
# 
# unique(person_with_summary$event)
```

Spread of eGFR

```{r}
# # Define a plausible range for eGFR values
# plausible_min <- 0
# plausible_max <- 120
# 
# # Identify all eGFR values that are out of the range 0 to 120
# out_of_range_eGFR <- person_with_summary %>%
#   filter(eGFR_value < plausible_min | eGFR_value > plausible_max) %>%
#   select(person_id, eGFR_value)
# 
# # Summarize the unique eGFR values and count the number of person_ids for each
# eGFR_summary <- out_of_range_eGFR %>%
#   group_by(eGFR_value) %>%
#   summarise(person_count = n()) %>%
#   ungroup()
# 
# # Get the top 5 minimum and top 5 maximum unique out-of-range eGFR values
# top_5_min_out_of_range_eGFR <- eGFR_summary %>%
#   arrange(eGFR_value) %>%
#   head(5)
# 
# top_5_max_out_of_range_eGFR <- eGFR_summary %>%
#   arrange(desc(eGFR_value)) %>%
#   head(5)
# 
# # Function to format numbers without scientific notation
# format_numbers <- function(df) {
#   df %>%
#     mutate(eGFR_value = format(eGFR_value, scientific = FALSE, trim = TRUE))
# }
# 
# # Apply the function to format the numbers
# top_5_min_out_of_range_eGFR <- format_numbers(top_5_min_out_of_range_eGFR)
# top_5_max_out_of_range_eGFR <- format_numbers(top_5_max_out_of_range_eGFR)
# out_of_range_eGFR <- format_numbers(eGFR_summary)
# 
# 
# # Print the top 5 minimum and top 5 maximum unique out-of-range eGFR values
# cat("Top 5 minimum out-of-range eGFR values:\n")
# print(top_5_min_out_of_range_eGFR)
# 
# cat("\nTop 5 maximum out-of-range eGFR values:\n")
# print(top_5_max_out_of_range_eGFR)
# 
# print(out_of_range_eGFR)
```

```{r}
# # Identify all eGFR values that are out of the range 0 to 120
# out_of_range_eGFR <- person_with_summary %>%
#   filter(eGFR_value < plausible_eGFR_min | eGFR_value > plausible_eGFR_max)
# 
# # Count the number of out-of-range eGFR values
# num_out_of_range_eGFR <- nrow(out_of_range_eGFR)
# 
# # Print the count of out-of-range eGFR values
# print(paste("Number of eGFR values out of the range 0 to 120:", num_out_of_range_eGFR))
# 
# # Count the number of NA eGFR values
# num_na_eGFR <- person_with_summary %>%
#   filter(is.na(eGFR_value)) %>%
#   nrow()
# 
# # Print the count of NA eGFR values
# print(paste("Number of NA eGFR values:", num_na_eGFR))

```

Scatter plot eGFR

```{r}
# # Filter the data to include only eGFR values below 500
# filtered_data <- person_with_summary %>%
#   filter(eGFR_value < 500)
# 
# # Create the ggplot
# p <- ggplot(filtered_data, aes(x = person_id, y = eGFR_value)) +
#   geom_point(alpha = 0.5) +
#   labs(title = "Scatter Plot of eGFR Values Below 500",
#        x = "Person ID",
#        y = "eGFR Value") +
#   theme_minimal()
# 
# # Print the plot
# print(p)

```

```{r}
# #library(ggplot2)
# #library(plotly)
# 
# # Filter the dataset for eGFR values below 500
# filtered_data <- person_with_summary %>%
#   filter(eGFR_value < 500)
# 
# # Create the ggplot histogram for eGFR values below 500
# p <- ggplot(filtered_data, aes(x = eGFR_value)) +
#   geom_histogram(bins = 50, fill = "blue", alpha = 0.7, color = "black") +
#   labs(title = "Histogram of eGFR Values Below 500",
#        x = "eGFR Value",
#        y = "Count") +
#   theme_minimal()
# 
# # Convert to interactive plotly plot
# interactive_histogram <- ggplotly(p)
# 
# # Print the interactive plot
# interactive_histogram

```

```{r}
# #library(ggplot2)
# 
# # Filter the dataset for eGFR values below 500
# filtered_data <- person_with_summary %>%
#   filter(eGFR_value < 500)
# 
# # Create the ggplot scatter plot for eGFR values below 500 and age
# ggplot(filtered_data, aes(x = age_reported, y = eGFR_value)) +
#   geom_point(alpha = 0.5, color = "blue") +
#   labs(title = "Scatter Plot of eGFR Values Below 500 and Age",
#        x = "Age",
#        y = "eGFR Value") +
#   theme_minimal()

```

```{r}
# # Filter the dataset for eGFR values below 120
# filtered_data <- person_with_summary %>%
#   filter(eGFR_value < 120)
# 
# # Create the scatter plot
# ggplot(filtered_data, aes(x = age_reported, y = eGFR_value)) +
#   geom_point(alpha = 0.5) +
#   labs(title = "Scatter Plot of eGFR Values Below 120 and Age",
#        x = "Age",
#        y = "eGFR Value") +
#   theme_minimal()

```

eGFR data: In range eGFR data

```{r}
# # Count the number of records with disease_status == 0 and disease_status == 1
# disease_status_count <- person_with_summary %>%
#   group_by(disease_status) %>%
#   summarise(count = n())
# 
# # Print the counts
# print(disease_status_count)

```

# Read-in data

```{r}
library(knitr)
library(dplyr)
library(tidyr)
library(lubridate)
library(purrr)
library(stringr)

#install.packages("survival")
#install.packages("survminer")

# Load the necessary libraries
library(survival)

# install.packages("RColorBrewer")
library(RColorBrewer)

# Install and load necessary packages
#install.packages("caret")
#install.packages("ggplot2")
library(caret)
library(ggplot2)

# Load necessary libraries
#library(pROC)

person_with_summary <- read.csv("survival_data.csv")
# colnames(person_with_summary)
```

# Range eGFR data

```{r}
disease_status_count <- person_with_summary %>%
  group_by(disease_status) %>%
  summarise(count = n())

# Print the counts
print(disease_status_count)

```

```{r}

# Define the range for eGFR values
plausible_eGFR_min <- 0
plausible_eGFR_max <- 500

# Filter the dataset to get only the rows with eGFR values outside the range
out_of_range_eGFR <- person_with_summary %>%
  filter(eGFR_value < plausible_eGFR_min | eGFR_value > plausible_eGFR_max)

# Count the number of times each out-of-range eGFR value occurs for both cases and controls
eGFR_out_of_range_count <- out_of_range_eGFR %>%
  group_by(eGFR_value, disease_status) %>%
  summarise(count = n()) %>%
  ungroup() %>%
  pivot_wider(names_from = disease_status, values_from = count, values_fill = 0) %>%
  rename(Controls = `0`, Cases = `1`)

# Convert the eGFR values and counts to non-scientific notation
eGFR_out_of_range_count <- eGFR_out_of_range_count %>%
  mutate(
    eGFR_value = format(eGFR_value, scientific = FALSE, trim = TRUE),
    Controls = format(Controls, scientific = FALSE, trim = TRUE),
    Cases = format(Cases, scientific = FALSE, trim = TRUE)
  )

# Count the number of NA values for both cases and controls
na_count <- person_with_summary %>%
  filter(is.na(eGFR_value)) %>%
  group_by(disease_status) %>%
  summarise(NA_count = n()) %>%
  ungroup() %>%
  pivot_wider(names_from = disease_status, values_from = NA_count, values_fill = 0) %>%
  rename(NA_Controls = `0`, NA_Cases = `1`)

# Print the out-of-range eGFR values and their counts
print("Out-of-range eGFR values and their counts (in whole numbers):")
print(eGFR_out_of_range_count)

# Print the NA counts for both cases and controls
print("Number of NA values for eGFR by disease status:")
print(na_count)

```

# 

## Multiple imputation

```{r}
# # Load the necessary library
# library(mice)
# 
# # Assuming numeric_data_filtered contains the person_id column
# # If it doesn't, make sure to add it back before proceeding
# 
# # Make a copy of the original data
# original_data <- numeric_data_filtered
# 
# # Replace values greater than 500 with NA for imputation
# numeric_data_filtered$eGFR_value <- ifelse(numeric_data_filtered$eGFR_value > 500, NA, numeric_data_filtered$eGFR_value)
# 
# # Perform multiple imputation using the mice package
# imputed_data <- mice(numeric_data_filtered, m = 5, method = 'pmm', seed = 123)
# 
# # Complete the dataset with the imputed values (you can select any of the 20 imputations, here we select the first one)
# completed_data <- complete(imputed_data, 1)
# 
# # Identify the indices of the values that were imputed
# imputed_indices <- which(is.na(original_data$eGFR_value) | original_data$eGFR_value > 500)
# 
# # Extract original and imputed values for comparison
# imputed_values <- data.frame(
#   person_id = original_data$person_id[imputed_indices],
#   original_value = original_data$eGFR_value[imputed_indices],
#   imputed_value = completed_data$eGFR_value[imputed_indices]
# )
# 
# # Print the original and imputed values
# print("Original and Imputed Values:")
# print(imputed_values)

```

# Selecting In range eGFR data

```{r}
# Filter the dataset to keep only the rows with eGFR values within the defined range
inrange_person_with_summary <- person_with_summary %>%
  filter(eGFR_value >= plausible_eGFR_min & eGFR_value <= plausible_eGFR_max)

# Print the filtered dataset (optional)
print("Filtered dataset with eGFR values within the range 0 to 500:")
print(inrange_person_with_summary)

# Count the number of cases and controls remaining after filtering
disease_status_count <- inrange_person_with_summary %>%
  group_by(disease_status) %>%
  summarise(count = n())

# Print the counts of cases and controls
print("Number of cases and controls after filtering eGFR values within the range 0 to 500:")
print(disease_status_count)

```

### Select only sex_at_birth== Male or Female

```{r}
# Filter the person_with_summary dataset
inrange_person_with_summary <- inrange_person_with_summary %>%
  filter(sex_at_birth %in% c("Female", "Male"))
dim(inrange_person_with_summary)

# Count the number of records with disease_status == 0 and disease_status == 1
disease_status_count <- inrange_person_with_summary %>%
  group_by(disease_status) %>%
  summarise(count = n())

# Print the counts
print(disease_status_count)

```

## Variable Check

```{r}
# colnames(inrange_person_with_summary)
# # Load necessary library
# library(dplyr)
# 
# # Get the min and max of "Albumin.Creatinine" and "eGFR_value"
# albumin_creatinine_min <- min(inrange_person_with_summary$Albumin.Creatinine, na.rm = TRUE)
# albumin_creatinine_max <- max(inrange_person_with_summary$Albumin.Creatinine, na.rm = TRUE)
# 
# egfr_value_min <- min(inrange_person_with_summary$eGFR_value, na.rm = TRUE)
# egfr_value_max <- max(inrange_person_with_summary$eGFR_value, na.rm = TRUE)
# 
# # Get the unique values of "sex_at_birth"
# unique_sex_at_birth <- unique(inrange_person_with_summary$sex_at_birth)
# 
# # Print the results
# cat("Min and Max of Albumin.Creatinine:\n")
# cat("Min:", albumin_creatinine_min, "Max:", albumin_creatinine_max, "\n\n")
# 
# cat("Min and Max of eGFR_value:\n")
# cat("Min:", egfr_value_min, "Max:", egfr_value_max, "\n\n")
# 
# cat("Unique values of sex_at_birth:\n")
# print(unique_sex_at_birth)
# 
# 
# # Create a summary table for non-missing values in each specified variable
# non_na_counts <- inrange_person_with_summary %>%
#   summarise(
#     Albumin_Creatinine = n_distinct(person_id[!is.na(Albumin.Creatinine)]),
#     Albumin_Creatinine_Urine = n_distinct(person_id[!is.na(Albumin.Creatinine__in_Urine_value)]),
#     Microalbumin_Creatinine_Urine = n_distinct(person_id[!is.na(Microalbumin.Creatinine__in_Urine_value)]),
#     Albumin_Creatinine_24_hour_Urine = n_distinct(person_id[!is.na(Albumin.Creatinine__in_24_hour_Urine_value)])
#   )
# 
# # Print the summary table
# print("Number of people with non-missing values in each variable:")
# print(non_na_counts)

```

```{r}
# , Albumin.Creatinine__in_Urine_value, Microalbumin.Creatinine__in_Urine_value

# Count the number of cases and controls with non-NA "Albumin.Creatinine" values
albumin_creatinine_counts <- inrange_person_with_summary %>%
  filter(!is.na(Albumin.Creatinine)) %>%  # Filter out rows with NA values in "Albumin.Creatinine"
  group_by(disease_status) %>%  # Group by case (1) and control (0)
  summarise(count = n())  # Count the number of observations in each group

# Print the results
cat("Number of cases and controls with Albumin.Creatinine values:\n")
print(albumin_creatinine_counts)

```

```{r}
# Filter the dataset for person_id with Albumin.Creatinine value greater than 700 and count by disease_status
disease_status_above_700 <- inrange_person_with_summary %>%
  filter(Albumin.Creatinine >= 8000) %>%  # Filter rows where Albumin.Creatinine is greater than 700
  group_by(disease_status) %>%  # Group by disease_status
  summarise(person_count = n_distinct(person_id))  # Count the number of unique person_id

# Print the result
cat("Number of person_id with Albumin.Creatinine values greater than 700 by disease_status:\n")
print(disease_status_above_700)

```

```{r}
disease_status_above_8000 <- inrange_person_with_summary %>%
  filter(Albumin.Creatinine >= 8000) %>%  # Filter rows where Albumin.Creatinine is greater than or equal to 8000
  group_by(Albumin.Creatinine, disease_status) %>%  # Group by Albumin.Creatinine and disease_status
  summarise(person_count = n_distinct(person_id))  # Count the number of unique person_id

# Print the result
cat("Number of person_id with Albumin.Creatinine values greater than or equal to 8000 by disease_status:\n")
print(disease_status_above_8000)
```

# Part 1: Evaluation of the Kidney Failure Risk Equation (KFRE) with ALL OF US DATA: North American Equation
```{r}
# Ensure dplyr is loaded
if (!require("dplyr")) install.packages("dplyr", dependencies = TRUE)
library(dplyr)

inrange_person_with_summary <- read.csv("survival_data.csv")


# Group by the 'time_to_event' column and count the occurrences of each unique value
time_to_event_counts <- inrange_person_with_summary %>%
  group_by(time_to_event) %>%
  summarise(count = n()) %>%
  arrange(time_to_event)

# Print the results
print(time_to_event_counts)

summary(inrange_person_with_summary$time_to_event)

```

# MDRD
## < 60: CKD Population

```{r}
# Load necessary libraries
library(dplyr)
library(survival)

# Load dataset
inrange_person_with_summary <- read.csv("survival_data.csv")

# Define a plausible range for eGFR and Albumin/Creatinine based on clinical thresholds
plausible_eGFR_min <- 0
plausible_eGFR_max <- 60  # Kidney disease defined as eGFR < 60 mL/min/1.73 m²

plausible_acr_min <- 0
plausible_acr_max <- 8000  # Keeping max at 8000, but focus on albuminuria cutoff
albuminuria_cutoff <- 30  # Albumin/Creatinine ratio (ACR) > 30 mg/g defines persistent albuminuria

# Step 1: Select relevant columns and rename Albumin_Creatinine for consistency
person_filtered_KFRE <- inrange_person_with_summary %>%
  select(
    person_id,
    sex_at_birth,
    eGFR_value,
    Albumin_Creatinine = `Albumin.Creatinine`,
    age_precise,
    race,
    disease_status,
    time_to_event
  )

# Step 2: Data preprocessing and filtering based on plausible values for eGFR and ACR
person_filtered_KFRE <- person_filtered_KFRE %>%
  mutate(
    event = disease_status
  ) %>%
  # Filter based on eGFR < 60 mL/min/1.73 m² and ACR > 30 mg/g for albuminuria
  filter(
    eGFR_value >= plausible_eGFR_min & eGFR_value < plausible_eGFR_max,  # eGFR threshold
    Albumin_Creatinine >= albuminuria_cutoff & Albumin_Creatinine <= plausible_acr_max,  # Albuminuria cutoff
    sex_at_birth %in% c("Female", "Male")  # Filter for specified sexes
  ) %>%
  # Log-transform Albumin/Creatinine ratio (ACR)
  mutate(logACR = log(Albumin_Creatinine)) %>%
  # Ensure complete cases (remove rows with missing data)
  filter(complete.cases(.))

# Step 3: Verify summary statistics for eGFR and Albumin/Creatinine
summary(person_filtered_KFRE$eGFR_value)
summary(person_filtered_KFRE$Albumin_Creatinine)

# Check how many records are retained after filtering
cat("Number of records after filtering:", nrow(person_filtered_KFRE), "\n")

# Perform a complete case analysis
initial_row_count <- nrow(person_filtered_KFRE)

complete_cases <- person_filtered_KFRE %>%
  filter(complete.cases(.))

final_row_count <- nrow(complete_cases)

# Check if any rows have been removed
rows_removed <- initial_row_count - final_row_count

if (rows_removed > 0) {
  cat(rows_removed, "rows have been removed during the complete case analysis.\n")
} else {
  cat("No rows have been removed during the complete case analysis.\n")
}

# Report the number of complete cases
num_complete_cases <- nrow(complete_cases)
cat("Number of complete cases:", num_complete_cases, "\n")

# Remove any rows with NA in time_to_event or disease_status
complete_cases <- complete_cases %>%
  filter(!is.na(time_to_event) & !is.na(disease_status))

# Check the event distribution
cat("Event distribution (disease_status):\n")
print(table(complete_cases$disease_status))

# Step 4: Define the formula for calculating βsum (log hazard)
calculate_beta_sum <- function(age, sex, eGFR, logACR) {
  (-0.2201 * (age / 10 - 7.036)) +
  (0.2467 * (ifelse(sex == "Male", 1, 0) - 0.5642)) -
  (0.5567 * (eGFR / 5 - 7.222)) +
  (0.4510 * (logACR - 5.137))
}

# Apply the formula to calculate βsum
complete_cases <- complete_cases %>%
  mutate(beta_sum = calculate_beta_sum(age_precise, sex_at_birth, eGFR_value, logACR))

# Remove any rows with infinite or NA beta_sum
complete_cases <- complete_cases %>%
  filter(is.finite(beta_sum) & !is.na(beta_sum))

# Step 5: Calculate the risk for two and five years using the updated formulas
complete_cases <- complete_cases %>%
  mutate(
    risk_2_years = 100 * (1 - 0.975^exp(beta_sum)),  # Two-Year Risk calculation
    risk_5_years = 100 * (1 - 0.9240^exp(beta_sum))  # Five-Year Risk calculation
  )

# Save the data before filtering
before_filtering <- complete_cases

# Step 6: Apply the filtering to create consistent_data
consistent_data <- complete_cases %>%
  filter(!is.na(risk_5_years) & !is.na(risk_2_years))

# Step 7: Define and create the 2-year and 5-year outcomes based on disease_status and time_to_event
consistent_data <- consistent_data %>%
  mutate(
    outcome_2_years = ifelse(disease_status == 1 & time_to_event <= 2 * 365.25, 1, 0),
    outcome_5_years = ifelse(disease_status == 1 & time_to_event <= 5 * 365.25, 1, 0)
  )

# Find the rows that were removed by comparing before and after datasets
removed_rows <- anti_join(before_filtering, consistent_data, by = "person_id")

# Print the removed rows
if (nrow(removed_rows) > 0) {
  cat("Rows that were removed during filtering:\n")
  print(removed_rows)
} else {
  cat("No rows were removed during filtering.\n")
}

# Step 8: Verify the risk values
summary(consistent_data$risk_5_years)
summary(consistent_data$risk_2_years)

# Count the number of person_id entries with event == 1 and event == 0
event_count <- consistent_data %>%
  group_by(event) %>%
  summarise(count = n_distinct(person_id))

# Print the results
cat("Number of person_id with event == 1:", event_count$count[event_count$event == 1], "\n")
cat("Number of person_id with event == 0:", event_count$count[event_count$event == 0], "\n")

# Check the event distribution for the 2-year and 5-year outcomes
cat("\n2-Year Outcome Distribution:\n")
print(table(consistent_data$outcome_2_years))

cat("\n5-Year Outcome Distribution:\n")
print(table(consistent_data$outcome_5_years))


```
#### Model evaluation: one full sample
```{r}
# Load necessary libraries
library(dplyr)
library(pROC)
library(rms)  # For Harrell's C-statistic
library(Hmisc)  # For Harrell's C-statistic with confidence intervals
library(kableExtra)

# Function to calculate metrics, including AUC and its confidence intervals, and Harrell's C-statistic with CI
calculate_metrics <- function(data, risk_col, outcome_col, time_col) {
  
  # Filter out rows with missing or invalid data
  data_filtered <- data %>% filter(!is.na(data[[outcome_col]]) & !is.na(data[[risk_col]]))
  
  # Ensure outcome_col is binary (0/1)
  data_filtered[[outcome_col]] <- ifelse(data_filtered[[outcome_col]] > 0, 1, 0)
  
  # Check if data_filtered has rows after filtering
  if (nrow(data_filtered) == 0) {
    stop("Error: No valid data after filtering. Check the columns for missing values or incorrect column names.")
  }

  # Calculate ROC and AUC with confidence intervals
  roc_curve <- roc(data_filtered[[outcome_col]], data_filtered[[risk_col]])
  auc_value <- as.numeric(auc(roc_curve))  # AUC
  auc_ci <- ci.auc(roc_curve)  # Calculate 95% CI for AUC
  
  # Calculate Harrell's C-statistic using Cox proportional hazards model with confidence intervals
  cox_model <- coxph(Surv(data_filtered[[time_col]], data_filtered[[outcome_col]]) ~ data_filtered[[risk_col]], data = data_filtered)
  c_stat <- as.numeric(summary(cox_model)$concordance[1])  # C-index (Harrell's C-statistic)

  # Use rcorr.cens from Hmisc to get C-statistic confidence intervals
  c_stat_ci <- rcorr.cens(data_filtered[[risk_col]], Surv(data_filtered[[time_col]], data_filtered[[outcome_col]]))
  
  # Extract lower and upper CI for C-statistic
  c_stat_lower <- as.numeric(c_stat_ci['Dxy'][3]) / 2 + 0.5  # Lower CI
  c_stat_upper <- as.numeric(c_stat_ci['Dxy'][4]) / 2 + 0.5  # Upper CI

  # Calculate confusion matrix statistics: Sensitivity, Specificity, PPV, NPV
  pred_class <- ifelse(data_filtered[[risk_col]] > 0.5, 1, 0)
  confusion_mat <- table(Predicted = pred_class, Actual = data_filtered[[outcome_col]])

  if (nrow(confusion_mat) < 2 || ncol(confusion_mat) < 2) {
    sensitivity <- NA
    specificity <- NA
    ppv <- NA
    npv <- NA
  } else {
    sensitivity <- confusion_mat[2, 2] / sum(confusion_mat[, 2])
    specificity <- confusion_mat[1, 1] / sum(confusion_mat[, 1])
    ppv <- confusion_mat[2, 2] / sum(confusion_mat[2, ])
    npv <- confusion_mat[1, 1] / sum(confusion_mat[1, ])
  }

  return(list(
    AUC = auc_value,
    AUC_CI_Lower = auc_ci[1],
    AUC_CI_Upper = auc_ci[3],
    C_Statistic = c_stat,
    C_Statistic_CI_Lower = c_stat_lower,
    C_Statistic_CI_Upper = c_stat_upper,
    Sensitivity = sensitivity,
    Specificity = specificity,
    PPV = ppv,
    NPV = npv
  ))
}

# Function to summarize metrics for each racial group
summarize_risk_metrics <- function(data, race_label) {
  # Calculate metrics for 2-year and 5-year outcomes
  metrics_2_years <- calculate_metrics(data, "risk_2_years", "outcome_2_years", "time_to_event")
  metrics_5_years <- calculate_metrics(data, "risk_5_years", "outcome_5_years", "time_to_event")

  case_control_2yr <- table(data$outcome_2_years)
  case_control_5yr <- table(data$outcome_5_years)

  # Return the metrics as a data frame
  return(data.frame(
    Race = race_label,
    `AUC 2 Years` = metrics_2_years$AUC,
    `AUC 2 Years CI Lower` = metrics_2_years$AUC_CI_Lower,
    `AUC 2 Years CI Upper` = metrics_2_years$AUC_CI_Upper,
    `Harrell's C-Statistic 2 Years` = metrics_2_years$C_Statistic,
    `Harrell's C-Statistic 2 Years CI Lower` = metrics_2_years$C_Statistic_CI_Lower,
    `Harrell's C-Statistic 2 Years CI Upper` = metrics_2_years$C_Statistic_CI_Upper,
    `Sensitivity 2 Years` = metrics_2_years$Sensitivity,
    `Specificity 2 Years` = metrics_2_years$Specificity,
    `PPV 2 Years` = metrics_2_years$PPV,
    `NPV 2 Years` = metrics_2_years$NPV,
    `Cases 2 Years` = case_control_2yr["1"],
    `Controls 2 Years` = case_control_2yr["0"],
    `AUC 5 Years` = metrics_5_years$AUC,
    `AUC 5 Years CI Lower` = metrics_5_years$AUC_CI_Lower,
    `AUC 5 Years CI Upper` = metrics_5_years$AUC_CI_Upper,
    `Harrell's C-Statistic 5 Years` = metrics_5_years$C_Statistic,
    `Harrell's C-Statistic 5 Years CI Lower` = metrics_5_years$C_Statistic_CI_Lower,
    `Harrell's C-Statistic 5 Years CI Upper` = metrics_5_years$C_Statistic_CI_Upper,
    `Sensitivity 5 Years` = metrics_5_years$Sensitivity,
    `Specificity 5 Years` = metrics_5_years$Specificity,
    `PPV 5 Years` = metrics_5_years$PPV,
    `NPV 5 Years` = metrics_5_years$NPV,
    `Cases 5 Years` = case_control_5yr["1"],
    `Controls 5 Years` = case_control_5yr["0"]
  ))
}

# Evaluate for all races
results_all <- summarize_risk_metrics(consistent_data, "All Races")

# Evaluate for Black or African American
results_black <- summarize_risk_metrics(
  consistent_data %>% filter(race == "Black or African American"),
  "Black or African American"
)

# Evaluate for White
results_white <- summarize_risk_metrics(
  consistent_data %>% filter(race == "White"),
  "White"
)

# Combine all results into one table
final_results <- bind_rows(results_all, results_black, results_white)

# Print the combined results
cat("\nCombined Risk Metrics, AUC (with CI), Harrell's C-Statistic (with CI), Sensitivity, Specificity, and Case-Control Counts:\n")
print(final_results)

# Save the final results to a CSV file
write.csv(final_results, "combined_risk_metrics_with_case_control_counts_and_CI.csv", row.names = FALSE)

```



# 0-500: Wide eGFR range
```{r}
# Load necessary libraries
library(dplyr)
library(survival)

# Load dataset
inrange_person_with_summary <- read.csv("survival_data.csv")

# Define a plausible range for eGFR and Albumin/Creatinine based on clinical thresholds
plausible_eGFR_min <- 0
plausible_eGFR_max <- 200  # Kidney disease defined as eGFR < 60 mL/min/1.73 m²

plausible_acr_min <- 0
plausible_acr_max <- 8000  # Keeping max at 8000, but focus on albuminuria cutoff
albuminuria_cutoff <- 0  # Albumin/Creatinine ratio (ACR) > 30 mg/g defines persistent albuminuria

# Step 1: Select relevant columns and rename Albumin_Creatinine for consistency
person_filtered_KFRE <- inrange_person_with_summary %>%
  select(
    person_id,
    sex_at_birth,
    eGFR_value,
    Albumin_Creatinine = `Albumin.Creatinine`,
    age_precise,
    race,
    disease_status,
    time_to_event
  )

# Step 2: Data preprocessing and filtering based on plausible values for eGFR and ACR
person_filtered_KFRE <- person_filtered_KFRE %>%
  mutate(
    event = disease_status
  ) %>%
  # Filter based on eGFR < 60 mL/min/1.73 m² and ACR > 30 mg/g for albuminuria
  filter(
    eGFR_value >= plausible_eGFR_min & eGFR_value <= plausible_eGFR_max,  # eGFR threshold
    Albumin_Creatinine > albuminuria_cutoff & Albumin_Creatinine <= plausible_acr_max,  # Albuminuria cutoff
    sex_at_birth %in% c("Female", "Male")  # Filter for specified sexes
  ) %>%
  # Log-transform Albumin/Creatinine ratio (ACR)
  mutate(logACR = log(Albumin_Creatinine)) %>%
  # Ensure complete cases (remove rows with missing data)
  filter(complete.cases(.))

# Step 3: Verify summary statistics for eGFR and Albumin/Creatinine
summary(person_filtered_KFRE$eGFR_value)
summary(person_filtered_KFRE$Albumin_Creatinine)

# Check how many records are retained after filtering
cat("Number of records after filtering:", nrow(person_filtered_KFRE), "\n")

# Perform a complete case analysis
initial_row_count <- nrow(person_filtered_KFRE)

complete_cases <- person_filtered_KFRE %>%
  filter(complete.cases(.))

final_row_count <- nrow(complete_cases)

# Check if any rows have been removed
rows_removed <- initial_row_count - final_row_count

if (rows_removed > 0) {
  cat(rows_removed, "rows have been removed during the complete case analysis.\n")
} else {
  cat("No rows have been removed during the complete case analysis.\n")
}

# Report the number of complete cases
num_complete_cases <- nrow(complete_cases)
cat("Number of complete cases:", num_complete_cases, "\n")

# Remove any rows with NA in time_to_event or disease_status
complete_cases <- complete_cases %>%
  filter(!is.na(time_to_event) & !is.na(disease_status))

# Check the event distribution
cat("Event distribution (disease_status):\n")
print(table(complete_cases$disease_status))

# Step 4: Define the formula for calculating βsum (log hazard)
calculate_beta_sum <- function(age, sex, eGFR, logACR) {
  (-0.2201 * (age / 10 - 7.036)) +
  (0.2467 * (ifelse(sex == "Male", 1, 0) - 0.5642)) -
  (0.5567 * (eGFR / 5 - 7.222)) +
  (0.4510 * (logACR - 5.137))
}

# Apply the formula to calculate βsum
complete_cases <- complete_cases %>%
  mutate(beta_sum = calculate_beta_sum(age_precise, sex_at_birth, eGFR_value, logACR))

# Remove any rows with infinite or NA beta_sum
complete_cases <- complete_cases %>%
  filter(is.finite(beta_sum) & !is.na(beta_sum))

# Step 5: Calculate the risk for two and five years using the updated formulas
complete_cases <- complete_cases %>%
  mutate(
    risk_2_years = 100 * (1 - 0.975^exp(beta_sum)),  # Two-Year Risk calculation
    risk_5_years = 100 * (1 - 0.9240^exp(beta_sum))  # Five-Year Risk calculation
  )

# Save the data before filtering
before_filtering <- complete_cases

# Step 6: Apply the filtering to create consistent_data
consistent_data <- complete_cases %>%
  filter(!is.na(risk_5_years) & !is.na(risk_2_years))

# Step 7: Define and create the 2-year and 5-year outcomes based on disease_status and time_to_event
consistent_data <- consistent_data %>%
  mutate(
    outcome_2_years = ifelse(disease_status == 1 & time_to_event <= 2 * 365.25, 1, 0),
    outcome_5_years = ifelse(disease_status == 1 & time_to_event <= 5 * 365.25, 1, 0)
  )

# Find the rows that were removed by comparing before and after datasets
removed_rows <- anti_join(before_filtering, consistent_data, by = "person_id")

# Print the removed rows
if (nrow(removed_rows) > 0) {
  cat("Rows that were removed during filtering:\n")
  print(removed_rows)
} else {
  cat("No rows were removed during filtering.\n")
}

# Step 8: Verify the risk values
summary(consistent_data$risk_5_years)
summary(consistent_data$risk_2_years)

# Count the number of person_id entries with event == 1 and event == 0
event_count <- consistent_data %>%
  group_by(event) %>%
  summarise(count = n_distinct(person_id))

# Print the results
cat("Number of person_id with event == 1:", event_count$count[event_count$event == 1], "\n")
cat("Number of person_id with event == 0:", event_count$count[event_count$event == 0], "\n")

# Check the event distribution for the 2-year and 5-year outcomes
cat("\n2-Year Outcome Distribution:\n")
print(table(consistent_data$outcome_2_years))

cat("\n5-Year Outcome Distribution:\n")
print(table(consistent_data$outcome_5_years))

```

### Model Evaluation: one full sample
```{r}
# Load necessary libraries
library(dplyr)
library(pROC)
library(rms)  # For Harrell's C-statistic
library(kableExtra)

# Function to calculate metrics, including AUC and its confidence intervals
calculate_metrics <- function(data, risk_col, outcome_col, time_col) {
  
  # Filter out rows with missing or invalid data
  data_filtered <- data %>% filter(!is.na(data[[outcome_col]]) & !is.na(data[[risk_col]]))
  
  # Ensure outcome_col is binary (0/1)
  data_filtered[[outcome_col]] <- ifelse(data_filtered[[outcome_col]] > 0, 1, 0)
  
  # Check if data_filtered has rows after filtering
  if (nrow(data_filtered) == 0) {
    stop("Error: No valid data after filtering. Check the columns for missing values or incorrect column names.")
  }

  # Calculate ROC and AUC with confidence intervals
  roc_curve <- roc(data_filtered[[outcome_col]], data_filtered[[risk_col]])
  auc_value <- as.numeric(auc(roc_curve))  # AUC
  auc_ci <- ci.auc(roc_curve)  # Calculate 95% CI for AUC
  
  # Calculate Harrell's C-statistic using Cox proportional hazards model
  cox_model <- coxph(Surv(data_filtered[[time_col]], data_filtered[[outcome_col]]) ~ data_filtered[[risk_col]], data = data_filtered)
  c_stat <- as.numeric(summary(cox_model)$concordance[1])  # C-index (Harrell's C-statistic)

  # Calculate confusion matrix statistics: Sensitivity, Specificity, PPV, NPV
  pred_class <- ifelse(data_filtered[[risk_col]] > 0.5, 1, 0)
  confusion_mat <- table(Predicted = pred_class, Actual = data_filtered[[outcome_col]])

  if (nrow(confusion_mat) < 2 || ncol(confusion_mat) < 2) {
    sensitivity <- NA
    specificity <- NA
    ppv <- NA
    npv <- NA
  } else {
    sensitivity <- confusion_mat[2, 2] / sum(confusion_mat[, 2])
    specificity <- confusion_mat[1, 1] / sum(confusion_mat[, 1])
    ppv <- confusion_mat[2, 2] / sum(confusion_mat[2, ])
    npv <- confusion_mat[1, 1] / sum(confusion_mat[1, ])
  }

  return(list(
    AUC = auc_value,
    AUC_CI_Lower = auc_ci[1],
    AUC_CI_Upper = auc_ci[3],
    C_Statistic = c_stat,
    Sensitivity = sensitivity,
    Specificity = specificity,
    PPV = ppv,
    NPV = npv
  ))
}

# Function to summarize metrics for each racial group
summarize_risk_metrics <- function(data, race_label) {
  # Calculate metrics for 2-year and 5-year outcomes
  metrics_2_years <- calculate_metrics(data, "risk_2_years", "outcome_2_years", "time_to_event")
  metrics_5_years <- calculate_metrics(data, "risk_5_years", "outcome_5_years", "time_to_event")

  case_control_2yr <- table(data$outcome_2_years)
  case_control_5yr <- table(data$outcome_5_years)

  # Return the metrics as a data frame
  return(data.frame(
    Race = race_label,
    `AUC 2 Years` = metrics_2_years$AUC,
    `AUC 2 Years CI Lower` = metrics_2_years$AUC_CI_Lower,
    `AUC 2 Years CI Upper` = metrics_2_years$AUC_CI_Upper,
    `Harrell's C-Statistic 2 Years` = metrics_2_years$C_Statistic,
    `Sensitivity 2 Years` = metrics_2_years$Sensitivity,
    `Specificity 2 Years` = metrics_2_years$Specificity,
    `PPV 2 Years` = metrics_2_years$PPV,
    `NPV 2 Years` = metrics_2_years$NPV,
    `Cases 2 Years` = case_control_2yr["1"],
    `Controls 2 Years` = case_control_2yr["0"],
    `AUC 5 Years` = metrics_5_years$AUC,
    `AUC 5 Years CI Lower` = metrics_5_years$AUC_CI_Lower,
    `AUC 5 Years CI Upper` = metrics_5_years$AUC_CI_Upper,
    `Harrell's C-Statistic 5 Years` = metrics_5_years$C_Statistic,
    `Sensitivity 5 Years` = metrics_5_years$Sensitivity,
    `Specificity 5 Years` = metrics_5_years$Specificity,
    `PPV 5 Years` = metrics_5_years$PPV,
    `NPV 5 Years` = metrics_5_years$NPV,
    `Cases 5 Years` = case_control_5yr["1"],
    `Controls 5 Years` = case_control_5yr["0"]
  ))
}

# Evaluate for all races
results_all <- summarize_risk_metrics(consistent_data, "All Races")

# Evaluate for Black or African American
results_black <- summarize_risk_metrics(
  consistent_data %>% filter(race == "Black or African American"),
  "Black or African American"
)

# Evaluate for White
results_white <- summarize_risk_metrics(
  consistent_data %>% filter(race == "White"),
  "White"
)

# Combine all results into one table
final_results <- bind_rows(results_all, results_black, results_white)

# Print the combined results
cat("\nCombined Risk Metrics, AUC (with CI), Harrell's C-Statistic, Sensitivity, Specificity, and Case-Control Counts:\n")
print(final_results)

# Save the final results to a CSV file
write.csv(final_results, "combined_risk_metrics_with_case_control_counts_and_CI.csv", row.names = FALSE)

```
#### Model evaluation: Bootstrap 5,000 random samples
```{r}
# Load necessary libraries
library(dplyr)
library(pROC)
library(rms)  # For Harrell's C-statistic
library(kableExtra)

# Function to calculate metrics and include bootstrapped AUC confidence intervals
calculate_metrics_with_bootstrap <- function(data, risk_col, outcome_col, time_col, n_bootstrap = 5000) {
  
  # Filter out rows with missing or invalid data
  data_filtered <- data %>% filter(!is.na(data[[outcome_col]]) & !is.na(data[[risk_col]]))
  
  # Ensure outcome_col is binary (0/1)
  data_filtered[[outcome_col]] <- ifelse(data_filtered[[outcome_col]] > 0, 1, 0)
  
  # Calculate ROC and AUC
  roc_curve <- roc(data_filtered[[outcome_col]], data_filtered[[risk_col]])
  auc_value <- as.numeric(auc(roc_curve))  # AUC

  # Bootstrapping AUC for confidence intervals
  auc_values <- numeric(n_bootstrap)
  
  for (i in 1:n_bootstrap) {
    bootstrap_sample <- data_filtered[sample(1:nrow(data_filtered), replace = TRUE), ]
    roc_curve <- roc(bootstrap_sample[[outcome_col]], bootstrap_sample[[risk_col]])
    auc_values[i] <- as.numeric(auc(roc_curve))
  }
  
  auc_ci <- quantile(auc_values, probs = c(0.025, 0.975))  # 95% CI
  
  # Calculate Harrell's C-statistic using Cox proportional hazards model
  cox_model <- coxph(Surv(data_filtered[[time_col]], data_filtered[[outcome_col]]) ~ data_filtered[[risk_col]], data = data_filtered)
  c_stat <- as.numeric(summary(cox_model)$concordance[1])  # C-index (Harrell's C-statistic)

  # Calculate confusion matrix statistics: Sensitivity, Specificity, PPV, NPV
  pred_class <- ifelse(data_filtered[[risk_col]] > 0.5, 1, 0)
  confusion_mat <- table(Predicted = pred_class, Actual = data_filtered[[outcome_col]])

  sensitivity <- confusion_mat[2, 2] / sum(confusion_mat[, 2])
  specificity <- confusion_mat[1, 1] / sum(confusion_mat[, 1])
  ppv <- confusion_mat[2, 2] / sum(confusion_mat[2, ])
  npv <- confusion_mat[1, 1] / sum(confusion_mat[1, ])

  return(list(
    AUC = auc_value,
    AUC_Lower = auc_ci[1],
    AUC_Upper = auc_ci[2],
    C_Statistic = c_stat,
    Sensitivity = sensitivity,
    Specificity = specificity,
    PPV = ppv,
    NPV = npv
  ))
}

# Function to summarize metrics for each racial group
summarize_risk_metrics <- function(data, race_label) {
  # Calculate metrics for 2-year and 5-year outcomes
  metrics_2_years <- calculate_metrics_with_bootstrap(data, "risk_2_years", "outcome_2_years", "time_to_event")
  metrics_5_years <- calculate_metrics_with_bootstrap(data, "risk_5_years", "outcome_5_years", "time_to_event")

  case_control_2yr <- table(data$outcome_2_years)
  case_control_5yr <- table(data$outcome_5_years)

  # Summarize the metrics
  data %>%
    summarise(
      Race = race_label,
      `AUC 2 Years` = metrics_2_years$AUC,
      `AUC 2 Years Lower CI` = metrics_2_years$AUC_Lower,
      `AUC 2 Years Upper CI` = metrics_2_years$AUC_Upper,
      `Harrell's C-Statistic 2 Years` = metrics_2_years$C_Statistic,
      `Sensitivity 2 Years` = metrics_2_years$Sensitivity,
      `Specificity 2 Years` = metrics_2_years$Specificity,
      `PPV 2 Years` = metrics_2_years$PPV,
      `NPV 2 Years` = metrics_2_years$NPV,
      `Cases 2 Years` = case_control_2yr["1"],
      `Controls 2 Years` = case_control_2yr["0"],
      `AUC 5 Years` = metrics_5_years$AUC,
      `AUC 5 Years Lower CI` = metrics_5_years$AUC_Lower,
      `AUC 5 Years Upper CI` = metrics_5_years$AUC_Upper,
      `Harrell's C-Statistic 5 Years` = metrics_5_years$C_Statistic,
      `Sensitivity 5 Years` = metrics_5_years$Sensitivity,
      `Specificity 5 Years` = metrics_5_years$Specificity,
      `PPV 5 Years` = metrics_5_years$PPV,
      `NPV 5 Years` = metrics_5_years$NPV,
      `Cases 5 Years` = case_control_5yr["1"],
      `Controls 5 Years` = case_control_5yr["0"]
    )
}

# Evaluate for all races
results_all <- summarize_risk_metrics(consistent_data, "All Races")

# Evaluate for Black or African American
results_black <- summarize_risk_metrics(
  consistent_data %>% filter(race == "Black or African American"),
  "Black or African American"
)

# Evaluate for White
results_white <- summarize_risk_metrics(
  consistent_data %>% filter(race == "White"),
  "White"
)

# Combine all results into one table
final_results <- bind_rows(results_all, results_black, results_white)

# Print the combined results
cat("\nCombined Risk Metrics, AUC (with CI), Harrell's C-Statistic, Sensitivity, Specificity, and Case-Control Counts:\n")
print(final_results)

# Save the final results to a CSV file
write.csv(final_results, "combined_risk_metrics_with_case_control_counts_and_CI.csv", row.names = FALSE)

```

# CKD-EPI 2009
### Value calculation
```{r}
inrange_person_with_summary <- read.csv("survival_data.csv")
#colnames(inrange_person_with_summary)

# Step 1: Extract relevant columns and set up the CKD-EPI 2009 equation variables
inrange_person_with_summary <- inrange_person_with_summary %>%
  mutate(
    # Define the kappa and alpha parameters based on sex
    kappa = ifelse(sex_at_birth == "Female", 0.7, 0.9),
    alpha = ifelse(sex_at_birth == "Female", -0.329, -0.411),
    sex_factor = ifelse(sex_at_birth == "Female", 1.018, 1),

    # Define the race factor (1 for White, 1.159 for Black or African American)
    race_factor = ifelse(race == "Black or African American", 1.159, 1),

    # Apply the CKD-EPI 2009 equation for eGFR calculation
    eGFR_ckd_epi_2009 = 141 * pmin(serum_creatinine / kappa, 1)^alpha * 
                        pmax(serum_creatinine / kappa, 1)^(-1.209) * 
                        (0.993^age_precise) * sex_factor * race_factor
  )

# Step 2: Print the number of cases and controls that have a valid eGFR_ckd_epi_2009 value
# Assuming disease_status == 1 means case and disease_status == 0 means control
valid_eGFR_counts <- inrange_person_with_summary %>%
  filter(!is.na(eGFR_ckd_epi_2009)) %>%
  group_by(disease_status) %>%
  summarise(count = n())

cat("Number of cases (disease_status == 1) with valid eGFR_ckd_epi_2009:", 
    valid_eGFR_counts$count[valid_eGFR_counts$disease_status == 1], "\n")
cat("Number of controls (disease_status == 0) with valid eGFR_ckd_epi_2009:", 
    valid_eGFR_counts$count[valid_eGFR_counts$disease_status == 0], "\n")

# Step 3: Print the summary statistics of the eGFR_ckd_epi_2009 values
summary_eGFR <- summary(inrange_person_with_summary$eGFR_ckd_epi_2009)

cat("\nSummary statistics for eGFR_ckd_epi_2009:\n")
print(summary_eGFR)

# Optional: Print a few rows of the dataset to verify the eGFR values are appended
head(inrange_person_with_summary, 10)


```
## < 60: CKD Population
```{r}
# Load necessary libraries
library(dplyr)
library(survival)

# Define a plausible range for eGFR and Albumin/Creatinine
plausible_eGFR_min <- 0
plausible_eGFR_max <- 60

plausible_acr_min <- 0
plausible_acr_max <- 8000  # 8000 mg/g is a very high value, but still clinically plausible

# Step 1: Filter out implausible eGFR and Albumin/Creatinine values
person_filtered_KFRE <- inrange_person_with_summary %>%
  select(person_id, sex_at_birth, eGFR_ckd_epi_2009, Albumin_Creatinine = `Albumin.Creatinine`, 
         age_precise, race, disease_status, time_to_event)

# Step 2: Data preprocessing
person_filtered_KFRE <- person_filtered_KFRE %>%
  mutate(event = disease_status) %>%
  filter(
    eGFR_ckd_epi_2009 >= plausible_eGFR_min & eGFR_ckd_epi_2009 < plausible_eGFR_max, # Using eGFR_ckd_epi_2009
    Albumin_Creatinine >= plausible_acr_min & Albumin_Creatinine <= plausible_acr_max,
    sex_at_birth %in% c("Female", "Male")
  ) %>%
  mutate(logACR = log(Albumin_Creatinine)) %>%
  filter(complete.cases(.))

# Perform a complete case analysis
initial_row_count <- nrow(person_filtered_KFRE)

complete_cases <- person_filtered_KFRE %>%
  filter(complete.cases(.))

final_row_count <- nrow(complete_cases)

# Check if any rows have been removed
rows_removed <- initial_row_count - final_row_count

if (rows_removed > 0) {
  cat(rows_removed, "rows have been removed during the complete case analysis.\n")
} else {
  cat("No rows have been removed during the complete case analysis.\n")
}

# Report the number of complete cases
num_complete_cases <- nrow(complete_cases)
print(paste("Number of complete cases:", num_complete_cases))

# Remove any rows with NA in time_to_event or disease_status
complete_cases <- complete_cases %>%
  filter(!is.na(time_to_event) & !is.na(disease_status))

# Check the event distribution
print(table(complete_cases$disease_status))

# Define the formula for βsum using eGFR_ckd_epi_2009
calculate_beta_sum <- function(age, sex, eGFR_ckd_epi_2009, logACR) {
  -0.2201 * (age / 10 - 7.036) +
    0.2467 * (ifelse(sex == "Male", 1, 0) - 0.5642) -
    0.5567 * (eGFR_ckd_epi_2009 / 5 - 7.222) +
    0.4510 * (logACR - 5.137)
}

# Apply the formula to calculate βsum
complete_cases <- complete_cases %>%
  mutate(beta_sum = calculate_beta_sum(age_precise, sex_at_birth, eGFR_ckd_epi_2009, logACR))

# Remove any rows with infinite or NA beta_sum
complete_cases <- complete_cases %>%
  filter(is.finite(beta_sum) & !is.na(beta_sum))

# Calculate the risk for two and five years using the updated formulas
complete_cases <- complete_cases %>%
  mutate(
    risk_2_years = 100 * (1 - 0.975^exp(beta_sum)),  # Two-Year Risk calculation
    risk_5_years = 100 * (1 - 0.9240^exp(beta_sum))  # Five-Year Risk calculation
  )

# Save the data before filtering
before_filtering <- complete_cases

# Apply the filtering to create consistent_data
consistent_data <- complete_cases %>%
  filter(!is.na(risk_5_years) & !is.na(risk_2_years))

# Find the rows that were removed by comparing before and after datasets
removed_rows <- anti_join(before_filtering, consistent_data, by = "person_id")

# Print the removed rows
if (nrow(removed_rows) > 0) {
  cat("Rows that were removed during filtering:\n")
  print(removed_rows)
} else {
  cat("No rows were removed during filtering.\n")
}

# Verify the risk values
summary(consistent_data$risk_5_years)
summary(consistent_data$risk_2_years)

# Count the number of person_id entries with event == 1 and event == 0
event_count <- consistent_data %>%
  group_by(event) %>%
  summarise(count = n_distinct(person_id))

# Print the results
cat("Number of person_id with event == 1:", event_count$count[event_count$event == 1], "\n")
cat("Number of person_id with event == 0:", event_count$count[event_count$event == 0], "\n")

```
#### Model evaluation: non-coss validation
```{r}
# Load necessary libraries
library(dplyr)
library(survival)
library(pROC)
library(rms)  # For Harrell's C-statistic
library(kableExtra)

# Create 2-year and 5-year outcomes based on disease status and time to event
consistent_data <- consistent_data %>%
  mutate(
    outcome_2_years = ifelse(disease_status == 1 & time_to_event <= 2 * 365.25, 1, 0),
    outcome_5_years = ifelse(disease_status == 1 & time_to_event <= 5 * 365.25, 1, 0)
  )

# Define a function to calculate AUC, Harrell's C-statistic, and other metrics
calculate_metrics <- function(data, risk_col, outcome_col, time_col) {
  # Filter out rows with missing or invalid data
  data_filtered <- data %>% filter(!is.na(data[[outcome_col]]) & !is.na(data[[risk_col]]))

  # Ensure outcome_col is binary (0/1)
  data_filtered[[outcome_col]] <- ifelse(data_filtered[[outcome_col]] > 0, 1, 0)

  # Calculate AUC
  roc_curve <- roc(data_filtered[[outcome_col]], data_filtered[[risk_col]])
  auc_value <- as.numeric(auc(roc_curve))  # Convert AUC to numeric

  # Calculate Harrell's C-statistic using Cox proportional hazards model
  cox_model <- coxph(Surv(data_filtered[[time_col]], data_filtered[[outcome_col]]) ~ data_filtered[[risk_col]], data = data_filtered)
  c_stat <- as.numeric(summary(cox_model)$concordance[1])  # Extract C-index (Harrell's C-statistic)

  # Calculate confusion matrix statistics: Sensitivity, Specificity, PPV, NPV
  pred_class <- ifelse(data_filtered[[risk_col]] > 0.5, 1, 0)
  confusion_mat <- table(Predicted = pred_class, Actual = data_filtered[[outcome_col]])

  sensitivity <- confusion_mat[2, 2] / sum(confusion_mat[, 2])
  specificity <- confusion_mat[1, 1] / sum(confusion_mat[, 1])
  ppv <- confusion_mat[2, 2] / sum(confusion_mat[2, ])
  npv <- confusion_mat[1, 1] / sum(confusion_mat[1, ])

  return(list(
    AUC = auc_value,
    C_Statistic = c_stat,
    Sensitivity = sensitivity,
    Specificity = specificity,
    PPV = ppv,
    NPV = npv
  ))
}

# Define a function to summarize metrics for each racial group
summarize_risk_metrics <- function(data, race_label) {
  # Calculate metrics for 2-year and 5-year outcomes
  metrics_2_years <- calculate_metrics(data, "risk_2_years", "outcome_2_years", "time_to_event")
  metrics_5_years <- calculate_metrics(data, "risk_5_years", "outcome_5_years", "time_to_event")

  case_control_2yr <- table(data$outcome_2_years)
  case_control_5yr <- table(data$outcome_5_years)

  # Summarize the metrics
  data %>%
    summarise(
      Race = race_label,
      `AUC 2 Years` = metrics_2_years$AUC,
      `Harrell's C-Statistic 2 Years` = metrics_2_years$C_Statistic,
      `Sensitivity 2 Years` = metrics_2_years$Sensitivity,
      `Specificity 2 Years` = metrics_2_years$Specificity,
      `PPV 2 Years` = metrics_2_years$PPV,
      `NPV 2 Years` = metrics_2_years$NPV,
      `Cases 2 Years` = case_control_2yr["1"],
      `Controls 2 Years` = case_control_2yr["0"],
      `AUC 5 Years` = metrics_5_years$AUC,
      `Harrell's C-Statistic 5 Years` = metrics_5_years$C_Statistic,
      `Sensitivity 5 Years` = metrics_5_years$Sensitivity,
      `Specificity 5 Years` = metrics_5_years$Specificity,
      `PPV 5 Years` = metrics_5_years$PPV,
      `NPV 5 Years` = metrics_5_years$NPV,
      `Cases 5 Years` = case_control_5yr["1"],
      `Controls 5 Years` = case_control_5yr["0"]
    )
}

# Evaluate for all races
results_all <- summarize_risk_metrics(consistent_data, "All Races")

# Evaluate for Black or African American
results_black <- summarize_risk_metrics(
  consistent_data %>% filter(race == "Black or African American"),
  "Black or African American"
)

# Evaluate for White
results_white <- summarize_risk_metrics(
  consistent_data %>% filter(race == "White"),
  "White"
)

# Combine all results into one table
final_results <- bind_rows(results_all, results_black, results_white)

# Print the combined results
cat("\nCombined Risk Metrics, AUC, Harrell's C-Statistic, Sensitivity, Specificity, and Case-Control Counts:\n")
print(final_results)

# Save the final results to a CSV file
write.csv(final_results, "combined_risk_metrics_with_case_control_counts.csv", row.names = FALSE)

```
#### Model:Evaluation: Cross validation
```{r}
# Load necessary libraries
library(dplyr)
library(caret)
library(pROC)
library(survival)

# Ensure consistent_data is correctly set up with the necessary columns
consistent_data <- consistent_data %>%
  mutate(
    outcome_2_years = ifelse(disease_status == 1 & time_to_event <= 2 * 365.25, 1, 0),
    outcome_5_years = ifelse(disease_status == 1 & time_to_event <= 5 * 365.25, 1, 0)
  ) %>%
  filter(!is.na(time_to_event) & !is.na(disease_status))  # Ensure no missing values in time_to_event or disease_status

# Function to calculate metrics for model evaluation
calculate_metrics <- function(true_labels, predicted_probs, threshold = 0.5) {
  if (length(unique(true_labels)) < 2) {
    # If true_labels doesn't have two levels, we can't calculate these metrics
    warning("Fold has only one class, skipping calculations.")
    return(list(Sensitivity = NA, Specificity = NA, PPV = NA, NPV = NA))
  }
  
  predicted_classes <- ifelse(predicted_probs > threshold, 1, 0)
  
  # Create a confusion matrix
  conf_matrix <- table(Predicted = predicted_classes, Actual = true_labels)
  
  # Ensure the confusion matrix has all cells (fill missing cells with 0)
  conf_matrix <- as.table(matrix(c(conf_matrix), nrow = 2, ncol = 2, 
                                 dimnames = list(Predicted = c(0, 1), Actual = c(0, 1)), byrow = TRUE))
  
  # Extract TP, TN, FP, and FN
  TP <- conf_matrix["1", "1"]
  TN <- conf_matrix["0", "0"]
  FP <- conf_matrix["1", "0"]
  FN <- conf_matrix["0", "1"]
  
  # Calculate Sensitivity, Specificity, PPV, NPV
  sensitivity <- ifelse(TP + FN > 0, TP / (TP + FN), NA)
  specificity <- ifelse(TN + FP > 0, TN / (TN + FP), NA)
  ppv <- ifelse(TP + FP > 0, TP / (TP + FP), NA)  # Positive Predictive Value
  npv <- ifelse(TN + FN > 0, TN / (TN + FN), NA)  # Negative Predictive Value
  
  return(list(Sensitivity = sensitivity, Specificity = specificity, PPV = ppv, NPV = npv))
}

# Function to perform cross-validation and evaluate the model
cross_validate_model <- function(data, risk_col, outcome_col, k_folds = 10) {
  # Filter out any rows where risk_col or outcome_col is missing
  data <- data %>%
    filter(!is.na(.data[[risk_col]]) & !is.na(.data[[outcome_col]]))
  
  set.seed(123)  # For reproducibility
  
  # Create the folds
  folds <- createFolds(data[[outcome_col]], k = k_folds, returnTrain = FALSE)

  # Initialize vectors to store results
  aucs <- c()
  harrell_c_stats <- c()
  sensitivities <- c()
  specificities <- c()
  ppvs <- c()
  npvs <- c()
  
  for (i in 1:k_folds) {
    # Split the data into training and test sets
    fold_test <- data[folds[[i]], ]
    
    # Check if there is enough data in the fold to calculate metrics
    if (nrow(fold_test) == 0) {
      next  # Skip empty folds
    }
    
    true_labels <- fold_test[[outcome_col]]
    
    if (length(unique(true_labels)) < 2) {
      warning("Fold has only one class, skipping ROC calculation for this fold.")
      next  # Skip if there's only one class in this fold
    }
    
    predicted_probs <- fold_test[[risk_col]]
    
    # Calculate AUC
    roc_curve <- tryCatch({
      roc(true_labels, predicted_probs)
    }, error = function(e) {
      warning("Error in ROC calculation: ", e$message)
      return(NULL)
    })
    
    if (!is.null(roc_curve)) {
      auc_value <- auc(roc_curve)
      aucs <- c(aucs, auc_value)
    }
    
    # Calculate Harrell's C-statistic using Cox proportional hazards model
    cox_model <- tryCatch({
      coxph(Surv(fold_test$time_to_event, true_labels) ~ predicted_probs, data = fold_test, control = coxph.control(iter.max = 50))
    }, warning = function(w) {
      warning("Warning in CoxPH: ", w$message)
      return(NULL)
    }, error = function(e) {
      warning("Error in CoxPH: ", e$message)
      return(NULL)
    })
    
    if (!is.null(cox_model)) {
      harrell_c_stat <- summary(cox_model)$concordance[1]
      harrell_c_stats <- c(harrell_c_stats, harrell_c_stat)
    }
    
    # Calculate Sensitivity, Specificity, PPV, NPV
    metrics <- calculate_metrics(true_labels, predicted_probs)
    sensitivities <- c(sensitivities, metrics$Sensitivity)
    specificities <- c(specificities, metrics$Specificity)
    ppvs <- c(ppvs, metrics$PPV)
    npvs <- c(npvs, metrics$NPV)
  }
  
  return(list(
    AUC_mean = mean(aucs, na.rm = TRUE),
    Harrell_C_mean = mean(harrell_c_stats, na.rm = TRUE),
    Sensitivity_mean = mean(sensitivities, na.rm = TRUE),
    Specificity_mean = mean(specificities, na.rm = TRUE),
    PPV_mean = mean(ppvs, na.rm = TRUE),
    NPV_mean = mean(npvs, na.rm = TRUE)
  ))
}

# Function to run cross-validation for a specific group and risk/outcome columns
cross_validate_model_by_group <- function(data, risk_col, outcome_col, group_filter) {
  # Filter data by group if a filter is provided
  if (!is.null(group_filter)) {
    data <- data %>% filter(race == group_filter)
  }

  # Run cross-validation only if there is data left after filtering
  if (nrow(data) > 0) {
    return(cross_validate_model(data, risk_col, outcome_col, k_folds = 10))
  } else {
    return(list(
      AUC_mean = NA, Harrell_C_mean = NA, Sensitivity_mean = NA, Specificity_mean = NA,
      PPV_mean = NA, NPV_mean = NA
    ))
  }
}

# Run cross-validation for All Races, Black or African American (race == "Black or African American"), and White (race == "White")
cv_results_all <- cross_validate_model_by_group(consistent_data, "risk_2_years", "outcome_2_years", group_filter = NULL)
cv_results_black <- cross_validate_model_by_group(consistent_data, "risk_2_years", "outcome_2_years", group_filter = "Black or African American")
cv_results_white <- cross_validate_model_by_group(consistent_data, "risk_2_years", "outcome_2_years", group_filter = "White")

# Compile the results into a comparison table for both 2-year and 5-year outcomes
cv_results_table <- data.frame(
  Group = c("All Races", "Black or African American", "White"),
  AUC_2_years = c(cv_results_all$AUC_mean, cv_results_black$AUC_mean, cv_results_white$AUC_mean),
  Harrell_C_2_years = c(cv_results_all$Harrell_C_mean, cv_results_black$Harrell_C_mean, cv_results_white$Harrell_C_mean),
  Sensitivity_2_years = c(cv_results_all$Sensitivity_mean, cv_results_black$Sensitivity_mean, cv_results_white$Sensitivity_mean),
  Specificity_2_years = c(cv_results_all$Specificity_mean, cv_results_black$Specificity_mean, cv_results_white$Specificity_mean),
  PPV_2_years = c(cv_results_all$PPV_mean, cv_results_black$PPV_mean, cv_results_white$PPV_mean),
  NPV_2_years = c(cv_results_all$NPV_mean, cv_results_black$NPV_mean, cv_results_white$NPV_mean)
)

# Repeat the process for 5-year risk outcomes
cv_results_all_5 <- cross_validate_model_by_group(consistent_data, "risk_5_years", "outcome_5_years", group_filter = NULL)
cv_results_black_5 <- cross_validate_model_by_group(consistent_data, "risk_5_years", "outcome_5_years", group_filter = "Black or African American")
cv_results_white_5 <- cross_validate_model_by_group(consistent_data, "risk_5_years", "outcome_5_years", group_filter = "White")

# Add 5-year metrics to the table
cv_results_table$AUC_5_years <- c(cv_results_all_5$AUC_mean, cv_results_black_5$AUC_mean, cv_results_white_5$AUC_mean)
cv_results_table$Harrell_C_5_years <- c(cv_results_all_5$Harrell_C_mean, cv_results_black_5$Harrell_C_mean, cv_results_white_5$Harrell_C_mean)
cv_results_table$Sensitivity_5_years <- c(cv_results_all_5$Sensitivity_mean, cv_results_black_5$Sensitivity_mean, cv_results_white_5$Sensitivity_mean)
cv_results_table$Specificity_5_years <- c(cv_results_all_5$Specificity_mean, cv_results_black_5$Specificity_mean, cv_results_white_5$Specificity_mean)
cv_results_table$PPV_5_years <- c(cv_results_all_5$PPV_mean, cv_results_black_5$PPV_mean, cv_results_white_5$PPV_mean)
cv_results_table$NPV_5_years <- c(cv_results_all_5$NPV_mean, cv_results_black_5$NPV_mean, cv_results_white_5$NPV_mean)

# Add case-control counts
case_control_counts <- consistent_data %>%
  group_by(race) %>%
  summarise(
    cases = sum(disease_status == 1),
    controls = sum(disease_status == 0)
  )

# Print the final results table along with case-control counts
print(cv_results_table)
print(case_control_counts)
```


## 60-500: Non- CKD Population
```{r}
# Load necessary libraries
library(dplyr)
library(survival)

# Define a plausible range for eGFR and Albumin/Creatinine
plausible_eGFR_min <- 0
plausible_eGFR_max <- 500

plausible_acr_min <- 0
plausible_acr_max <- 8000  # 8000 mg/g is a very high value, but still clinically plausible

# Step 1: Filter out implausible eGFR and Albumin/Creatinine values
person_filtered_KFRE <- inrange_person_with_summary %>%
  select(person_id, sex_at_birth, eGFR_ckd_epi_2009, Albumin_Creatinine = `Albumin.Creatinine`, 
         age_precise, race, disease_status, time_to_event)

# Step 2: Data preprocessing
person_filtered_KFRE <- person_filtered_KFRE %>%
  mutate(event = disease_status) %>%
  filter(
    eGFR_ckd_epi_2009 >= plausible_eGFR_min & eGFR_ckd_epi_2009 <= plausible_eGFR_max, # Using eGFR_ckd_epi_2009
    Albumin_Creatinine >= plausible_acr_min & Albumin_Creatinine <= plausible_acr_max,
    sex_at_birth %in% c("Female", "Male")
  ) %>%
  mutate(logACR = log(Albumin_Creatinine)) %>%
  filter(complete.cases(.))

# Perform a complete case analysis
initial_row_count <- nrow(person_filtered_KFRE)

complete_cases <- person_filtered_KFRE %>%
  filter(complete.cases(.))

final_row_count <- nrow(complete_cases)

# Check if any rows have been removed
rows_removed <- initial_row_count - final_row_count

if (rows_removed > 0) {
  cat(rows_removed, "rows have been removed during the complete case analysis.\n")
} else {
  cat("No rows have been removed during the complete case analysis.\n")
}

# Report the number of complete cases
num_complete_cases <- nrow(complete_cases)
print(paste("Number of complete cases:", num_complete_cases))

# Remove any rows with NA in time_to_event or disease_status
complete_cases <- complete_cases %>%
  filter(!is.na(time_to_event) & !is.na(disease_status))

# Check the event distribution
print(table(complete_cases$disease_status))

# Define the formula for βsum using eGFR_ckd_epi_2009
calculate_beta_sum <- function(age, sex, eGFR_ckd_epi_2009, logACR) {
  -0.2201 * (age / 10 - 7.036) +
    0.2467 * (ifelse(sex == "Male", 1, 0) - 0.5642) -
    0.5567 * (eGFR_ckd_epi_2009 / 5 - 7.222) +
    0.4510 * (logACR - 5.137)
}

# Apply the formula to calculate βsum
complete_cases <- complete_cases %>%
  mutate(beta_sum = calculate_beta_sum(age_precise, sex_at_birth, eGFR_ckd_epi_2009, logACR))

# Remove any rows with infinite or NA beta_sum
complete_cases <- complete_cases %>%
  filter(is.finite(beta_sum) & !is.na(beta_sum))

# Calculate the risk for two and five years using the updated formulas
complete_cases <- complete_cases %>%
  mutate(
    risk_2_years = 100 * (1 - 0.975^exp(beta_sum)),  # Two-Year Risk calculation
    risk_5_years = 100 * (1 - 0.9240^exp(beta_sum))  # Five-Year Risk calculation
  )

# Save the data before filtering
before_filtering <- complete_cases

# Apply the filtering to create consistent_data
consistent_data <- complete_cases %>%
  filter(!is.na(risk_5_years) & !is.na(risk_2_years))

# Find the rows that were removed by comparing before and after datasets
removed_rows <- anti_join(before_filtering, consistent_data, by = "person_id")

# Print the removed rows
if (nrow(removed_rows) > 0) {
  cat("Rows that were removed during filtering:\n")
  print(removed_rows)
} else {
  cat("No rows were removed during filtering.\n")
}

# Verify the risk values
summary(consistent_data$risk_5_years)
summary(consistent_data$risk_2_years)

# Count the number of person_id entries with event == 1 and event == 0
event_count <- consistent_data %>%
  group_by(event) %>%
  summarise(count = n_distinct(person_id))

# Print the results
cat("Number of person_id with event == 1:", event_count$count[event_count$event == 1], "\n")
cat("Number of person_id with event == 0:", event_count$count[event_count$event == 0], "\n")

```

```{r}
# Load necessary libraries
library(dplyr)
library(survival)
library(pROC)
library(rms)  # For Harrell's C-statistic
library(kableExtra)

# Create 2-year and 5-year outcomes based on disease status and time to event
consistent_data <- consistent_data %>%
  mutate(
    outcome_2_years = ifelse(disease_status == 1 & time_to_event <= 2 * 365.25, 1, 0),
    outcome_5_years = ifelse(disease_status == 1 & time_to_event <= 5 * 365.25, 1, 0)
  )

# Define a function to calculate AUC, Harrell's C-statistic, and other metrics
calculate_metrics <- function(data, risk_col, outcome_col, time_col) {
  # Filter out rows with missing or invalid data
  data_filtered <- data %>% filter(!is.na(data[[outcome_col]]) & !is.na(data[[risk_col]]))

  # Ensure outcome_col is binary (0/1)
  data_filtered[[outcome_col]] <- ifelse(data_filtered[[outcome_col]] > 0, 1, 0)

  # Calculate AUC
  roc_curve <- roc(data_filtered[[outcome_col]], data_filtered[[risk_col]])
  auc_value <- as.numeric(auc(roc_curve))  # Convert AUC to numeric

  # Calculate Harrell's C-statistic using Cox proportional hazards model
  cox_model <- coxph(Surv(data_filtered[[time_col]], data_filtered[[outcome_col]]) ~ data_filtered[[risk_col]], data = data_filtered)
  c_stat <- as.numeric(summary(cox_model)$concordance[1])  # Extract C-index (Harrell's C-statistic)

  # Calculate confusion matrix statistics: Sensitivity, Specificity, PPV, NPV
  pred_class <- ifelse(data_filtered[[risk_col]] > 0.5, 1, 0)
  confusion_mat <- table(Predicted = pred_class, Actual = data_filtered[[outcome_col]])

  sensitivity <- confusion_mat[2, 2] / sum(confusion_mat[, 2])
  specificity <- confusion_mat[1, 1] / sum(confusion_mat[, 1])
  ppv <- confusion_mat[2, 2] / sum(confusion_mat[2, ])
  npv <- confusion_mat[1, 1] / sum(confusion_mat[1, ])

  return(list(
    AUC = auc_value,
    C_Statistic = c_stat,
    Sensitivity = sensitivity,
    Specificity = specificity,
    PPV = ppv,
    NPV = npv
  ))
}

# Define a function to summarize metrics for each racial group
summarize_risk_metrics <- function(data, race_label) {
  # Calculate metrics for 2-year and 5-year outcomes
  metrics_2_years <- calculate_metrics(data, "risk_2_years", "outcome_2_years", "time_to_event")
  metrics_5_years <- calculate_metrics(data, "risk_5_years", "outcome_5_years", "time_to_event")

  case_control_2yr <- table(data$outcome_2_years)
  case_control_5yr <- table(data$outcome_5_years)

  # Summarize the metrics
  data %>%
    summarise(
      Race = race_label,
      `AUC 2 Years` = metrics_2_years$AUC,
      `Harrell's C-Statistic 2 Years` = metrics_2_years$C_Statistic,
      `Sensitivity 2 Years` = metrics_2_years$Sensitivity,
      `Specificity 2 Years` = metrics_2_years$Specificity,
      `PPV 2 Years` = metrics_2_years$PPV,
      `NPV 2 Years` = metrics_2_years$NPV,
      `Cases 2 Years` = case_control_2yr["1"],
      `Controls 2 Years` = case_control_2yr["0"],
      `AUC 5 Years` = metrics_5_years$AUC,
      `Harrell's C-Statistic 5 Years` = metrics_5_years$C_Statistic,
      `Sensitivity 5 Years` = metrics_5_years$Sensitivity,
      `Specificity 5 Years` = metrics_5_years$Specificity,
      `PPV 5 Years` = metrics_5_years$PPV,
      `NPV 5 Years` = metrics_5_years$NPV,
      `Cases 5 Years` = case_control_5yr["1"],
      `Controls 5 Years` = case_control_5yr["0"]
    )
}

# Evaluate for all races
results_all <- summarize_risk_metrics(consistent_data, "All Races")

# Evaluate for Black or African American
results_black <- summarize_risk_metrics(
  consistent_data %>% filter(race == "Black or African American"),
  "Black or African American"
)

# Evaluate for White
results_white <- summarize_risk_metrics(
  consistent_data %>% filter(race == "White"),
  "White"
)

# Combine all results into one table
final_results <- bind_rows(results_all, results_black, results_white)

# Print the combined results
cat("\nCombined Risk Metrics, AUC, Harrell's C-Statistic, Sensitivity, Specificity, and Case-Control Counts:\n")
print(final_results)

# Save the final results to a CSV file
write.csv(final_results, "combined_risk_metrics_with_case_control_counts.csv", row.names = FALSE)

```

```{r}
# Load necessary libraries
library(dplyr)
library(caret)
library(pROC)
library(survival)

# Ensure consistent_data is correctly set up with the necessary columns
consistent_data <- consistent_data %>%
  mutate(
    outcome_2_years = ifelse(disease_status == 1 & time_to_event <= 2 * 365.25, 1, 0),
    outcome_5_years = ifelse(disease_status == 1 & time_to_event <= 5 * 365.25, 1, 0)
  ) %>%
  filter(!is.na(time_to_event) & !is.na(disease_status))  # Ensure no missing values in time_to_event or disease_status

# Function to calculate metrics for model evaluation
calculate_metrics <- function(true_labels, predicted_probs, threshold = 0.5) {
  if (length(unique(true_labels)) < 2) {
    # If true_labels doesn't have two levels, we can't calculate these metrics
    warning("Fold has only one class, skipping calculations.")
    return(list(Sensitivity = NA, Specificity = NA, PPV = NA, NPV = NA))
  }
  
  predicted_classes <- ifelse(predicted_probs > threshold, 1, 0)
  
  # Create a confusion matrix
  conf_matrix <- table(Predicted = predicted_classes, Actual = true_labels)
  
  # Ensure the confusion matrix has all cells (fill missing cells with 0)
  conf_matrix <- as.table(matrix(c(conf_matrix), nrow = 2, ncol = 2, 
                                 dimnames = list(Predicted = c(0, 1), Actual = c(0, 1)), byrow = TRUE))
  
  # Extract TP, TN, FP, and FN
  TP <- conf_matrix["1", "1"]
  TN <- conf_matrix["0", "0"]
  FP <- conf_matrix["1", "0"]
  FN <- conf_matrix["0", "1"]
  
  # Calculate Sensitivity, Specificity, PPV, NPV
  sensitivity <- ifelse(TP + FN > 0, TP / (TP + FN), NA)
  specificity <- ifelse(TN + FP > 0, TN / (TN + FP), NA)
  ppv <- ifelse(TP + FP > 0, TP / (TP + FP), NA)  # Positive Predictive Value
  npv <- ifelse(TN + FN > 0, TN / (TN + FN), NA)  # Negative Predictive Value
  
  return(list(Sensitivity = sensitivity, Specificity = specificity, PPV = ppv, NPV = npv))
}

# Function to perform cross-validation and evaluate the model
cross_validate_model <- function(data, risk_col, outcome_col, k_folds = 10) {
  # Filter out any rows where risk_col or outcome_col is missing
  data <- data %>%
    filter(!is.na(.data[[risk_col]]) & !is.na(.data[[outcome_col]]))
  
  set.seed(123)  # For reproducibility
  
  # Create the folds
  folds <- createFolds(data[[outcome_col]], k = k_folds, returnTrain = FALSE)

  # Initialize vectors to store results
  aucs <- c()
  harrell_c_stats <- c()
  sensitivities <- c()
  specificities <- c()
  ppvs <- c()
  npvs <- c()
  
  for (i in 1:k_folds) {
    # Split the data into training and test sets
    fold_test <- data[folds[[i]], ]
    
    # Check if there is enough data in the fold to calculate metrics
    if (nrow(fold_test) == 0) {
      next  # Skip empty folds
    }
    
    true_labels <- fold_test[[outcome_col]]
    
    if (length(unique(true_labels)) < 2) {
      warning("Fold has only one class, skipping ROC calculation for this fold.")
      next  # Skip if there's only one class in this fold
    }
    
    predicted_probs <- fold_test[[risk_col]]
    
    # Calculate AUC
    roc_curve <- tryCatch({
      roc(true_labels, predicted_probs)
    }, error = function(e) {
      warning("Error in ROC calculation: ", e$message)
      return(NULL)
    })
    
    if (!is.null(roc_curve)) {
      auc_value <- auc(roc_curve)
      aucs <- c(aucs, auc_value)
    }
    
    # Calculate Harrell's C-statistic using Cox proportional hazards model
    cox_model <- tryCatch({
      coxph(Surv(fold_test$time_to_event, true_labels) ~ predicted_probs, data = fold_test, control = coxph.control(iter.max = 50))
    }, warning = function(w) {
      warning("Warning in CoxPH: ", w$message)
      return(NULL)
    }, error = function(e) {
      warning("Error in CoxPH: ", e$message)
      return(NULL)
    })
    
    if (!is.null(cox_model)) {
      harrell_c_stat <- summary(cox_model)$concordance[1]
      harrell_c_stats <- c(harrell_c_stats, harrell_c_stat)
    }
    
    # Calculate Sensitivity, Specificity, PPV, NPV
    metrics <- calculate_metrics(true_labels, predicted_probs)
    sensitivities <- c(sensitivities, metrics$Sensitivity)
    specificities <- c(specificities, metrics$Specificity)
    ppvs <- c(ppvs, metrics$PPV)
    npvs <- c(npvs, metrics$NPV)
  }
  
  return(list(
    AUC_mean = mean(aucs, na.rm = TRUE),
    Harrell_C_mean = mean(harrell_c_stats, na.rm = TRUE),
    Sensitivity_mean = mean(sensitivities, na.rm = TRUE),
    Specificity_mean = mean(specificities, na.rm = TRUE),
    PPV_mean = mean(ppvs, na.rm = TRUE),
    NPV_mean = mean(npvs, na.rm = TRUE)
  ))
}

# Function to run cross-validation for a specific group and risk/outcome columns
cross_validate_model_by_group <- function(data, risk_col, outcome_col, group_filter) {
  # Filter data by group if a filter is provided
  if (!is.null(group_filter)) {
    data <- data %>% filter(race == group_filter)
  }

  # Run cross-validation only if there is data left after filtering
  if (nrow(data) > 0) {
    return(cross_validate_model(data, risk_col, outcome_col, k_folds = 10))
  } else {
    return(list(
      AUC_mean = NA, Harrell_C_mean = NA, Sensitivity_mean = NA, Specificity_mean = NA,
      PPV_mean = NA, NPV_mean = NA
    ))
  }
}

# Run cross-validation for All Races, Black or African American (race == "Black or African American"), and White (race == "White")
cv_results_all <- cross_validate_model_by_group(consistent_data, "risk_2_years", "outcome_2_years", group_filter = NULL)
cv_results_black <- cross_validate_model_by_group(consistent_data, "risk_2_years", "outcome_2_years", group_filter = "Black or African American")
cv_results_white <- cross_validate_model_by_group(consistent_data, "risk_2_years", "outcome_2_years", group_filter = "White")

# Compile the results into a comparison table for both 2-year and 5-year outcomes
cv_results_table <- data.frame(
  Group = c("All Races", "Black or African American", "White"),
  AUC_2_years = c(cv_results_all$AUC_mean, cv_results_black$AUC_mean, cv_results_white$AUC_mean),
  Harrell_C_2_years = c(cv_results_all$Harrell_C_mean, cv_results_black$Harrell_C_mean, cv_results_white$Harrell_C_mean),
  Sensitivity_2_years = c(cv_results_all$Sensitivity_mean, cv_results_black$Sensitivity_mean, cv_results_white$Sensitivity_mean),
  Specificity_2_years = c(cv_results_all$Specificity_mean, cv_results_black$Specificity_mean, cv_results_white$Specificity_mean),
  PPV_2_years = c(cv_results_all$PPV_mean, cv_results_black$PPV_mean, cv_results_white$PPV_mean),
  NPV_2_years = c(cv_results_all$NPV_mean, cv_results_black$NPV_mean, cv_results_white$NPV_mean)
)

# Repeat the process for 5-year risk outcomes
cv_results_all_5 <- cross_validate_model_by_group(consistent_data, "risk_5_years", "outcome_5_years", group_filter = NULL)
cv_results_black_5 <- cross_validate_model_by_group(consistent_data, "risk_5_years", "outcome_5_years", group_filter = "Black or African American")
cv_results_white_5 <- cross_validate_model_by_group(consistent_data, "risk_5_years", "outcome_5_years", group_filter = "White")

# Add 5-year metrics to the table
cv_results_table$AUC_5_years <- c(cv_results_all_5$AUC_mean, cv_results_black_5$AUC_mean, cv_results_white_5$AUC_mean)
cv_results_table$Harrell_C_5_years <- c(cv_results_all_5$Harrell_C_mean, cv_results_black_5$Harrell_C_mean, cv_results_white_5$Harrell_C_mean)
cv_results_table$Sensitivity_5_years <- c(cv_results_all_5$Sensitivity_mean, cv_results_black_5$Sensitivity_mean, cv_results_white_5$Sensitivity_mean)
cv_results_table$Specificity_5_years <- c(cv_results_all_5$Specificity_mean, cv_results_black_5$Specificity_mean, cv_results_white_5$Specificity_mean)
cv_results_table$PPV_5_years <- c(cv_results_all_5$PPV_mean, cv_results_black_5$PPV_mean, cv_results_white_5$PPV_mean)
cv_results_table$NPV_5_years <- c(cv_results_all_5$NPV_mean, cv_results_black_5$NPV_mean, cv_results_white_5$NPV_mean)

# Add case-control counts
case_control_counts <- consistent_data %>%
  group_by(race) %>%
  summarise(
    cases = sum(disease_status == 1),
    controls = sum(disease_status == 0)
  )

# Print the final results table along with case-control counts
print(cv_results_table)
print(case_control_counts)
```



# CKD-EPI 2021
## Value calculation
```{r}
# Load necessary libraries
library(dplyr)

# Step 1: Extract relevant columns and set up the CKD-EPI 2021 equation variables
inrange_person_with_summary <- read.csv("survival_data.csv") %>%
  mutate(
    # Define the kappa and alpha parameters based on sex (CKD-EPI 2021 without race)
    kappa = ifelse(sex_at_birth == "Female", 0.7, 0.9),
    alpha = ifelse(sex_at_birth == "Female", -0.241, -0.302),
    sex_factor = ifelse(sex_at_birth == "Female", 1.012, 1),

    # Apply the CKD-EPI 2021 equation for eGFR calculation (without race)
    eGFR_ckd_epi_2021 = 142 * pmin(serum_creatinine / kappa, 1)^alpha * 
                        pmax(serum_creatinine / kappa, 1)^(-1.2) * 
                        (0.9938^age_precise) * sex_factor
  )

# Step 2: Print the number of cases and controls that have a valid eGFR_ckd_epi_2021 value
# Assuming disease_status == 1 means case and disease_status == 0 means control
valid_eGFR_counts_2021 <- inrange_person_with_summary %>%
  filter(!is.na(eGFR_ckd_epi_2021)) %>%
  group_by(disease_status) %>%
  summarise(count = n())

cat("Number of cases (disease_status == 1) with valid eGFR_ckd_epi_2021:", 
    valid_eGFR_counts_2021$count[valid_eGFR_counts_2021$disease_status == 1], "\n")
cat("Number of controls (disease_status == 0) with valid eGFR_ckd_epi_2021:", 
    valid_eGFR_counts_2021$count[valid_eGFR_counts_2021$disease_status == 0], "\n")

# Step 3: Print the summary statistics of the eGFR_ckd_epi_2021 values
summary_eGFR_2021 <- summary(inrange_person_with_summary$eGFR_ckd_epi_2021)

cat("\nSummary statistics for eGFR_ckd_epi_2021:\n")
print(summary_eGFR_2021)

# Optional: Print a few rows of the dataset to verify the eGFR values are appended
head(inrange_person_with_summary, 10)


```


## < 60: CKD Population
```{r}
# Load necessary libraries
library(dplyr)
library(survival)

# Define a plausible range for eGFR and Albumin/Creatinine
plausible_eGFR_min <- 0
plausible_eGFR_max <- 60

plausible_acr_min <- 0
plausible_acr_max <- 8000  # 8000 mg/g is a very high value, but still clinically plausible

# Step 1: Filter out implausible eGFR and Albumin/Creatinine values
person_filtered_KFRE <- inrange_person_with_summary %>%
  select(person_id, sex_at_birth, eGFR_ckd_epi_2021, Albumin_Creatinine = `Albumin.Creatinine`, 
         age_precise, race, disease_status, time_to_event)

# Step 2: Data preprocessing
person_filtered_KFRE <- person_filtered_KFRE %>%
  mutate(event = disease_status) %>%
  filter(
    eGFR_ckd_epi_2021 >= plausible_eGFR_min & eGFR_ckd_epi_2021 <= plausible_eGFR_max, # Using eGFR_ckd_epi_2021
    Albumin_Creatinine >= plausible_acr_min & Albumin_Creatinine < plausible_acr_max,
    sex_at_birth %in% c("Female", "Male")
  ) %>%
  mutate(logACR = log(Albumin_Creatinine)) %>%
  filter(complete.cases(.))

# Perform a complete case analysis
initial_row_count <- nrow(person_filtered_KFRE)

complete_cases <- person_filtered_KFRE %>%
  filter(complete.cases(.))

final_row_count <- nrow(complete_cases)

# Check if any rows have been removed
rows_removed <- initial_row_count - final_row_count

if (rows_removed > 0) {
  cat(rows_removed, "rows have been removed during the complete case analysis.\n")
} else {
  cat("No rows have been removed during the complete case analysis.\n")
}

# Report the number of complete cases
num_complete_cases <- nrow(complete_cases)
print(paste("Number of complete cases:", num_complete_cases))

# Remove any rows with NA in time_to_event or disease_status
complete_cases <- complete_cases %>%
  filter(!is.na(time_to_event) & !is.na(disease_status))

# Check the event distribution
print(table(complete_cases$disease_status))

# Define the formula for βsum using eGFR_ckd_epi_2009
calculate_beta_sum <- function(age, sex, eGFR_ckd_epi_2009, logACR) {
  -0.2201 * (age / 10 - 7.036) +
    0.2467 * (ifelse(sex == "Male", 1, 0) - 0.5642) -
    0.5567 * (eGFR_ckd_epi_2009 / 5 - 7.222) +
    0.4510 * (logACR - 5.137)
}

# Apply the formula to calculate βsum
complete_cases <- complete_cases %>%
  mutate(beta_sum = calculate_beta_sum(age_precise, sex_at_birth, eGFR_ckd_epi_2021, logACR))

# Remove any rows with infinite or NA beta_sum
complete_cases <- complete_cases %>%
  filter(is.finite(beta_sum) & !is.na(beta_sum))

# Calculate the risk for two and five years using the updated formulas
complete_cases <- complete_cases %>%
  mutate(
    risk_2_years = 100 * (1 - 0.975^exp(beta_sum)),  # Two-Year Risk calculation
    risk_5_years = 100 * (1 - 0.9240^exp(beta_sum))  # Five-Year Risk calculation
  )

# Save the data before filtering
before_filtering <- complete_cases

# Apply the filtering to create consistent_data
consistent_data <- complete_cases %>%
  filter(!is.na(risk_5_years) & !is.na(risk_2_years))

# Find the rows that were removed by comparing before and after datasets
removed_rows <- anti_join(before_filtering, consistent_data, by = "person_id")

# Print the removed rows
if (nrow(removed_rows) > 0) {
  cat("Rows that were removed during filtering:\n")
  print(removed_rows)
} else {
  cat("No rows were removed during filtering.\n")
}

# Verify the risk values
summary(consistent_data$risk_5_years)
summary(consistent_data$risk_2_years)

# Count the number of person_id entries with event == 1 and event == 0
event_count <- consistent_data %>%
  group_by(event) %>%
  summarise(count = n_distinct(person_id))

# Print the results
cat("Number of person_id with event == 1:", event_count$count[event_count$event == 1], "\n")
cat("Number of person_id with event == 0:", event_count$count[event_count$event == 0], "\n")

```
#### Model evaluation: non-coss validation
```{r}
# Load necessary libraries
library(dplyr)
library(survival)
library(pROC)
library(rms)  # For Harrell's C-statistic
library(kableExtra)

# Create 2-year and 5-year outcomes based on disease status and time to event
consistent_data <- consistent_data %>%
  mutate(
    outcome_2_years = ifelse(disease_status == 1 & time_to_event <= 2 * 365.25, 1, 0),
    outcome_5_years = ifelse(disease_status == 1 & time_to_event <= 5 * 365.25, 1, 0)
  )

# Define a function to calculate AUC, Harrell's C-statistic, and other metrics
calculate_metrics <- function(data, risk_col, outcome_col, time_col) {
  # Filter out rows with missing or invalid data
  data_filtered <- data %>% filter(!is.na(data[[outcome_col]]) & !is.na(data[[risk_col]]))

  # Ensure outcome_col is binary (0/1)
  data_filtered[[outcome_col]] <- ifelse(data_filtered[[outcome_col]] > 0, 1, 0)

  # Calculate AUC
  roc_curve <- roc(data_filtered[[outcome_col]], data_filtered[[risk_col]])
  auc_value <- as.numeric(auc(roc_curve))  # Convert AUC to numeric

  # Calculate Harrell's C-statistic using Cox proportional hazards model
  cox_model <- coxph(Surv(data_filtered[[time_col]], data_filtered[[outcome_col]]) ~ data_filtered[[risk_col]], data = data_filtered)
  c_stat <- as.numeric(summary(cox_model)$concordance[1])  # Extract C-index (Harrell's C-statistic)

  # Calculate confusion matrix statistics: Sensitivity, Specificity, PPV, NPV
  pred_class <- ifelse(data_filtered[[risk_col]] > 0.5, 1, 0)
  confusion_mat <- table(Predicted = pred_class, Actual = data_filtered[[outcome_col]])

  sensitivity <- confusion_mat[2, 2] / sum(confusion_mat[, 2])
  specificity <- confusion_mat[1, 1] / sum(confusion_mat[, 1])
  ppv <- confusion_mat[2, 2] / sum(confusion_mat[2, ])
  npv <- confusion_mat[1, 1] / sum(confusion_mat[1, ])

  return(list(
    AUC = auc_value,
    C_Statistic = c_stat,
    Sensitivity = sensitivity,
    Specificity = specificity,
    PPV = ppv,
    NPV = npv
  ))
}

# Define a function to summarize metrics for each racial group
summarize_risk_metrics <- function(data, race_label) {
  # Calculate metrics for 2-year and 5-year outcomes
  metrics_2_years <- calculate_metrics(data, "risk_2_years", "outcome_2_years", "time_to_event")
  metrics_5_years <- calculate_metrics(data, "risk_5_years", "outcome_5_years", "time_to_event")

  case_control_2yr <- table(data$outcome_2_years)
  case_control_5yr <- table(data$outcome_5_years)

  # Summarize the metrics
  data %>%
    summarise(
      Race = race_label,
      `AUC 2 Years` = metrics_2_years$AUC,
      `Harrell's C-Statistic 2 Years` = metrics_2_years$C_Statistic,
      `Sensitivity 2 Years` = metrics_2_years$Sensitivity,
      `Specificity 2 Years` = metrics_2_years$Specificity,
      `PPV 2 Years` = metrics_2_years$PPV,
      `NPV 2 Years` = metrics_2_years$NPV,
      `Cases 2 Years` = case_control_2yr["1"],
      `Controls 2 Years` = case_control_2yr["0"],
      `AUC 5 Years` = metrics_5_years$AUC,
      `Harrell's C-Statistic 5 Years` = metrics_5_years$C_Statistic,
      `Sensitivity 5 Years` = metrics_5_years$Sensitivity,
      `Specificity 5 Years` = metrics_5_years$Specificity,
      `PPV 5 Years` = metrics_5_years$PPV,
      `NPV 5 Years` = metrics_5_years$NPV,
      `Cases 5 Years` = case_control_5yr["1"],
      `Controls 5 Years` = case_control_5yr["0"]
    )
}

# Evaluate for all races
results_all <- summarize_risk_metrics(consistent_data, "All Races")

# Evaluate for Black or African American
results_black <- summarize_risk_metrics(
  consistent_data %>% filter(race == "Black or African American"),
  "Black or African American"
)

# Evaluate for White
results_white <- summarize_risk_metrics(
  consistent_data %>% filter(race == "White"),
  "White"
)

# Combine all results into one table
final_results <- bind_rows(results_all, results_black, results_white)

# Print the combined results
cat("\nCombined Risk Metrics, AUC, Harrell's C-Statistic, Sensitivity, Specificity, and Case-Control Counts:\n")
print(final_results)

# Save the final results to a CSV file
write.csv(final_results, "combined_risk_metrics_with_case_control_counts.csv", row.names = FALSE)

```
#### Model:Evaluation: Cross validation
```{r}
# Load necessary libraries
library(dplyr)
library(caret)
library(pROC)
library(survival)

# Ensure consistent_data is correctly set up with the necessary columns
consistent_data <- consistent_data %>%
  mutate(
    outcome_2_years = ifelse(disease_status == 1 & time_to_event <= 2 * 365.25, 1, 0),
    outcome_5_years = ifelse(disease_status == 1 & time_to_event <= 5 * 365.25, 1, 0)
  ) %>%
  filter(!is.na(time_to_event) & !is.na(disease_status))  # Ensure no missing values in time_to_event or disease_status

# Function to calculate metrics for model evaluation
calculate_metrics <- function(true_labels, predicted_probs, threshold = 0.5) {
  if (length(unique(true_labels)) < 2) {
    # If true_labels doesn't have two levels, we can't calculate these metrics
    warning("Fold has only one class, skipping calculations.")
    return(list(Sensitivity = NA, Specificity = NA, PPV = NA, NPV = NA))
  }
  
  predicted_classes <- ifelse(predicted_probs > threshold, 1, 0)
  
  # Create a confusion matrix
  conf_matrix <- table(Predicted = predicted_classes, Actual = true_labels)
  
  # Ensure the confusion matrix has all cells (fill missing cells with 0)
  conf_matrix <- as.table(matrix(c(conf_matrix), nrow = 2, ncol = 2, 
                                 dimnames = list(Predicted = c(0, 1), Actual = c(0, 1)), byrow = TRUE))
  
  # Extract TP, TN, FP, and FN
  TP <- conf_matrix["1", "1"]
  TN <- conf_matrix["0", "0"]
  FP <- conf_matrix["1", "0"]
  FN <- conf_matrix["0", "1"]
  
  # Calculate Sensitivity, Specificity, PPV, NPV
  sensitivity <- ifelse(TP + FN > 0, TP / (TP + FN), NA)
  specificity <- ifelse(TN + FP > 0, TN / (TN + FP), NA)
  ppv <- ifelse(TP + FP > 0, TP / (TP + FP), NA)  # Positive Predictive Value
  npv <- ifelse(TN + FN > 0, TN / (TN + FN), NA)  # Negative Predictive Value
  
  return(list(Sensitivity = sensitivity, Specificity = specificity, PPV = ppv, NPV = npv))
}

# Function to perform cross-validation and evaluate the model
cross_validate_model <- function(data, risk_col, outcome_col, k_folds = 10) {
  # Filter out any rows where risk_col or outcome_col is missing
  data <- data %>%
    filter(!is.na(.data[[risk_col]]) & !is.na(.data[[outcome_col]]))
  
  set.seed(123)  # For reproducibility
  
  # Create the folds
  folds <- createFolds(data[[outcome_col]], k = k_folds, returnTrain = FALSE)

  # Initialize vectors to store results
  aucs <- c()
  harrell_c_stats <- c()
  sensitivities <- c()
  specificities <- c()
  ppvs <- c()
  npvs <- c()
  
  for (i in 1:k_folds) {
    # Split the data into training and test sets
    fold_test <- data[folds[[i]], ]
    
    # Check if there is enough data in the fold to calculate metrics
    if (nrow(fold_test) == 0) {
      next  # Skip empty folds
    }
    
    true_labels <- fold_test[[outcome_col]]
    
    if (length(unique(true_labels)) < 2) {
      warning("Fold has only one class, skipping ROC calculation for this fold.")
      next  # Skip if there's only one class in this fold
    }
    
    predicted_probs <- fold_test[[risk_col]]
    
    # Calculate AUC
    roc_curve <- tryCatch({
      roc(true_labels, predicted_probs)
    }, error = function(e) {
      warning("Error in ROC calculation: ", e$message)
      return(NULL)
    })
    
    if (!is.null(roc_curve)) {
      auc_value <- auc(roc_curve)
      aucs <- c(aucs, auc_value)
    }
    
    # Calculate Harrell's C-statistic using Cox proportional hazards model
    cox_model <- tryCatch({
      coxph(Surv(fold_test$time_to_event, true_labels) ~ predicted_probs, data = fold_test, control = coxph.control(iter.max = 50))
    }, warning = function(w) {
      warning("Warning in CoxPH: ", w$message)
      return(NULL)
    }, error = function(e) {
      warning("Error in CoxPH: ", e$message)
      return(NULL)
    })
    
    if (!is.null(cox_model)) {
      harrell_c_stat <- summary(cox_model)$concordance[1]
      harrell_c_stats <- c(harrell_c_stats, harrell_c_stat)
    }
    
    # Calculate Sensitivity, Specificity, PPV, NPV
    metrics <- calculate_metrics(true_labels, predicted_probs)
    sensitivities <- c(sensitivities, metrics$Sensitivity)
    specificities <- c(specificities, metrics$Specificity)
    ppvs <- c(ppvs, metrics$PPV)
    npvs <- c(npvs, metrics$NPV)
  }
  
  return(list(
    AUC_mean = mean(aucs, na.rm = TRUE),
    Harrell_C_mean = mean(harrell_c_stats, na.rm = TRUE),
    Sensitivity_mean = mean(sensitivities, na.rm = TRUE),
    Specificity_mean = mean(specificities, na.rm = TRUE),
    PPV_mean = mean(ppvs, na.rm = TRUE),
    NPV_mean = mean(npvs, na.rm = TRUE)
  ))
}

# Function to run cross-validation for a specific group and risk/outcome columns
cross_validate_model_by_group <- function(data, risk_col, outcome_col, group_filter) {
  # Filter data by group if a filter is provided
  if (!is.null(group_filter)) {
    data <- data %>% filter(race == group_filter)
  }

  # Run cross-validation only if there is data left after filtering
  if (nrow(data) > 0) {
    return(cross_validate_model(data, risk_col, outcome_col, k_folds = 10))
  } else {
    return(list(
      AUC_mean = NA, Harrell_C_mean = NA, Sensitivity_mean = NA, Specificity_mean = NA,
      PPV_mean = NA, NPV_mean = NA
    ))
  }
}

# Run cross-validation for All Races, Black or African American (race == "Black or African American"), and White (race == "White")
cv_results_all <- cross_validate_model_by_group(consistent_data, "risk_2_years", "outcome_2_years", group_filter = NULL)
cv_results_black <- cross_validate_model_by_group(consistent_data, "risk_2_years", "outcome_2_years", group_filter = "Black or African American")
cv_results_white <- cross_validate_model_by_group(consistent_data, "risk_2_years", "outcome_2_years", group_filter = "White")

# Compile the results into a comparison table for both 2-year and 5-year outcomes
cv_results_table <- data.frame(
  Group = c("All Races", "Black or African American", "White"),
  AUC_2_years = c(cv_results_all$AUC_mean, cv_results_black$AUC_mean, cv_results_white$AUC_mean),
  Harrell_C_2_years = c(cv_results_all$Harrell_C_mean, cv_results_black$Harrell_C_mean, cv_results_white$Harrell_C_mean),
  Sensitivity_2_years = c(cv_results_all$Sensitivity_mean, cv_results_black$Sensitivity_mean, cv_results_white$Sensitivity_mean),
  Specificity_2_years = c(cv_results_all$Specificity_mean, cv_results_black$Specificity_mean, cv_results_white$Specificity_mean),
  PPV_2_years = c(cv_results_all$PPV_mean, cv_results_black$PPV_mean, cv_results_white$PPV_mean),
  NPV_2_years = c(cv_results_all$NPV_mean, cv_results_black$NPV_mean, cv_results_white$NPV_mean)
)

# Repeat the process for 5-year risk outcomes
cv_results_all_5 <- cross_validate_model_by_group(consistent_data, "risk_5_years", "outcome_5_years", group_filter = NULL)
cv_results_black_5 <- cross_validate_model_by_group(consistent_data, "risk_5_years", "outcome_5_years", group_filter = "Black or African American")
cv_results_white_5 <- cross_validate_model_by_group(consistent_data, "risk_5_years", "outcome_5_years", group_filter = "White")

# Add 5-year metrics to the table
cv_results_table$AUC_5_years <- c(cv_results_all_5$AUC_mean, cv_results_black_5$AUC_mean, cv_results_white_5$AUC_mean)
cv_results_table$Harrell_C_5_years <- c(cv_results_all_5$Harrell_C_mean, cv_results_black_5$Harrell_C_mean, cv_results_white_5$Harrell_C_mean)
cv_results_table$Sensitivity_5_years <- c(cv_results_all_5$Sensitivity_mean, cv_results_black_5$Sensitivity_mean, cv_results_white_5$Sensitivity_mean)
cv_results_table$Specificity_5_years <- c(cv_results_all_5$Specificity_mean, cv_results_black_5$Specificity_mean, cv_results_white_5$Specificity_mean)
cv_results_table$PPV_5_years <- c(cv_results_all_5$PPV_mean, cv_results_black_5$PPV_mean, cv_results_white_5$PPV_mean)
cv_results_table$NPV_5_years <- c(cv_results_all_5$NPV_mean, cv_results_black_5$NPV_mean, cv_results_white_5$NPV_mean)

# Add case-control counts
case_control_counts <- consistent_data %>%
  group_by(race) %>%
  summarise(
    cases = sum(disease_status == 1),
    controls = sum(disease_status == 0)
  )

# Print the final results table along with case-control counts
print(cv_results_table)
print(case_control_counts)
```


## 0-500: CKD Population
```{r}
# Load necessary libraries
library(dplyr)
library(survival)

# Define a plausible range for eGFR and Albumin/Creatinine
plausible_eGFR_min <- 0
plausible_eGFR_max <- 500

plausible_acr_min <- 0
plausible_acr_max <- 8000  # 8000 mg/g is a very high value, but still clinically plausible

# Step 1: Filter out implausible eGFR and Albumin/Creatinine values
person_filtered_KFRE <- inrange_person_with_summary %>%
  select(person_id, sex_at_birth, eGFR_ckd_epi_2021, Albumin_Creatinine = `Albumin.Creatinine`, 
         age_precise, race, disease_status, time_to_event)

# Step 2: Data preprocessing
person_filtered_KFRE <- person_filtered_KFRE %>%
  mutate(event = disease_status) %>%
  filter(
    eGFR_ckd_epi_2021 >= plausible_eGFR_min & eGFR_ckd_epi_2021 <= plausible_eGFR_max, # Using eGFR_ckd_epi_2009
    Albumin_Creatinine >= plausible_acr_min & Albumin_Creatinine <= plausible_acr_max,
    sex_at_birth %in% c("Female", "Male")
  ) %>%
  mutate(logACR = log(Albumin_Creatinine)) %>%
  filter(complete.cases(.))

# Perform a complete case analysis
initial_row_count <- nrow(person_filtered_KFRE)

complete_cases <- person_filtered_KFRE %>%
  filter(complete.cases(.))

final_row_count <- nrow(complete_cases)

# Check if any rows have been removed
rows_removed <- initial_row_count - final_row_count

if (rows_removed > 0) {
  cat(rows_removed, "rows have been removed during the complete case analysis.\n")
} else {
  cat("No rows have been removed during the complete case analysis.\n")
}

# Report the number of complete cases
num_complete_cases <- nrow(complete_cases)
print(paste("Number of complete cases:", num_complete_cases))

# Remove any rows with NA in time_to_event or disease_status
complete_cases <- complete_cases %>%
  filter(!is.na(time_to_event) & !is.na(disease_status))

# Check the event distribution
print(table(complete_cases$disease_status))

# Define the formula for βsum using eGFR_ckd_epi_2009
calculate_beta_sum <- function(age, sex, eGFR_ckd_epi_2009, logACR) {
  -0.2201 * (age / 10 - 7.036) +
    0.2467 * (ifelse(sex == "Male", 1, 0) - 0.5642) -
    0.5567 * (eGFR_ckd_epi_2009 / 5 - 7.222) +
    0.4510 * (logACR - 5.137)
}

# Apply the formula to calculate βsum
complete_cases <- complete_cases %>%
  mutate(beta_sum = calculate_beta_sum(age_precise, sex_at_birth, eGFR_ckd_epi_2021, logACR))

# Remove any rows with infinite or NA beta_sum
complete_cases <- complete_cases %>%
  filter(is.finite(beta_sum) & !is.na(beta_sum))

# Calculate the risk for two and five years using the updated formulas
complete_cases <- complete_cases %>%
  mutate(
    risk_2_years = 100 * (1 - 0.975^exp(beta_sum)),  # Two-Year Risk calculation
    risk_5_years = 100 * (1 - 0.9240^exp(beta_sum))  # Five-Year Risk calculation
  )

# Save the data before filtering
before_filtering <- complete_cases

# Apply the filtering to create consistent_data
consistent_data <- complete_cases %>%
  filter(!is.na(risk_5_years) & !is.na(risk_2_years))

# Find the rows that were removed by comparing before and after datasets
removed_rows <- anti_join(before_filtering, consistent_data, by = "person_id")

# Print the removed rows
if (nrow(removed_rows) > 0) {
  cat("Rows that were removed during filtering:\n")
  print(removed_rows)
} else {
  cat("No rows were removed during filtering.\n")
}

# Verify the risk values
summary(consistent_data$risk_5_years)
summary(consistent_data$risk_2_years)

# Count the number of person_id entries with event == 1 and event == 0
event_count <- consistent_data %>%
  group_by(event) %>%
  summarise(count = n_distinct(person_id))

# Print the results
cat("Number of person_id with event == 1:", event_count$count[event_count$event == 1], "\n")
cat("Number of person_id with event == 0:", event_count$count[event_count$event == 0], "\n")

```

```{r}
# Load necessary libraries
library(dplyr)
library(survival)
library(pROC)
library(rms)  # For Harrell's C-statistic
library(kableExtra)

# Create 2-year and 5-year outcomes based on disease status and time to event
consistent_data <- consistent_data %>%
  mutate(
    outcome_2_years = ifelse(disease_status == 1 & time_to_event <= 2 * 365.25, 1, 0),
    outcome_5_years = ifelse(disease_status == 1 & time_to_event <= 5 * 365.25, 1, 0)
  )

# Define a function to calculate AUC, Harrell's C-statistic, and other metrics
calculate_metrics <- function(data, risk_col, outcome_col, time_col) {
  # Filter out rows with missing or invalid data
  data_filtered <- data %>% filter(!is.na(data[[outcome_col]]) & !is.na(data[[risk_col]]))

  # Ensure outcome_col is binary (0/1)
  data_filtered[[outcome_col]] <- ifelse(data_filtered[[outcome_col]] > 0, 1, 0)

  # Calculate AUC
  roc_curve <- roc(data_filtered[[outcome_col]], data_filtered[[risk_col]])
  auc_value <- as.numeric(auc(roc_curve))  # Convert AUC to numeric

  # Calculate Harrell's C-statistic using Cox proportional hazards model
  cox_model <- coxph(Surv(data_filtered[[time_col]], data_filtered[[outcome_col]]) ~ data_filtered[[risk_col]], data = data_filtered)
  c_stat <- as.numeric(summary(cox_model)$concordance[1])  # Extract C-index (Harrell's C-statistic)

  # Calculate confusion matrix statistics: Sensitivity, Specificity, PPV, NPV
  pred_class <- ifelse(data_filtered[[risk_col]] > 0.5, 1, 0)
  confusion_mat <- table(Predicted = pred_class, Actual = data_filtered[[outcome_col]])

  sensitivity <- confusion_mat[2, 2] / sum(confusion_mat[, 2])
  specificity <- confusion_mat[1, 1] / sum(confusion_mat[, 1])
  ppv <- confusion_mat[2, 2] / sum(confusion_mat[2, ])
  npv <- confusion_mat[1, 1] / sum(confusion_mat[1, ])

  return(list(
    AUC = auc_value,
    C_Statistic = c_stat,
    Sensitivity = sensitivity,
    Specificity = specificity,
    PPV = ppv,
    NPV = npv
  ))
}

# Define a function to summarize metrics for each racial group
summarize_risk_metrics <- function(data, race_label) {
  # Calculate metrics for 2-year and 5-year outcomes
  metrics_2_years <- calculate_metrics(data, "risk_2_years", "outcome_2_years", "time_to_event")
  metrics_5_years <- calculate_metrics(data, "risk_5_years", "outcome_5_years", "time_to_event")

  case_control_2yr <- table(data$outcome_2_years)
  case_control_5yr <- table(data$outcome_5_years)

  # Summarize the metrics
  data %>%
    summarise(
      Race = race_label,
      `AUC 2 Years` = metrics_2_years$AUC,
      `Harrell's C-Statistic 2 Years` = metrics_2_years$C_Statistic,
      `Sensitivity 2 Years` = metrics_2_years$Sensitivity,
      `Specificity 2 Years` = metrics_2_years$Specificity,
      `PPV 2 Years` = metrics_2_years$PPV,
      `NPV 2 Years` = metrics_2_years$NPV,
      `Cases 2 Years` = case_control_2yr["1"],
      `Controls 2 Years` = case_control_2yr["0"],
      `AUC 5 Years` = metrics_5_years$AUC,
      `Harrell's C-Statistic 5 Years` = metrics_5_years$C_Statistic,
      `Sensitivity 5 Years` = metrics_5_years$Sensitivity,
      `Specificity 5 Years` = metrics_5_years$Specificity,
      `PPV 5 Years` = metrics_5_years$PPV,
      `NPV 5 Years` = metrics_5_years$NPV,
      `Cases 5 Years` = case_control_5yr["1"],
      `Controls 5 Years` = case_control_5yr["0"]
    )
}

# Evaluate for all races
results_all <- summarize_risk_metrics(consistent_data, "All Races")

# Evaluate for Black or African American
results_black <- summarize_risk_metrics(
  consistent_data %>% filter(race == "Black or African American"),
  "Black or African American"
)

# Evaluate for White
results_white <- summarize_risk_metrics(
  consistent_data %>% filter(race == "White"),
  "White"
)

# Combine all results into one table
final_results <- bind_rows(results_all, results_black, results_white)

# Print the combined results
cat("\nCombined Risk Metrics, AUC, Harrell's C-Statistic, Sensitivity, Specificity, and Case-Control Counts:\n")
print(final_results)

# Save the final results to a CSV file
write.csv(final_results, "combined_risk_metrics_with_case_control_counts.csv", row.names = FALSE)

```

```{r}
# Load necessary libraries
library(dplyr)
library(caret)
library(pROC)
library(survival)

# Ensure consistent_data is correctly set up with the necessary columns
consistent_data <- consistent_data %>%
  mutate(
    outcome_2_years = ifelse(disease_status == 1 & time_to_event <= 2 * 365.25, 1, 0),
    outcome_5_years = ifelse(disease_status == 1 & time_to_event <= 5 * 365.25, 1, 0)
  ) %>%
  filter(!is.na(time_to_event) & !is.na(disease_status))  # Ensure no missing values in time_to_event or disease_status

# Function to calculate metrics for model evaluation
calculate_metrics <- function(true_labels, predicted_probs, threshold = 0.5) {
  if (length(unique(true_labels)) < 2) {
    # If true_labels doesn't have two levels, we can't calculate these metrics
    warning("Fold has only one class, skipping calculations.")
    return(list(Sensitivity = NA, Specificity = NA, PPV = NA, NPV = NA))
  }
  
  predicted_classes <- ifelse(predicted_probs > threshold, 1, 0)
  
  # Create a confusion matrix
  conf_matrix <- table(Predicted = predicted_classes, Actual = true_labels)
  
  # Ensure the confusion matrix has all cells (fill missing cells with 0)
  conf_matrix <- as.table(matrix(c(conf_matrix), nrow = 2, ncol = 2, 
                                 dimnames = list(Predicted = c(0, 1), Actual = c(0, 1)), byrow = TRUE))
  
  # Extract TP, TN, FP, and FN
  TP <- conf_matrix["1", "1"]
  TN <- conf_matrix["0", "0"]
  FP <- conf_matrix["1", "0"]
  FN <- conf_matrix["0", "1"]
  
  # Calculate Sensitivity, Specificity, PPV, NPV
  sensitivity <- ifelse(TP + FN > 0, TP / (TP + FN), NA)
  specificity <- ifelse(TN + FP > 0, TN / (TN + FP), NA)
  ppv <- ifelse(TP + FP > 0, TP / (TP + FP), NA)  # Positive Predictive Value
  npv <- ifelse(TN + FN > 0, TN / (TN + FN), NA)  # Negative Predictive Value
  
  return(list(Sensitivity = sensitivity, Specificity = specificity, PPV = ppv, NPV = npv))
}

# Function to perform cross-validation and evaluate the model
cross_validate_model <- function(data, risk_col, outcome_col, k_folds = 10) {
  # Filter out any rows where risk_col or outcome_col is missing
  data <- data %>%
    filter(!is.na(.data[[risk_col]]) & !is.na(.data[[outcome_col]]))
  
  set.seed(123)  # For reproducibility
  
  # Create the folds
  folds <- createFolds(data[[outcome_col]], k = k_folds, returnTrain = FALSE)

  # Initialize vectors to store results
  aucs <- c()
  harrell_c_stats <- c()
  sensitivities <- c()
  specificities <- c()
  ppvs <- c()
  npvs <- c()
  
  for (i in 1:k_folds) {
    # Split the data into training and test sets
    fold_test <- data[folds[[i]], ]
    
    # Check if there is enough data in the fold to calculate metrics
    if (nrow(fold_test) == 0) {
      next  # Skip empty folds
    }
    
    true_labels <- fold_test[[outcome_col]]
    
    if (length(unique(true_labels)) < 2) {
      warning("Fold has only one class, skipping ROC calculation for this fold.")
      next  # Skip if there's only one class in this fold
    }
    
    predicted_probs <- fold_test[[risk_col]]
    
    # Calculate AUC
    roc_curve <- tryCatch({
      roc(true_labels, predicted_probs)
    }, error = function(e) {
      warning("Error in ROC calculation: ", e$message)
      return(NULL)
    })
    
    if (!is.null(roc_curve)) {
      auc_value <- auc(roc_curve)
      aucs <- c(aucs, auc_value)
    }
    
    # Calculate Harrell's C-statistic using Cox proportional hazards model
    cox_model <- tryCatch({
      coxph(Surv(fold_test$time_to_event, true_labels) ~ predicted_probs, data = fold_test, control = coxph.control(iter.max = 50))
    }, warning = function(w) {
      warning("Warning in CoxPH: ", w$message)
      return(NULL)
    }, error = function(e) {
      warning("Error in CoxPH: ", e$message)
      return(NULL)
    })
    
    if (!is.null(cox_model)) {
      harrell_c_stat <- summary(cox_model)$concordance[1]
      harrell_c_stats <- c(harrell_c_stats, harrell_c_stat)
    }
    
    # Calculate Sensitivity, Specificity, PPV, NPV
    metrics <- calculate_metrics(true_labels, predicted_probs)
    sensitivities <- c(sensitivities, metrics$Sensitivity)
    specificities <- c(specificities, metrics$Specificity)
    ppvs <- c(ppvs, metrics$PPV)
    npvs <- c(npvs, metrics$NPV)
  }
  
  return(list(
    AUC_mean = mean(aucs, na.rm = TRUE),
    Harrell_C_mean = mean(harrell_c_stats, na.rm = TRUE),
    Sensitivity_mean = mean(sensitivities, na.rm = TRUE),
    Specificity_mean = mean(specificities, na.rm = TRUE),
    PPV_mean = mean(ppvs, na.rm = TRUE),
    NPV_mean = mean(npvs, na.rm = TRUE)
  ))
}

# Function to run cross-validation for a specific group and risk/outcome columns
cross_validate_model_by_group <- function(data, risk_col, outcome_col, group_filter) {
  # Filter data by group if a filter is provided
  if (!is.null(group_filter)) {
    data <- data %>% filter(race == group_filter)
  }

  # Run cross-validation only if there is data left after filtering
  if (nrow(data) > 0) {
    return(cross_validate_model(data, risk_col, outcome_col, k_folds = 10))
  } else {
    return(list(
      AUC_mean = NA, Harrell_C_mean = NA, Sensitivity_mean = NA, Specificity_mean = NA,
      PPV_mean = NA, NPV_mean = NA
    ))
  }
}

# Run cross-validation for All Races, Black or African American (race == "Black or African American"), and White (race == "White")
cv_results_all <- cross_validate_model_by_group(consistent_data, "risk_2_years", "outcome_2_years", group_filter = NULL)
cv_results_black <- cross_validate_model_by_group(consistent_data, "risk_2_years", "outcome_2_years", group_filter = "Black or African American")
cv_results_white <- cross_validate_model_by_group(consistent_data, "risk_2_years", "outcome_2_years", group_filter = "White")

# Compile the results into a comparison table for both 2-year and 5-year outcomes
cv_results_table <- data.frame(
  Group = c("All Races", "Black or African American", "White"),
  AUC_2_years = c(cv_results_all$AUC_mean, cv_results_black$AUC_mean, cv_results_white$AUC_mean),
  Harrell_C_2_years = c(cv_results_all$Harrell_C_mean, cv_results_black$Harrell_C_mean, cv_results_white$Harrell_C_mean),
  Sensitivity_2_years = c(cv_results_all$Sensitivity_mean, cv_results_black$Sensitivity_mean, cv_results_white$Sensitivity_mean),
  Specificity_2_years = c(cv_results_all$Specificity_mean, cv_results_black$Specificity_mean, cv_results_white$Specificity_mean),
  PPV_2_years = c(cv_results_all$PPV_mean, cv_results_black$PPV_mean, cv_results_white$PPV_mean),
  NPV_2_years = c(cv_results_all$NPV_mean, cv_results_black$NPV_mean, cv_results_white$NPV_mean)
)

# Repeat the process for 5-year risk outcomes
cv_results_all_5 <- cross_validate_model_by_group(consistent_data, "risk_5_years", "outcome_5_years", group_filter = NULL)
cv_results_black_5 <- cross_validate_model_by_group(consistent_data, "risk_5_years", "outcome_5_years", group_filter = "Black or African American")
cv_results_white_5 <- cross_validate_model_by_group(consistent_data, "risk_5_years", "outcome_5_years", group_filter = "White")

# Add 5-year metrics to the table
cv_results_table$AUC_5_years <- c(cv_results_all_5$AUC_mean, cv_results_black_5$AUC_mean, cv_results_white_5$AUC_mean)
cv_results_table$Harrell_C_5_years <- c(cv_results_all_5$Harrell_C_mean, cv_results_black_5$Harrell_C_mean, cv_results_white_5$Harrell_C_mean)
cv_results_table$Sensitivity_5_years <- c(cv_results_all_5$Sensitivity_mean, cv_results_black_5$Sensitivity_mean, cv_results_white_5$Sensitivity_mean)
cv_results_table$Specificity_5_years <- c(cv_results_all_5$Specificity_mean, cv_results_black_5$Specificity_mean, cv_results_white_5$Specificity_mean)
cv_results_table$PPV_5_years <- c(cv_results_all_5$PPV_mean, cv_results_black_5$PPV_mean, cv_results_white_5$PPV_mean)
cv_results_table$NPV_5_years <- c(cv_results_all_5$NPV_mean, cv_results_black_5$NPV_mean, cv_results_white_5$NPV_mean)

# Add case-control counts
case_control_counts <- consistent_data %>%
  group_by(race) %>%
  summarise(
    cases = sum(disease_status == 1),
    controls = sum(disease_status == 0)
  )

# Print the final results table along with case-control counts
print(cv_results_table)
print(case_control_counts)
```

# Part 2
Add all eGFR from all three models
```{r}
# Load necessary libraries
library(dplyr)

# Load the dataset
inrange_person_with_summary <- read.csv("survival_data.csv")

# Rename original eGFR to eGFR_mdrd
inrange_person_with_summary <- inrange_person_with_summary %>%
  rename(eGFR_mdrd = eGFR_value)  # Assuming eGFR_value is the column name for MDRD

# Step 1: Extract relevant columns and set up the CKD-EPI 2009 equation variables
inrange_person_with_summary <- inrange_person_with_summary %>%
  mutate(
    # Define the kappa and alpha parameters based on sex for CKD-EPI 2009
    kappa_2009 = ifelse(sex_at_birth == "Female", 0.7, 0.9),
    alpha_2009 = ifelse(sex_at_birth == "Female", -0.329, -0.411),
    sex_factor_2009 = ifelse(sex_at_birth == "Female", 1.018, 1),

    # Define the race factor (1 for White, 1.159 for Black or African American) for CKD-EPI 2009
    race_factor = ifelse(race == "Black or African American", 1.159, 1),

    # Apply the CKD-EPI 2009 equation for eGFR calculation
    eGFR_ckd_epi_2009 = 141 * pmin(serum_creatinine / kappa_2009, 1)^alpha_2009 * 
                        pmax(serum_creatinine / kappa_2009, 1)^(-1.209) * 
                        (0.993^age_precise) * sex_factor_2009 * race_factor
  )

# Step 2: Add CKD-EPI 2021 eGFR calculation (without race)
inrange_person_with_summary <- inrange_person_with_summary %>%
  mutate(
    # Define the kappa and alpha parameters based on sex for CKD-EPI 2021 without race
    kappa_2021 = ifelse(sex_at_birth == "Female", 0.7, 0.9),
    alpha_2021 = ifelse(sex_at_birth == "Female", -0.241, -0.302),
    sex_factor_2021 = ifelse(sex_at_birth == "Female", 1.012, 1),

    # Apply the CKD-EPI 2021 equation for eGFR calculation (without race)
    eGFR_ckd_epi_2021 = 142 * pmin(serum_creatinine / kappa_2021, 1)^alpha_2021 * 
                        pmax(serum_creatinine / kappa_2021, 1)^(-1.2) * 
                        (0.9938^age_precise) * sex_factor_2021
  )

# Step 3: Save the full dataset with the new eGFR_ckd_epi_2009 and eGFR_ckd_epi_2021 columns
ml_data <- inrange_person_with_summary

# Save the dataset to a CSV file
write.csv(ml_data, "ml_data.csv", row.names = FALSE)

# Step 4: Print the number of cases and controls that have a valid eGFR_mdrd, eGFR_ckd_epi_2009, and eGFR_ckd_epi_2021 value
valid_eGFR_counts <- inrange_person_with_summary %>%
  filter(!is.na(eGFR_mdrd) & !is.na(eGFR_ckd_epi_2009) & !is.na(eGFR_ckd_epi_2021)) %>%
  group_by(disease_status) %>%
  summarise(count = n())

cat("Number of cases (disease_status == 1) with valid eGFR_mdrd, eGFR_ckd_epi_2009, and eGFR_ckd_epi_2021:", 
    valid_eGFR_counts$count[valid_eGFR_counts$disease_status == 1], "\n")
cat("Number of controls (disease_status == 0) with valid eGFR_mdrd, eGFR_ckd_epi_2009, and eGFR_ckd_epi_2021:", 
    valid_eGFR_counts$count[valid_eGFR_counts$disease_status == 0], "\n")

# Step 5: Print the summary statistics of the eGFR_mdrd, eGFR_ckd_epi_2009, and eGFR_ckd_epi_2021 values
summary_eGFR_mdrd <- summary(inrange_person_with_summary$eGFR_mdrd)
summary_eGFR_2009 <- summary(inrange_person_with_summary$eGFR_ckd_epi_2009)
summary_eGFR_2021 <- summary(inrange_person_with_summary$eGFR_ckd_epi_2021)

cat("\nSummary statistics for eGFR_mdrd:\n")
print(summary_eGFR_mdrd)

cat("\nSummary statistics for eGFR_ckd_epi_2009:\n")
print(summary_eGFR_2009)

cat("\nSummary statistics for eGFR_ckd_epi_2021:\n")
print(summary_eGFR_2021)

# Optional: Print a few rows of the dataset to verify the eGFR values are appended
head(inrange_person_with_summary, 10)


```


## Remove Features with count less than 10

```{r}
# Load necessary libraries
library(dplyr)
library(tidyr)

person_with_summary <- read.csv("ml_data.csv")


person_with_summary <- person_with_summary %>%
  select(-age_reported,
        -`eGFR_measurement_type`, -eGFR_group, -kappa_2009, -kappa_2021,
        -Glomerular.filtration.rate.1.73.sq.M.predicted..Volume.Rate.Area..in.Serum..Plasma.or.Blood.by.Creatinine.based.formula..MDRD., -alpha_2009, -alpha_2021, -sex_factor_2009, -sex_factor_2021, -race_factor)


# # Convert 'race' and 'eGFR_group' to factors in the 'person_with_summary' dataset
# person_with_summary <- person_with_summary %>%
#   mutate(
#     race = factor(race),
#     eGFR_group = factor(eGFR_group)
#   )


# Step 3: View the updated dataset to verify the changes
#print(colnames(person_with_summary))

# Filter out non-numeric columns and pivot longer
person_long <- person_with_summary %>%
  select(person_id, disease_status, where(is.numeric)) %>%
  pivot_longer(
    cols = -c(person_id, disease_status),
    names_to = "variable",
    values_to = "value"
  ) %>%
  filter(!is.na(value) & value != 0)  # Remove rows where the value is NA or zero


# Count occurrences of disease_status for each variable
status_counts <- person_long %>%
  group_by(variable, disease_status) %>%
  summarise(count = n_distinct(person_id), .groups = 'drop')


# Pivot the summary table to a wide format
summary_wide <- status_counts %>%
  pivot_wider(
    names_from = disease_status,
    values_from = count,
    names_prefix = "disease_status_",
    values_fill = list(count = 0)
  )


# Filter out variables that have less than a count of 10 for either disease_status_0 or disease_status_1
filtered_summary <- summary_wide %>%
  filter(disease_status_0 >= 10 & disease_status_1 >= 10)

# Extract the variable names that passed the filter
remaining_variables <- filtered_summary$variable

#print(remaining_variables)

# Retain only the person_id, disease_status, and the remaining variables in the original person_with_summary dataset
filtered_person_with_summary <- person_with_summary %>%
  select(person_id, disease_status, race, gender, sex_at_birth, ethnicity,  all_of(remaining_variables))

# Print the filtered person_with_summary dataset
#print(filtered_person_with_summary)

# Identify the dropped variables
all_variables <- names(select(person_with_summary, where(is.numeric)))
dropped_variables <- setdiff(all_variables, remaining_variables)


#print(dropped_variables)

colnames(filtered_person_with_summary)
```

### Rename variables

```{r}
filtered_person_with_summary <- filtered_person_with_summary %>%
  rename('gouty arthropathy' = 'PheCode..274.11',                                                                                                               
          'iron deficiency anemias' = 'PheCode..280',                                                                                                               
          'iron deficiency anemias, unspecified or not due to blood loss' = 'PheCode..280.1',                                                                                                    
          'septicemia' = 'PheCode.038',                                                                                                                   
          'bacteremia' = 'PheCode.038.3',                                                                                                                 
          'hypothyroidism' = 'PheCode.244',                                                                                                                   
          'acquired hypothyroidism' = 'PheCode.244.2',                                                                                                                 
          'secondary diabetes mellitus' = 'PheCode.249',                                                                                                                   
          'diabetes mellitus' = 'PheCode.250',                                                                                                                   
          'type 1 diabetes' = 'PheCode.250.1',                                                                                                                 
          'type 1 diabetes with renal manifestations' = 'PheCode.250.12',                                                                                                                
          'type 2 diabetes' = 'PheCode.250.2',                                                                                                                 
          'type 2 diabetes with renal manifestations' = 'PheCode.250.22',                                                                                                                
          'type 2 diabetes with ophthalmic manifestations' = 'PheCode.250.23',                                                                                                                
          'diabetes type 2 with peripheral circulatory disorders' = 'PheCode.250.25',                                                                                                                
          'polyneuropathy in diabetes' = 'PheCode.250.6',                                                                                                                 
          'diabetic retinopathy' = 'PheCode.250.7',                                                                                                                 
          'hypoglycemia' = 'PheCode.251.1',                                                                                                                 
          'hyperparathyroidism' = 'PheCode.252.1',                                                                                                                 
          'proteinuria' = 'PheCode.269',                                                                                                                   
          'other specified disorders of plasma protein metabolism' = 'PheCode.270.38',                                                                                                                
          'gout' = 'PheCode.274.1',                                                                                                                 
          'hypocalcemia' = 'PheCode.275.51',                                                                                                                
          'disorders of phosphorus metabolism' = 'PheCode.275.53',                                                                                                                
          'hyperpotassemia' = 'PheCode.276.13',                                                                                                                
          'acidosis' = 'PheCode.276.41',                                                                                                                
          'fluid overload' = 'PheCode.276.6',                                                                                                                 
          'other anemias' = 'PheCode.285',                                                                                                                   
          'anemia of chronic disease' = 'PheCode.285.2',                                                                                                                 
          'anemia in chronic kidney disease' = 'PheCode.285.21',                                                                                                                
          'altered mental status' = 'PheCode.292.4',                                                                                                                 
          'chronic pain syndrome' = 'PheCode.355.1',                                                                                                                 
          'myopathy' = 'PheCode.359.2',                                                                                                                 
          'essential hypertension' = 'PheCode.401.1',                                                                                                                 
          'hypertensive heart and/or renal disease' = 'PheCode.401.2',                                                                                                                 
          'hypertensive chronic kidney disease' = 'PheCode.401.22',                                                                                                                
          'other hypertensive complications' = 'PheCode.401.3',                                                                                                                 
          'congestive heart failure; nonhypertensive' = 'PheCode.428',                                                                                                                   
          'atherosclerosis of the extremities' = 'PheCode.440.2',                                                                                                                 
          'atherosclerosis of native arteries of the extremities with ulceration or gangrene' = 'PheCode.440.21',                                                                                                                
          'peripheral vascular disease, unspecified' = 'PheCode.443.9',                                                                                                                 
          'other disorders of arteries and arterioles' = 'PheCode.447',                                                                                                                   
          'chronic venous insufficiency [cvi]' = 'PheCode.456',                                                                                                                   
          'hypotension nos' = 'PheCode.458.9',                                                                                                                 
          'other disorders of circulatory system' = 'PheCode.459',                                                                                                                   
          'respiratory failure' = 'PheCode.509.1',                                                                                                                 
          'diseases of hard tissues of teeth' = 'PheCode.521',                                                                                                                   
          'non-proliferative glomerulonephritis' = 'PheCode.580.12',                                                                                                                
          'nephrotic syndrome without mention of glomerulonephritis' = 'PheCode.580.2',                                                                                                                 
          'nephritis and nephropathy without mention of glomerulonephritis' = 'PheCode.580.3',                                                                                                                 
          'nephritis and nephropathy in diseases classified elsewhere' = 'PheCode.580.31',                                                                                                                
          'nephritis and nephropathy with pathological lesion' = 'PheCode.580.32',                                                                                                                
          'renal failure' = 'PheCode.585',                                                                                                                   
          'acute renal failure' = 'PheCode.585.1',                                                                                                                 
          'renal failure nos' = 'PheCode.585.2',                                                                                                                 
          'chronic kidney disease, stage iii' = 'PheCode.585.33',                                                                                                                
          'chronic kidney disease, stage iv' = 'PheCode.585.34',                                                                                                                
          'chronic kidney disease, stage i or ii' = 'PheCode.585.4',                                                                                                                 
          'anatomical abnormatilies of kidney and ureters' = 'PheCode.586.1',                                                                                                                 
          'kidney replaced by transplant' = 'PheCode.587',                                                                                                                   
          'disorders resulting from impaired renal function' = 'PheCode.588',                                                                                                                   
          'secondary hyperparathyroidism (of renal origin)' = 'PheCode.588.2',                                                                                                                 
          'abnormal results of function study of kidney' = 'PheCode.589',                                                                                                                   
          'gross hematuria' = 'PheCode.593.1',                                                                                                                 
          'hydronephrosis' = 'PheCode.595',                                                                                                                   
          'disorders of penis' = 'PheCode.604',                                                                                                                   
          'changes in skin texture' = 'PheCode.687.3',                                                                                                                 
          'lupus (localized and systemic)' = 'PheCode.695.4',                                                                                                                 
          'systemic lupus erythematosus' = 'PheCode.695.42',                                                                                                                
          'degenerative skin disorders' = 'PheCode.702.4',                                                                                                                 
          'sepsis' = 'PheCode.994.2',                                                                                                                 
          'septic shock' = 'PheCode.994.21' 
  )

colnames(filtered_person_with_summary)
```

### cleaning data

```{r}
# Remove specified columns from the dataset

filtered_person_with_summary <- filtered_person_with_summary %>%
  #select(-Albumin.Creatinine__in_Urine_value, -gender) %>%
  rename(Smoking = highest_smoking_status_rank)

# Verify the columns have been removed
colnames(filtered_person_with_summary)

 

```

# Check for multicolinearity

```{r}
# Load necessary libraries
library(dplyr)
library(corrr)
library(knitr)
library(kableExtra)

# Assuming filtered_person_with_summary is the dataset after filtration

# Step 1: Ensure that only numeric columns with variance are checked for correlation
filtered_data_no_constant <- filtered_person_with_summary %>%
  select(-person_id, -disease_status) %>%
  select(where(is.numeric)) %>%  # Ensure we are only dealing with numeric columns
  select_if(~ sd(., na.rm = TRUE) > 0)  # Ensure non-zero standard deviation

# Step 2: Calculate the correlation matrix for the filtered dataset
correlation_matrix <- filtered_data_no_constant %>%
  correlate(method = "pearson", use = "pairwise.complete.obs")

# Step 3: Identify pairs of variables with high correlations (e.g., |correlation| > 0.9)
highly_correlated_pairs <- correlation_matrix %>%
  stretch() %>%  # Convert the correlation matrix into a long format
  filter(abs(r) > 0.9, r != 1) %>%  # Keep only pairs with |correlation| > 0.9 and not self-correlation
  mutate(pair = paste(pmin(x, y), pmax(x, y), sep = "_")) %>%  # Create a unique identifier for pairs
  distinct(pair, .keep_all = TRUE) %>%  # Remove duplicates (e.g., (x,y) and (y,x))
  select(-pair)  # Remove the auxiliary 'pair' column

# Step 4: Prepare the table for the number of observations for each correlated pair
if (nrow(highly_correlated_pairs) > 0) {
  result_table <- data.frame(
    `Variable 1` = character(),
    `Variable 2` = character(),
    `Correlation` = numeric(),
    `Cases (Var 1)` = integer(),
    `Controls (Var 1)` = integer(),
    `Cases (Var 2)` = integer(),
    `Controls (Var 2)` = integer(),
    stringsAsFactors = FALSE
  )
  
  for (i in 1:nrow(highly_correlated_pairs)) {
    var1 <- highly_correlated_pairs$x[i]
    var2 <- highly_correlated_pairs$y[i]
    correlation <- highly_correlated_pairs$r[i]
    
    # Number of non-zero cases (disease_status == 1) and non-zero controls (disease_status == 0) for var1
    cases_var1 <- sum(filtered_person_with_summary$disease_status == 1 & 
                      filtered_person_with_summary[[var1]] != 0 & !is.na(filtered_person_with_summary[[var1]]))
    controls_var1 <- sum(filtered_person_with_summary$disease_status == 0 & 
                         filtered_person_with_summary[[var1]] != 0 & !is.na(filtered_person_with_summary[[var1]]))
    
    # Number of non-zero cases (disease_status == 1) and non-zero controls (disease_status == 0) for var2
    cases_var2 <- sum(filtered_person_with_summary$disease_status == 1 & 
                      filtered_person_with_summary[[var2]] != 0 & !is.na(filtered_person_with_summary[[var2]]))
    controls_var2 <- sum(filtered_person_with_summary$disease_status == 0 & 
                         filtered_person_with_summary[[var2]] != 0 & !is.na(filtered_person_with_summary[[var2]]))
    
    # Add the results to the table
    result_table <- rbind(result_table, data.frame(
      `Variable 1` = var1,
      `Variable 2` = var2,
      `Correlation` = round(correlation, 2),  # Round correlation to 2 decimal places
      `Cases (Var 1)` = cases_var1,
      `Controls (Var 1)` = controls_var1,
      `Cases (Var 2)` = cases_var2,
      `Controls (Var 2)` = controls_var2
    ))
  }
  
  # Print the result table with formatting for publication
  print(result_table)
} else {
  cat("No highly correlated variable pairs found (|correlation| > 0.9).\n")
}

# Print the column names of the original dataset
print(colnames(filtered_person_with_summary))

```

Remove correlated data

```{r}
filtered_person_with_summary <- filtered_person_with_summary %>% select(
                                       -'Microalbumin.Creatinine__in_Urine_value',
                                       -'Nephrectomy',
                                       -'Iron.binding.capacity..Mass.volume..in.Serum.or.Plasma',
                                       -'iron deficiency anemias',
                                       -'septicemia',
                                       -'diabetes mellitus',
                                       -'systemic lupus erythematosus',
                                       -'epoetin.alfa',
                                       -'Erythrocyte.distribution.width..Ratio..by.Automated.count'
                                      )
```

### Recheck for multicolinearity

```{r}
# Load necessary libraries
library(dplyr)
library(corrr)
library(knitr)
library(kableExtra)

# Assuming filtered_person_with_summary is the dataset after filtration

# Step 1: Ensure that only numeric columns are checked for variance
filtered_data_no_constant <- filtered_person_with_summary %>%
  select(-person_id, -disease_status) %>%
  select(where(is.numeric)) %>%  # Ensure we are only dealing with numeric columns
  select_if(~ !all(is.na(.)) && sd(., na.rm = TRUE) > 0)  # Ensure non-zero standard deviation, no NA values, and proper data type

# Step 2: Calculate the correlation matrix for the filtered dataset
correlation_matrix <- filtered_data_no_constant %>%
  correlate(method = "pearson", use = "pairwise.complete.obs")

# Step 3: Identify pairs of variables with high correlations (e.g., |correlation| > 0.9)
highly_correlated_pairs <- correlation_matrix %>%
  stretch() %>%  # Convert the correlation matrix into a long format
  filter(abs(r) > 0.9, r != 1)  # Keep only pairs with |correlation| > 0.9 and not self-correlation

# Step 4: Prepare the table for the number of observations for each correlated pair
if (nrow(highly_correlated_pairs) > 0) {
  result_table <- data.frame(
    `Variable 1` = character(),
    `Variable 2` = character(),
    `Correlation` = numeric(),
    `Cases (Var 1)` = integer(),
    `Controls (Var 1)` = integer(),
    `Cases (Var 2)` = integer(),
    `Controls (Var 2)` = integer(),
    stringsAsFactors = FALSE
  )
  
  for (i in 1:nrow(highly_correlated_pairs)) {
    var1 <- highly_correlated_pairs$x[i]
    var2 <- highly_correlated_pairs$y[i]
    correlation <- highly_correlated_pairs$r[i]
    
    # Number of non-zero cases (disease_status == 1) and non-zero controls (disease_status == 0) for var1
    cases_var1 <- sum(filtered_person_with_summary$disease_status == 1 & 
                      filtered_person_with_summary[[var1]] != 0 & !is.na(filtered_person_with_summary[[var1]]))
    controls_var1 <- sum(filtered_person_with_summary$disease_status == 0 & 
                         filtered_person_with_summary[[var1]] != 0 & !is.na(filtered_person_with_summary[[var1]]))
    
    # Number of non-zero cases (disease_status == 1) and non-zero controls (disease_status == 0) for var2
    cases_var2 <- sum(filtered_person_with_summary$disease_status == 1 & 
                      filtered_person_with_summary[[var2]] != 0 & !is.na(filtered_person_with_summary[[var2]]))
    controls_var2 <- sum(filtered_person_with_summary$disease_status == 0 & 
                         filtered_person_with_summary[[var2]] != 0 & !is.na(filtered_person_with_summary[[var2]]))
    
    # Add the results to the table
    result_table <- rbind(result_table, data.frame(
      `Variable 1` = var1,
      `Variable 2` = var2,
      `Correlation` = round(correlation, 2),  # Round correlation to 2 decimal places
      `Cases (Var 1)` = cases_var1,
      `Controls (Var 1)` = controls_var1,
      `Cases (Var 2)` = cases_var2,
      `Controls (Var 2)` = controls_var2
    ))
  }
  
  # Print the result table with formatting for publication
  print(result_table)
} else {
  cat("No highly correlated variable pairs found (|correlation| > 0.9).\n")
}

# Print the column names of the original dataset
print(colnames(filtered_person_with_summary))

# Save the data frame to a CSV file
write.csv(filtered_person_with_summary, "filtered_person_with_summary", row.names = FALSE)


```

# MODELS
#### Data range: 114 COVARIATES

```{r}
# filtered_person_with_summary <- read.csv("filtered_person_with_summary")
# typeof(filtered_person_with_summary$sex_at_birth)
# 
# # Define plausible ranges for eGFR and Albumin/Creatinine
# plausible_eGFR_min <- 0
# plausible_eGFR_max <- 500
# 
# # Filter the dataset for plausible eGFR values and sex "Male" or "Female"
# filtered_data <- filtered_person_with_summary %>%
#   filter(
#     eGFR_value >= plausible_eGFR_min & eGFR_value <= plausible_eGFR_max,
#     sex_at_birth %in% c("Male", "Female")
#   ) %>%
#   select(-Albumin.Creatinine)
# 
# # Print the dimensions of the filtered data to confirm the filtering
# cat("Number of rows in the filtered data: ", nrow(filtered_data), "\n")
# cat("Number of columns in the filtered data: ", ncol(filtered_data), "\n")
# 
# # Count the number of people with disease_status == 0 and disease_status == 1
# disease_status_counts <- filtered_data %>%
#   group_by(disease_status) %>%
#   summarise(count = n())
# 
# # Print the counts
# cat("Number of people with disease_status == 0: ", disease_status_counts$count[disease_status_counts$disease_status == 0], "\n")
# cat("Number of people with disease_status == 1: ", disease_status_counts$count[disease_status_counts$disease_status == 1], "\n")

```

### Data Preprocessing: standardization & Factorization

```{r}
# Load necessary libraries
library(dplyr)
library(tidyr)

data <- read.csv("filtered_person_with_summary")

# Step 1: Convert "race" column to categories and replace NA values in "race" and "ethnicity" with "Not specified"
data <- data %>%
  mutate(race = ifelse(is.na(race), "Not specified", 
                       ifelse(race == "Black or African American", "Black or African American",
                              ifelse(race == "White", "White", "Other"))),
         ethnicity = ifelse(is.na(ethnicity), "Not specified", ethnicity))

# Step 2: One-hot encode categorical variables: 'sex_at_birth', 'ethnicity', 'gender', 'race', 'Smoking'
# For one-hot encoding, we use model.matrix or pivot_longer + pivot_wider for explicit control

# First, convert the necessary columns to factors
data <- data %>%
  mutate(across(c(sex_at_birth, ethnicity, gender, race, Smoking), as.factor))

# Step 3: Apply one-hot encoding
data_one_hot <- data %>%
  pivot_longer(cols = c(sex_at_birth, ethnicity, gender, race, Smoking),
               names_to = "category", 
               values_to = "value") %>%
  unite("category_value", category, value, remove = TRUE) %>%
  mutate(value = 1) %>%
  pivot_wider(names_from = category_value, values_from = value, values_fill = list(value = 0))

# Step 4: Fill any remaining NA values with 0 (if any)
data_one_hot <- data_one_hot %>%
  mutate(across(everything(), ~ replace_na(., 0)))

# Step 5: Apply log(value + 1) transformation to all relevant numeric columns
# Excluding ID columns and categorical columns that were just one-hot encoded
data_one_hot <- data_one_hot %>%
  mutate(across(where(is.numeric) & !any_of(c("person_id", "disease_status", "time_to_event")), ~ log(. + 1)))

# Step 6: Save the transformed dataset to a CSV file
write.csv(data_one_hot, "ml_model_data.csv", row.names = FALSE)

# Optional: Print a few rows to verify the encoding and transformation
head(data_one_hot)



```


### Counts
```{r}
# Load necessary libraries
library(dplyr)
library(tidyr)
library(knitr)
library(caret)

# Load dataset
data <- read.csv("filtered_person_with_summary")

# Step 1: Convert time_to_event thresholds from years to days and create 2-year and 5-year outcome variables
data <- data %>%
  mutate(
    outcome_2_year = ifelse(time_to_event <= 730 & disease_status == 1, 1, 0),  # 730 days = 2 years
    outcome_5_year = ifelse(time_to_event <= 1825 & disease_status == 1, 1, 0)  # 1825 days = 5 years
  )

# Step 2: Split the data into training and testing sets (70% training, 30% testing) without considering the outcome
set.seed(123)  # Setting a seed for reproducibility
trainIndex <- createDataPartition(data$person_id, p = 0.7, list = FALSE)  # Random partition based on `person_id`
train_data <- data[trainIndex, ]
test_data <- data[-trainIndex, ]

# Step 3: Exclude non-covariate columns from training and testing data
train_data <- train_data %>%
  select(-c(person_id, time_to_event, disease_status))

test_data <- test_data %>%
  select(-c(person_id, time_to_event, disease_status))

# Function to count cases and controls
count_cases_controls <- function(data, label_col) {
  counts <- data %>%
    group_by(!!sym(label_col)) %>%
    summarise(count = n(), .groups = 'drop') %>%
    mutate(label = ifelse(!!sym(label_col) == 1, "Cases", "Controls"))
  
  # Ensure both "Cases" and "Controls" are present
  if (!"Cases" %in% counts$label) {
    counts <- bind_rows(counts, data.frame(label = "Cases", count = 0))
  }
  if (!"Controls" %in% counts$label) {
    counts <- bind_rows(counts, data.frame(label = "Controls", count = 0))
  }
  
  return(counts)
}

# Count cases and controls for the 2-year and 5-year training and test sets
train_2_year_counts <- count_cases_controls(train_data, "outcome_2_year")
test_2_year_counts <- count_cases_controls(test_data, "outcome_2_year")

train_5_year_counts <- count_cases_controls(train_data, "outcome_5_year")
test_5_year_counts <- count_cases_controls(test_data, "outcome_5_year")

# Count cases and controls stratified by race
train_2_year_race_counts <- train_data %>%
  group_by(race, outcome_2_year) %>%
  summarise(count = n(), .groups = 'drop') %>%
  mutate(label = ifelse(outcome_2_year == 1, "Cases", "Controls"))

train_5_year_race_counts <- train_data %>%
  group_by(race, outcome_5_year) %>%
  summarise(count = n(), .groups = 'drop') %>%
  mutate(label = ifelse(outcome_5_year == 1, "Cases", "Controls"))

test_2_year_race_counts <- test_data %>%
  group_by(race, outcome_2_year) %>%
  summarise(count = n(), .groups = 'drop') %>%
  mutate(label = ifelse(outcome_2_year == 1, "Cases", "Controls"))

test_5_year_race_counts <- test_data %>%
  group_by(race, outcome_5_year) %>%
  summarise(count = n(), .groups = 'drop') %>%
  mutate(label = ifelse(outcome_5_year == 1, "Cases", "Controls"))

# Ensure that all race groups are handled correctly, including "Black or African American" and "White"
expand_race <- function(data, race_col) {
  races <- unique(data[[race_col]])
  labels <- c("Cases", "Controls")
  expanded <- expand.grid(race = races, label = labels)
  
  data <- left_join(expanded, data, by = c("race", "label")) %>%
    mutate(count = ifelse(is.na(count), 0, count))
  
  return(data)
}

train_2_year_race_counts <- expand_race(train_2_year_race_counts, "race")
train_5_year_race_counts <- expand_race(train_5_year_race_counts, "race")

test_2_year_race_counts <- expand_race(test_2_year_race_counts, "race")
test_5_year_race_counts <- expand_race(test_5_year_race_counts, "race")

# Separate cases and controls in race-stratified data
train_2_year_race_cases <- train_2_year_race_counts %>%
  filter(label == "Cases") %>%
  select(race, Cases = count)

train_2_year_race_controls <- train_2_year_race_counts %>%
  filter(label == "Controls") %>%
  select(race, Controls = count)

train_5_year_race_cases <- train_5_year_race_counts %>%
  filter(label == "Cases") %>%
  select(race, Cases = count)

train_5_year_race_controls <- train_5_year_race_counts %>%
  filter(label == "Controls") %>%
  select(race, Controls = count)

test_2_year_race_cases <- test_2_year_race_counts %>%
  filter(label == "Cases") %>%
  select(race, Cases = count)

test_2_year_race_controls <- test_2_year_race_counts %>%
  filter(label == "Controls") %>%
  select(race, Controls = count)

test_5_year_race_cases <- test_5_year_race_counts %>%
  filter(label == "Cases") %>%
  select(race, Cases = count)

test_5_year_race_controls <- test_5_year_race_counts %>%
  filter(label == "Controls") %>%
  select(race, Controls = count)

# Combine race-stratified cases and controls
train_2_year_race_combined <- left_join(train_2_year_race_cases, train_2_year_race_controls, by = "race")
train_5_year_race_combined <- left_join(train_5_year_race_cases, train_5_year_race_controls, by = "race")

test_2_year_race_combined <- left_join(test_2_year_race_cases, test_2_year_race_controls, by = "race")
test_5_year_race_combined <- left_join(test_5_year_race_cases, test_5_year_race_controls, by = "race")

# Summary for 2-Year
summary_2_year <- bind_rows(
  data.frame(Set = "Training Set", Outcome = "2-Year", Race = "Overall", Cases = sum(train_2_year_race_cases$Cases), Controls = sum(train_2_year_race_controls$Controls)),
  train_2_year_race_combined %>%
    mutate(Set = "Training Set", Outcome = "2-Year", Race = ifelse(race == "Black or African American", "Black or African American", "White")) %>%
    select(Set, Outcome, Race, Cases, Controls),
  data.frame(Set = "Test Set", Outcome = "2-Year", Race = "Overall", Cases = sum(test_2_year_race_cases$Cases), Controls = sum(test_2_year_race_controls$Controls)),
  test_2_year_race_combined %>%
    mutate(Set = "Test Set", Outcome = "2-Year", Race = ifelse(race == "Black or African American", "Black or African American", "White")) %>%
    select(Set, Outcome, Race, Cases, Controls)
)


# Summary for 5-Year
summary_5_year <- bind_rows(
  data.frame(Set = "Training Set", Outcome = "5-Year", Race = "Overall", Cases = sum(train_5_year_race_cases$Cases), Controls = sum(train_5_year_race_controls$Controls)),
  train_5_year_race_combined %>%
    mutate(Set = "Training Set", Outcome = "5-Year", Race = ifelse(race == "Black or African American", "Black or African American", "White")) %>%
    select(Set, Outcome, Race, Cases, Controls),
  data.frame(Set = "Test Set", Outcome = "5-Year", Race = "Overall", Cases = sum(test_5_year_race_cases$Cases), Controls = sum(test_5_year_race_controls$Controls)),
  test_5_year_race_combined %>%
    mutate(Set = "Test Set", Outcome = "5-Year", Race = ifelse(race == "Black or African American", "Black or African American", "White")) %>%
    select(Set, Outcome, Race, Cases, Controls)
)

# Combine 2-Year and 5-Year summaries into one table
combined_summary <- bind_rows(summary_2_year, summary_5_year)

# Format the table for publication
formatted_table <- combined_summary %>%
  mutate(
    Outcome = gsub("-", " ", Outcome),
    Race = ifelse(Race == "Overall", "Overall", Race)
  )

# Use knitr::kable to format the table for publication
kable(
  formatted_table,
  col.names = c("Dataset", "Prediction Timeframe", "Race Group", "Number of Cases", "Number of Controls"),
  caption = "Summary of Cases and Controls for 2-Year and 5-Year Predictions Stratified by Race",
  format = "pipe"
)

# Print the formatted table
print(formatted_table)

# Save the formatted table as a CSV file
write.csv(formatted_table, "formatted_case_control_summary.csv", row.names = FALSE)

# Optional: Inspect the formatted table
head(formatted_table)


```

# 2 Year Risk Prediction

### 1. **XGBoost Model**

```{r}
# Load necessary libraries
library(dplyr)
library(caret)
library(xgboost)
library(randomForest)
library(glmnet)
library(e1071)
library(pROC)
library(ggplot2)

#### CHECK MARGINALLY WHAT IS THE MOST CORRELATTED FEATURE WITH OUTCOME, .999 MIGHT BE A PROBLEM!
##SHARP VALUE, feature importance calculation, r package might be available
## keep gap between two windows, no overlapping between 2 windows, to avoid possible middle overlap, 2 months gap inbetween( 2 months after the index date, who ever gets esrd between 0-2 months after index date, you remove them, check the most predictive features. Include feature importance, remove featiures that might be a surrogate of ESRD.)

# Load dataset
data <- read.csv("ml_model_data.csv")
colnames(data)

# Step 1: Convert time_to_event thresholds from years to days and create 2-year and 5-year outcome variables
data <- data %>%
  mutate(
    outcome_2_year = factor(ifelse(time_to_event <= 730 & disease_status == 1, 1, 0), levels = c(0, 1), labels = c("class0", "class1")),  # 730 days = 2 years
    outcome_5_year = factor(ifelse(time_to_event <= 1825 & disease_status == 1, 1, 0), levels = c(0, 1), labels = c("class0", "class1"))  # 1825 days = 5 years
  )

# Step 2: Split the data into training and testing sets (70/30)
set.seed(123)
trainIndex <- createDataPartition(data$person_id, p = 0.7, list = FALSE)
train_data <- data[trainIndex, ]
test_data <- data[-trainIndex, ]

# Step 3: Exclude non-covariate columns
train_data <- train_data %>%
  select(-c(person_id, time_to_event, disease_status))

test_data <- test_data %>%
  select(-c(person_id, time_to_event, disease_status))

# Step 4: Check for NA or Inf values in the training and testing datasets
# Replace any NA or Inf values with 0 or other meaningful values
train_data <- train_data %>%
  mutate(across(where(is.numeric), ~ ifelse(is.na(.) | is.infinite(.), 0, .)))

test_data <- test_data %>%
  mutate(across(where(is.numeric), ~ ifelse(is.na(.) | is.infinite(.), 0, .)))

# Step 5: Convert labels to factors for classification with valid levels
train_label_2_year <- train_data$outcome_2_year
train_label_5_year <- train_data$outcome_5_year

test_label_2_year <- test_data$outcome_2_year
test_label_5_year <- test_data$outcome_5_year

# Step 6: Convert data to matrix format for XGBoost
train_matrix_2_year <- model.matrix(outcome_2_year ~ . -1, data = train_data)
train_matrix_5_year <- model.matrix(outcome_5_year ~ . -1, data = train_data)

test_matrix_2_year <- model.matrix(outcome_2_year ~ . -1, data = test_data)
test_matrix_5_year <- model.matrix(outcome_5_year ~ . -1, data = test_data)

# Step 7: Define XGBoost parameters and train model for 2-year prediction
xgb_params <- list(
  booster = "gbtree",
  objective = "binary:logistic",  # Use binary classification for outcomes
  eval_metric = "logloss",        # You can change this to 'auc' or other metrics
  eta = 0.1,                      # Learning rate
  max_depth = 6,                  # Maximum depth of trees
  gamma = 0.1,                    # Minimum loss reduction required to make a split
  colsample_bytree = 0.8,         # Subsample ratio of columns
  subsample = 0.8                 # Subsample ratio of rows
)

# Prepare the DMatrix for XGBoost (required format)
dtrain_2_year <- xgb.DMatrix(data = train_matrix_2_year, label = as.numeric(train_label_2_year) - 1)
dtest_2_year <- xgb.DMatrix(data = test_matrix_2_year, label = as.numeric(test_label_2_year) - 1)

# Train the XGBoost model for 2-year prediction
xgb_model_2_year <- xgb.train(
  params = xgb_params,
  data = dtrain_2_year,
  nrounds = 200,                  # Number of boosting rounds
  watchlist = list(train = dtrain_2_year, test = dtest_2_year),
  early_stopping_rounds = 10,     # Early stopping criteria
  verbose = 1                     # Display training progress
)

# Step 8: Train model for 5-year prediction
dtrain_5_year <- xgb.DMatrix(data = train_matrix_5_year, label = as.numeric(train_label_5_year) - 1)
dtest_5_year <- xgb.DMatrix(data = test_matrix_5_year, label = as.numeric(test_label_5_year) - 1)

# Train the XGBoost model for 5-year prediction
xgb_model_5_year <- xgb.train(
  params = xgb_params,
  data = dtrain_5_year,
  nrounds = 200,                  # Number of boosting rounds
  watchlist = list(train = dtrain_5_year, test = dtest_5_year),
  early_stopping_rounds = 10,     # Early stopping criteria
  verbose = 1                     # Display training progress
)

# Step 9: Model evaluation for 2-year prediction
preds_2_year <- predict(xgb_model_2_year, newdata = dtest_2_year)
roc_2_year <- roc(as.numeric(test_label_2_year) - 1, preds_2_year)
auc_2_year <- auc(roc_2_year)
cat("AUC for 2-year prediction: ", auc_2_year, "\n")

# Step 10: Model evaluation for 5-year prediction
preds_5_year <- predict(xgb_model_5_year, newdata = dtest_5_year)
roc_5_year <- roc(as.numeric(test_label_5_year) - 1, preds_5_year)
auc_5_year <- auc(roc_5_year)
cat("AUC for 5-year prediction: ", auc_5_year, "\n")

# Optional: Plot ROC curve
plot(roc_2_year, main = "ROC Curve for 2-Year Prediction", col = "blue")
plot(roc_5_year, main = "ROC Curve for 5-Year Prediction", col = "red", add = TRUE)
legend("bottomright", legend = c("2-Year", "5-Year"), col = c("blue", "red"), lwd = 2)


```

### 2. **Random Forest Model**
```{r}
# Load necessary libraries
library(dplyr)
library(caret)
library(randomForest)
library(pROC)
library(knitr)
library(ggplot2)

# Load dataset
data <- read.csv("ml_model_data.csv")

# Step 1: Convert time_to_event thresholds from years to days and create 2-year and 5-year outcome variables
data <- data %>%
  mutate(
    outcome_2_year = factor(ifelse(time_to_event <= 730 & disease_status == 1, 1, 0), levels = c(0, 1), labels = c("class0", "class1")),  # 730 days = 2 years
    outcome_5_year = factor(ifelse(time_to_event <= 1825 & disease_status == 1, 1, 0), levels = c(0, 1), labels = c("class0", "class1"))  # 1825 days = 5 years
  )

# Step 2: Split the data into training and testing sets (70/30) without considering outcome
set.seed(123)
trainIndex <- createDataPartition(data$person_id, p = 0.7, list = FALSE)
train_data <- data[trainIndex, ]
test_data <- data[-trainIndex, ]

# Step 3: Exclude non-covariate columns
train_data <- train_data %>%
  select(-c(person_id, time_to_event, disease_status))

test_data <- test_data %>%
  select(-c(person_id, time_to_event, disease_status))

# Step 4: Data cleaning to remove NA, NaN, and Inf values
clean_data <- function(df) {
  df <- df %>%
    mutate_all(~ ifelse(is.infinite(.), NA, .)) %>%  # Convert Inf to NA
    na.omit()  # Remove rows with NA values
  return(df)
}

train_data <- clean_data(train_data)
test_data <- clean_data(test_data)

# Function to run the Random Forest model and capture performance metrics for one-hot encoded data
run_rf_model_by_race <- function(train_data, test_data, race_column = NULL, label_column, outcome_column) {
  
  # Filter data based on one-hot-encoded race column if specified
  if (!is.null(race_column)) {
    train_subset <- train_data %>% filter(!!sym(race_column) == 1)
    test_subset <- test_data %>% filter(!!sym(race_column) == 1)
  } else {
    train_subset <- train_data
    test_subset <- test_data
  }
  
  # Adjust labels for the test set
  test_label_subset <- test_subset %>% pull(label_column)
  
  # Train the Random Forest model
  rf_model <- randomForest(
    as.formula(paste(outcome_column, "~ .")),
    data = train_subset,
    ntree = 500,
    mtry = floor(sqrt(ncol(train_subset) - 2))  # Exclude outcome columns
  )
  
  # Predictions (probabilities and classes)
  rf_probs <- predict(rf_model, newdata = test_subset, type = "prob")[,2]
  rf_preds <- factor(ifelse(rf_probs > 0.5, "class1", "class0"), levels = c("class0", "class1"))
  
  # Evaluate model performance using AUC
  auc <- as.numeric(roc(test_label_subset, rf_probs)$auc)
  
  # Calculate the confusion matrix
  conf_matrix <- confusionMatrix(rf_preds, test_label_subset, positive = "class1")
  
  # Extract metrics
  accuracy <- conf_matrix$overall["Accuracy"]
  sensitivity <- conf_matrix$byClass["Sensitivity"]
  specificity <- conf_matrix$byClass["Specificity"]
  
  # Variable Importance
  importance_scores <- importance(rf_model)
  var_importance <- data.frame(Variable = rownames(importance_scores), Importance = importance_scores[, 1])
  var_importance <- var_importance[order(var_importance$Importance, decreasing = TRUE), ]
  
  # Remove outcome variables if they appear in the list
  var_importance <- var_importance[!grepl("outcome_2_year|outcome_5_year", var_importance$Variable), ]
  
  # Race label for saving files
  race_label <- ifelse(is.null(race_column), "all_races", race_column)
  
  # Print performance metrics and variable importance
  cat("\nPerformance Metrics for", race_label, ":\n")
  print(data.frame(
    Race = ifelse(is.null(race_column), "All Races", race_column),
    AUC = auc,
    Accuracy = as.numeric(accuracy),
    Sensitivity = as.numeric(sensitivity),
    Specificity = as.numeric(specificity)
  ))
  
  cat("\nVariable Importance for", race_label, ":\n")
  print(var_importance)
  
  # Plot Variable Importance
  importance_plot <- ggplot(var_importance, aes(x = reorder(Variable, Importance), y = Importance)) +
    geom_bar(stat = "identity") +
    coord_flip() +
    xlab("Variables") +
    ylab("Importance Score") +
    ggtitle(paste("Variable Importance for", race_label)) +
    theme_minimal()
  
  # Print the plot
  print(importance_plot)
  
  # Save variable importance table
  write.csv(var_importance, paste0("rf_", outcome_column, "_var_importance_", race_label, ".csv"), row.names = FALSE)
  
  # Save the plot
  ggsave(paste0("rf_", outcome_column, "_var_importance_plot_", race_label, ".png"), plot = importance_plot)
  
  # Return the overall performance metrics for comparison
  return(data.frame(
    Race = ifelse(is.null(race_column), "All Races", race_column),
    AUC = auc,
    Accuracy = as.numeric(accuracy),
    Sensitivity = as.numeric(sensitivity),
    Specificity = as.numeric(specificity)
  ))
}

# Start timing the execution
start_time <- Sys.time()

### 2-Year Prediction Models

# Run the model for all races (2-Year Prediction)
result_all_2_year <- run_rf_model_by_race(train_data, test_data, label_column = "outcome_2_year", outcome_column = "outcome_2_year")

# Run the model for race == "race_Black_or_African_American" (2-Year Prediction)
result_race_black_2_year <- run_rf_model_by_race(train_data, test_data, race_column = "race_Black_or_African_American", label_column = "outcome_2_year", outcome_column = "outcome_2_year")

# Run the model for race == "race_White" (2-Year Prediction)
result_race_white_2_year <- run_rf_model_by_race(train_data, test_data, race_column = "race_White", label_column = "outcome_2_year", outcome_column = "outcome_2_year")

# Combine performance results for 2-Year prediction into a single table
combined_performance_2_year <- bind_rows(result_all_2_year, result_race_black_2_year, result_race_white_2_year)

# Print the combined performance table for 2-Year prediction
cat("\nCombined Performance Metrics by Race Group (2-Year Prediction):\n")
print(combined_performance_2_year)

# Save the combined performance table for 2-Year prediction
write.csv(combined_performance_2_year, "rf_2_year_performance_combined.csv", row.names = FALSE)

### 5-Year Prediction Models

# Run the model for all races (5-Year Prediction)
result_all_5_year <- run_rf_model_by_race(train_data, test_data, label_column = "outcome_5_year", outcome_column = "outcome_5_year")

# Run the model for race == "race_Black_or_African_American" (5-Year Prediction)
result_race_black_5_year <- run_rf_model_by_race(train_data, test_data, race_column = "race_Black_or_African_American", label_column = "outcome_5_year", outcome_column = "outcome_5_year")

# Run the model for race == "race_White" (5-Year Prediction)
result_race_white_5_year <- run_rf_model_by_race(train_data, test_data, race_column = "race_White", label_column = "outcome_5_year", outcome_column = "outcome_5_year")

# Combine performance results for 5-Year prediction into a single table
combined_performance_5_year <- bind_rows(result_all_5_year, result_race_black_5_year, result_race_white_5_year)

# Print the combined performance table for 5-Year prediction
cat("\nCombined Performance Metrics by Race Group (5-Year Prediction):\n")
print(combined_performance_5_year)

# Save the combined performance table for 5-Year prediction
write.csv(combined_performance_5_year, "rf_5_year_performance_combined.csv", row.names = FALSE)

# End timing the execution
end_time <- Sys.time()

# Calculate and print the time taken to run the models
execution_time <- end_time - start_time
cat("\nTime taken to run the models: ", execution_time, "\n")

# Optional: Print a summary of the overall execution time and results
cat("\nSummary of Execution:\n")
cat("2-Year Prediction Performance Metrics:\n")
print(combined_performance_2_year)

cat("\n5-Year Prediction Performance Metrics:\n")
print(combined_performance_5_year)

cat("\nTotal Execution Time: ", execution_time, "\n")

# Optional: Save the execution time to a file for reference
write.csv(data.frame(ExecutionTime = execution_time), "rf_execution_time.csv", row.names = FALSE)

# Optional: Display the top variables for both 2-year and 5-year models for all races
cat("\nTop Variables for 2-Year Prediction (All Races):\n")
print(head(result_all_2_year$Variable))

cat("\nTop Variables for 5-Year Prediction (All Races):\n")
print(head(result_all_5_year$Variable))

# Save the top variables to CSV files for further analysis
write.csv(head(result_all_2_year$Variable), "top_variables_2_year_all_races.csv", row.names = FALSE)
write.csv(head(result_all_5_year$Variable), "top_variables_5_year_all_races.csv", row.names = FALSE)


```





### 3. **GLMNET Model for 2-Year Prediction**
```{r}
# Load necessary libraries
library(caret)
library(glmnet)
library(ggplot2)
library(dplyr)
library(pROC)

# Function to run the GLMNET model and capture performance metrics
run_glmnet_model_by_race <- function(train_matrix, test_matrix, train_labels, test_labels, train_race_vector, test_race_vector, race_value = NULL) {
  
  # Filter data based on race if specified
  if (!is.null(race_value)) {
    train_indices <- which(train_race_vector == race_value)
    test_indices <- which(test_race_vector == race_value)
    
    train_matrix <- train_matrix[train_indices, ]
    test_matrix <- test_matrix[test_indices, ]
    
    train_labels <- train_labels[train_indices]
    test_labels <- test_labels[test_indices]
  }
  
  # Define the tuning grid for alpha and lambda
  glmnet_grid <- expand.grid(
    alpha = c(0, 0.5, 1),  # 0 for ridge, 1 for lasso, 0.5 for elastic net. # FIX alpha at 0.5
    lambda = seq(0.0001, 0.1, length = 100)
  )
  
  # Train the GLMNET model with cross-validation
  glmnet_model <- train(
    x = train_matrix,
    y = train_labels,
    method = "glmnet",
    trControl = trainControl(
      method = "cv",
      number = 10,  # 10-fold cross-validation
      search = "grid",  # Grid search to explore different alpha and lambda values
      classProbs = TRUE,
      summaryFunction = twoClassSummary,
      sampling = "smote"
    ),
    tuneGrid = glmnet_grid,
    metric = "ROC"
  )
  
  # Predict class probabilities on the test set
  glmnet_probs <- predict(glmnet_model, newdata = test_matrix, type = "prob")[,2]
  
  # Evaluate model performance using AUC
  auc_glmnet <- as.numeric(roc(as.numeric(test_labels), glmnet_probs)$auc)
  
  # Predict classes and calculate confusion matrix
  glmnet_preds <- ifelse(glmnet_probs > 0.5, "class1", "class0")
  conf_matrix <- confusionMatrix(factor(glmnet_preds, levels = c("class0", "class1")), factor(test_labels, levels = c("class0", "class1")), positive = "class1")
  
  # Extract performance metrics
  accuracy <- conf_matrix$overall["Accuracy"]
  sensitivity <- conf_matrix$byClass["Sensitivity"]
  specificity <- conf_matrix$byClass["Specificity"]
  
  # Extract coefficients from the best GLMNET model
  best_lambda <- glmnet_model$bestTune$lambda
  coef_glmnet <- coef(glmnet_model$finalModel, s = best_lambda)
  coef_df <- as.data.frame(as.matrix(coef_glmnet))
  coef_df$Variable <- rownames(coef_df)
  coef_df <- coef_df %>% filter(Variable != "(Intercept)") %>% arrange(desc(abs(s1)))
  
  # Remove irrelevant outcome variables
  coef_df <- coef_df[!coef_df$Variable %in% c("outcome_2_yearclass0", "outcome_2_yearclass1", "outcome_5_yearclass0", "outcome_5_yearclass1"), ]
  
  # Print coefficients and variable importance
  cat("\nCoefficients and Importance Scores for Race", ifelse(is.null(race_value), "All Races", race_value), ":\n")
  print(coef_df)
  
  # Plot Variable Importance based on Coefficients
  importance_plot <- ggplot(coef_df, aes(x = reorder(Variable, abs(s1)), y = abs(s1))) +
    geom_bar(stat = "identity") +
    coord_flip() +
    xlab("Variables") +
    ylab("Coefficient Magnitude") +
    ggtitle(paste("Variable Importance for", ifelse(is.null(race_value), "All Races", paste("Race", race_value)))) +
    theme_minimal()
  
  print(importance_plot)
  
  # Save the coefficients and importance scores
  race_label <- ifelse(is.null(race_value), "all_races", paste0("race_", race_value))
  write.csv(coef_df, paste0("glmnet_2_year_coefficients_", race_label, ".csv"), row.names = FALSE)
  ggsave(paste0("glmnet_2_year_var_importance_plot_", race_label, ".png"), plot = importance_plot)
  
  # Return the performance metrics as numeric values for AUC
  return(data.frame(
    Race = ifelse(is.null(race_value), "All Races", ifelse(race_value == 1, "Black or African American", "White")),
    AUC = as.numeric(auc_glmnet),
    Accuracy = as.numeric(accuracy),
    Sensitivity = as.numeric(sensitivity),
    Specificity = as.numeric(specificity)
  ))
}

# Start timing the execution
start_time <- Sys.time()

# Define your data for training and testing
# Ensure train_matrix_2_year_clean, test_matrix_2_year_clean, train_label_2_year, test_label_2_year, train_race_vector, and test_race_vector are defined

# Run the model for all races
performance_all <- run_glmnet_model_by_race(train_matrix_2_year_clean, test_matrix_2_year_clean, train_label_2_year, test_label_2_year, train_race_vector, test_race_vector)

# Run the model for race == 1 (Black or African American)
performance_race_1 <- run_glmnet_model_by_race(train_matrix_2_year_clean, test_matrix_2_year_clean, train_label_2_year, test_label_2_year, train_race_vector, test_race_vector, race_value = 1)

# Run the model for race == 0 (White)
performance_race_0 <- run_glmnet_model_by_race(train_matrix_2_year_clean, test_matrix_2_year_clean, train_label_2_year, test_label_2_year, train_race_vector, test_race_vector, race_value = 0)

# Combine results into a single table
combined_performance <- bind_rows(performance_all, performance_race_1, performance_race_0)

# Print the combined performance table
cat("\nCombined Performance Metrics by Race Group:\n")
print(combined_performance)

# Save the combined performance table
write.csv(combined_performance, "glmnet_2_year_performance_combined.csv", row.names = FALSE)

# End timing the execution
end_time <- Sys.time()

# Calculate and print the time taken to run the models
execution_time <- end_time - start_time
cat("\nTime taken to run the models: ", execution_time, "\n")

```
### 4. **SVM Model for 2-Year Prediction**
```{r}
# Load necessary libraries
library(caret)
library(pROC)
library(dplyr)
library(ggplot2)

# Function to run the SVM model and capture performance metrics
run_svm_model_by_race <- function(train_data, test_data, race_value = NULL) {
  
  # Filter data based on race if specified
  if (!is.null(race_value)) {
    train_data <- train_data %>% filter(race == race_value)
    test_data <- test_data %>% filter(race == race_value)
  }
  
  # Extract labels
  train_label <- train_data$outcome_2_year
  test_label <- test_data$outcome_2_year
  
  # Train the SVM model with class weighting
  svm_model <- train(
    outcome_2_year ~ ., data = train_data %>% select(-race),
    method = "svmRadial",
    trControl = trainControl(
      method = "cv",
      number = 10,
      classProbs = TRUE,
      summaryFunction = twoClassSummary,
      sampling = "smote"
    ),
    tuneLength = 10,
    metric = "ROC"
  )
  
  # Predictions (probabilities and classes)
  svm_probs <- predict(svm_model, newdata = test_data %>% select(-race), type = "prob")[,2]
  svm_preds <- factor(ifelse(svm_probs > 0.5, "class1", "class0"), levels = c("class0", "class1"))
  
  # Evaluate model performance using AUC
  auc <- as.numeric(roc(test_label, svm_probs)$auc)
  
  # Calculate the confusion matrix
  conf_matrix <- confusionMatrix(svm_preds, test_label, positive = "class1")
  
  # Extract metrics
  accuracy <- conf_matrix$overall["Accuracy"]
  sensitivity <- conf_matrix$byClass["Sensitivity"]
  specificity <- conf_matrix$byClass["Specificity"]
  
  # Since SVM doesn't naturally provide variable importance, it's not included in this output.
  
  # Return the overall performance metrics for comparison
  return(data.frame(
    Race = ifelse(is.null(race_value), "All Races", ifelse(race_value == 1, "Black or African American", "White")),
    AUC = auc,
    Accuracy = as.numeric(accuracy),
    Sensitivity = as.numeric(sensitivity),
    Specificity = as.numeric(specificity)
  ))
}

# Start timing the execution
start_time <- Sys.time()

# Run the model for all races
performance_all <- run_svm_model_by_race(train_data, test_data)

# Run the model for race == 1 (Black or African American)
performance_race_1 <- run_svm_model_by_race(train_data, test_data, race_value = 1)

# Run the model for race == 0 (White)
performance_race_0 <- run_svm_model_by_race(train_data, test_data, race_value = 0)

# Combine results into a single table
combined_performance <- bind_rows(performance_all, performance_race_1, performance_race_0)

# Print the combined performance table
cat("\nCombined Performance Metrics by Race Group:\n")
print(combined_performance)

# Save the combined performance table
write.csv(combined_performance, "svm_2_year_performance_combined.csv", row.names = FALSE)

# End timing the execution
end_time <- Sys.time()

# Calculate and print the time taken to run the models
execution_time <- end_time - start_time
cat("\nTime taken to run the models: ", execution_time, "\n")

```

### 5. **Deep neural network Model for 2-Year Prediction**
```{r}
# put on hold
```



